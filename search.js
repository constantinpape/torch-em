window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "torch_em", "modulename": "torch_em", "kind": "module", "doc": "<p><a href=\"https://github.com/constantinpape/torch-em\">torch_em</a> is a library for deep learning in microscopy images. It is built on top of <a href=\"https://pytorch.org/\">PyTorch</a>.</p>\n\n<p>We are working on the documentation and will extend and improve it soon!</p>\n\n<h1 id=\"datasets-in-torch-em\">Datasets in <code>torch-em</code></h1>\n\n<p>We provide PyTorch Datasets / DataLoaders for many different biomedical datasets, mostly for segmentation tasks.\nThey are implemented in <code>torch_em.data.datasets</code>. See <code>scripts/datasets</code> for examples on how to visualize images from these datasets.</p>\n\n<h2 id=\"available-datasets\">Available Datasets</h2>\n\n<p>All datasets in <code>torch_em.data.datasets</code> are implemented according to the following logic:</p>\n\n<ul>\n<li>The function <code>get_..._data</code> downloads the respective datasets. Note that some datasets cannot be downloaded automatically. In these cases the function will raise an error with a message that explains how to download the data.</li>\n<li>The function <code>get_..._dataset</code> returns the PyTorch Dataset for the corresponding dataset.</li>\n<li>The function <code>get_..._dataloader</code> returns the PyTorch DataLoader for the corresponding dataset.</li>\n</ul>\n\n<h3 id=\"light-microscopy\">Light Microscopy</h3>\n\n<p>We provide several light microscopy datasets. See <code>torch_em.data.datasets.light_microscopy</code> for an overview.</p>\n\n<h3 id=\"electron-microscopy\">Electron Microscopy</h3>\n\n<p>We provide several electron microscopy datasets. See <code>torch_em.data.datasets.electron_microscopy</code> for an overview.</p>\n\n<h3 id=\"histopathology\">Histopathology</h3>\n\n<p><code>torch_em.data.datasets.histopathology</code></p>\n\n<h3 id=\"medical-imaging\">Medical Imaging</h3>\n\n<p><code>torch_em.data.datasets.medical</code></p>\n\n<h2 id=\"how-to-create-your-own-dataloader\">How to create your own dataloader?</h2>\n\n<p>Let's say you have a specific dataset of interest and would want to create a PyTorch supported <code>torch-em</code>-based dataloader for yourself. We will walk you through how this can be done. See <code>torch_em/notebooks/tutorial_data_loaders.ipynb</code> for an extensive tutorial with some examples.</p>\n\n<h3 id=\"supported-data-formats\">Supported Data Formats</h3>\n\n<p><code>torch-em</code> and <a href=\"https://github.com/constantinpape/elf\"><code>elf</code></a> currently support Zarr (<code>.zarr</code>), NIFTI (<code>.nii</code>, <code>.nii.gz</code>), HDF5 (<code>.h5</code>, <code>.hdf5</code>),  N5 (<code>.n5</code>), MRC (<code>.mrc</code>) and all imageio supported <a href=\"https://imageio.readthedocs.io/en/v2.5.0/formats.html\">formats</a> (eg. <code>.tif</code>, <code>.png</code>, <code>.jpg</code>, etc.).</p>\n\n<h3 id=\"supported-data-structures\">Supported Data Structures</h3>\n\n<blockquote>\n  <p>The recommended input shapes are hinted in all the below mentioned cases as an example.</p>\n</blockquote>\n\n<ul>\n<li><p>2d images</p>\n\n<ul>\n<li>Mono-channel inputs of:\n<ul>\n<li>\u2705 same size (i.e. all images have shape (256, 256), for example)\n<ul>\n<li>use <code>SegmentationDataset</code> (recommended) or <code>ImageCollectionDataset</code></li>\n</ul></li>\n<li>\u2705 different sizes (i.e. images have shapes like (256, 256), (378, 378), (512, 512), etc., for example)\n<ul>\n<li>use <code>ImageCollectionDataset</code></li>\n</ul></li>\n</ul></li>\n<li>Multi-channel inputs of:\n<ul>\n<li>&gt; NOTE: It's important to convert the images to be channels first (see above for the expected format)</li>\n<li>\u2705 same size (i.e. all images have shape (3, 256, 256), for example)\n<ul>\n<li>use <code>SegmentationDataset</code> (recommended) or <code>ImageCollectionDataset</code></li>\n</ul></li>\n<li>\u2705 different sizes (i.e. images have shapes like (3, 256, 256), (3, 378, 378), (3, 512, 512), etc., for example)\n<ul>\n<li>use <code>ImageCollectionDataset</code></li>\n</ul></li>\n</ul></li>\n</ul></li>\n<li><p>3d images</p>\n\n<ul>\n<li>Mono-channel inputs of:\n<ul>\n<li>\u2705 same size (i.e. all volumes have shape (100, 256, 256), for example)\n<ul>\n<li>use <code>SegmentationDataset</code></li>\n</ul></li>\n<li>\u2705 same shape per slice with different z-stack size (i.e. volumes have shape like (100, 256, 256), (100, 256, 256), (100, 256, 256), etc., for example)\n<ul>\n<li>use <code>SegmentationDataset</code> per volume</li>\n</ul></li>\n<li>\u2705 different sizes (i.e. volumes have shapes like (100, 256, 256), (200, 378, 378), (300, 512, 512), etc., for example)\n<ul>\n<li>use <code>SegmentationDataset</code> per volume</li>\n</ul></li>\n</ul></li>\n<li>Multi-channel inputs of:\n<ul>\n<li>\u2705 same size (i.e. all volumes have shape (100, 3, 256, 256), for example)\n<ul>\n<li>use <code>SegmentationDataset</code></li>\n</ul></li>\n<li>\u2705 same shape per slice with different z-stack size (i.e. volumes have shape like (100, 3, 256, 256), (100, 3, 256, 256), (100, 3, 256, 256), etc., for example)\n<ul>\n<li>use <code>SegmentationDataset</code> per volume</li>\n</ul></li>\n<li>\u2705 different sizes (i.e. volumes have shapes like (100, 3, 256, 256), (200, 2, 378, 378), (300, 4, 512, 512), etc., for example)\n<ul>\n<li>use <code>SegmentationDataset</code> per volume</li>\n</ul></li>\n</ul></li>\n</ul></li>\n</ul>\n\n<h4 id=\"note\">NOTE:</h4>\n\n<ol>\n<li>If your data isn't according to one of the suggested data formats, the data loader creation wouldn't work. It's recommended to convert the data into one of the currently supported data structures (we recommend using Zarr / HDF5 / N5 for this purpose) and then move ahead.</li>\n<li>If your data isn't according to one of the supported data structures, you might run into many issues, leading to incorrect formatting of inputs in your dataloader (we recommend taking a look above at <code>Supported Data Structures</code> -> <code>examples</code> per point).</li>\n<li>If you have suggestions (or requests) on additional data formats or data structures, let us know <a href=\"https://github.com/constantinpape/torch-em/issues\">here</a>.</li>\n</ol>\n\n<h3 id=\"create-the-dataset-object\">Create the dataset object</h3>\n\n<p>Once you have decided on your choice of dataset class object from above, here's an example on important parameters expected for your custom dataset.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">torch_em.data</span> <span class=\"kn\">import</span> <span class=\"n\">ImageCollectionDataset</span><span class=\"p\">,</span> <span class=\"n\">SegmentationDataset</span>\n\n<span class=\"c1\"># 1. choice: ImageCollectionDataset</span>\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">ImageCollectionDataset</span><span class=\"p\">(</span>\n    <span class=\"n\">raw_image_paths</span><span class=\"o\">=&lt;</span><span class=\"n\">SORTED_LIST_OF_IMAGE_PATHS</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># path to all images</span>\n    <span class=\"n\">label_image_paths</span><span class=\"o\">=&lt;</span><span class=\"n\">SORTED_LIST_OF_LABEL_PATHS</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># path to all labels</span>\n    <span class=\"n\">patch_shape</span><span class=\"o\">=&lt;</span><span class=\"n\">PATCH_SHAPE</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># the expected patch shape to be extracted from the image</span>\n    <span class=\"c1\"># there are other optional parameters, see `torch_em.data.image_collection_dataset.py` for details.</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># 2. choice: SegmentationDataset</span>\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">SegmentationDataset</span><span class=\"p\">(</span>\n    <span class=\"n\">raw_path</span><span class=\"o\">=&lt;</span><span class=\"n\">PATH_TO_IMAGE</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># path to one image volume or multiple image volumes (of same shape)</span>\n    <span class=\"n\">raw_key</span><span class=\"o\">=&lt;</span><span class=\"n\">IMAGE_KEY</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># the value to access images from heterogenous storage formats like zarr, hdf5, n5</span>\n    <span class=\"n\">label_path</span><span class=\"o\">=&lt;</span><span class=\"n\">PATH_TO_LABEL</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># path to one label volume or multiple label volumes (of same shape)</span>\n    <span class=\"n\">label_key</span><span class=\"o\">=&lt;</span><span class=\"n\">LABEL_KEY</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>   <span class=\"c1\"># the value to access labels from heterogenous storage formats like zarr, hdf5, n5</span>\n    <span class=\"n\">patch_shape</span><span class=\"o\">=&lt;</span><span class=\"n\">PATCH_SHAPE</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># the expected patch shape to be extracted from the image</span>\n    <span class=\"n\">ndim</span><span class=\"o\">=&lt;</span><span class=\"n\">NDIM</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># the expected dimension of your desired patches (2 for two-dimensional and 3 for three-dimensional)</span>\n    <span class=\"c1\"># there are other optional parameters, see `torch_em.data.segmentation_dataset.py` for details.</span>\n<span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"create-the-dataloader-object\">Create the dataloader object</h3>\n\n<p>Now that we have our dataset object created, let's finally create the dataloader object to start with the training.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">torch_em.segmentation</span> <span class=\"kn\">import</span> <span class=\"n\">get_data_loader</span>\n\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n\n<span class=\"n\">loader</span> <span class=\"o\">=</span> <span class=\"n\">get_data_loader</span><span class=\"p\">(</span>\n    <span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"p\">,</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span>\n    <span class=\"c1\"># there are other optional parameters, which work the same as for `torch.utils.data.DataLoader`.</span>\n    <span class=\"c1\"># feel free to pass them with the PyTorch convention, they should work fine.</span>\n    <span class=\"c1\"># e.g. `shuffle=True`, `num_workers=16`, etc.</span>\n<span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"recommendations\">Recommendations</h3>\n\n<ol>\n<li>Most of the open-source datasets come with their recommended train / val / test splits. In that case, the best practice is to create a function to automatically create the dataset / dataloader for all three splits for you (see <code>torch_em.data.datasets.dynamicnuclearnet.py</code> for inspiration) (OR, create three datasets / dataloader one after the other).</li>\n<li>Some datasets offer a training set and a test set. The best practice is create a \"balanced\" split internally (for train and val, if desired) and then create the datasets / dataloaders.</li>\n<li>Some datasets offer only one set of inputs for developing models. There are multiple ways to handle this case, either extend in the direction of <code>2.</code>, or design your own heuristic for your use-case.</li>\n<li>Some datasets offer only training images (without any form of labels). In this case, you could use <code>RawImageCollectionDataset</code> as the following (for inspiration, take <code>torch_em.data.datasets.neurips_cell_seg.py</code> -> <code>get_neurips_cellseg_unsupervised_dataset</code> as reference)</li>\n</ol>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">torch_em.data</span> <span class=\"kn\">import</span> <span class=\"n\">RawImageCollectionDataset</span>\n\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">RawImageCollectionDataset</span><span class=\"p\">(</span>\n    <span class=\"n\">raw_image_paths</span><span class=\"o\">=&lt;</span><span class=\"n\">LIST_TO_IMAGE_PATHS</span><span class=\"o\">&gt;</span><span class=\"p\">,</span>  <span class=\"c1\"># path to all images</span>\n    <span class=\"c1\"># there are other optional parameters, see `torch_em.data.raw_image_collection_dataset.py` for details.</span>\n<span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<!-- I would rather auto-generate this in a proper pdoc documentation.\n- ASEM (`asem.py`): Segmentation of organelles in FIB-SEM cells.\n- AxonDeepSeg (`axondeepseg.py`): Segmentation of myelinated axons in electron microscopy.\n- MitoLab* (`cem.py`):\n    - CEM MitoLab: Segmentation of mitochondria in electron microscopy.\n    - CEM Mito Benchmark: Segmentation of mitochondria in 7 benchmark electron microscopy datasets.\n- Covid IF (`covidif.py`): Segmentation of cells and nuclei in immunofluoroscence.\n- CREMI (`cremi.py`): Segmentation of neurons in electron microscopy.\n- Cell Tracking Challenge (`ctc.py`): Segmentation data for cell tracking challenge (consists of 10 datasets).\n- DeepBacs (`deepbacs.py`): Segmentation of bacteria in light microscopy.\n- DSB (`dsb.py`): Segmentation of nuclei in light microscopy.\n- DynamicNuclearNet* (`dynamicnuclearnet.py`): Segmentation of nuclei in fluorescence microscopy.\n- HPA (`hpa.py`): Segmentation of cells in light microscopy.\n- ISBI (`isbi2012.py`): Segmentation of neurons in electron microscopy.\n- Kasthuri (`kasthuri.py`): Segmentation of mitochondria in electron microscopy.\n- LIVECell (`livecell.py`): Segmentation of cells in phase-contrast microscopy.\n- Lucchi (`lucchi.py`): Segmentation of mitochondria in electron microscopy.\n- MitoEM (`mitoem.py`): Segmentation of mitochondria in electron microscopy.\n- Mouse Embryo (`mouse_embryo.py`): Segmentation of nuclei in confocal microscopy.\n- NeurIPS CellSeg (`neurips_cell_seg.py`): Segmentation of cells in multi-modality light microscopy datasets.\n- NucMM (`nuc_mm.py`): Segmentation of nuclei in electron microscopy and micro-CT.\n- PlantSeg (`plantseg.py`): Segmentation of cells in confocal and light-sheet microscopy.\n- Platynereis (`platynereis.py`): Segmentation of nuclei in electron microscopy.\n- PNAS* (`pnas_arabidopsis.py`): TODO\n- SNEMI (`snemi.py`): Segmentation of neurons in electron microscopy.\n- Sponge EM (`sponge_em.py`): Segmentation of sponge cells and organelles in electron microscopy.\n- TissueNet* (`tissuenet.py`): Segmentation of cellls in tissue imaged with light microscopy.\n- UroCell (`uro_cell.py`): Segmentation of mitochondria and other organelles in electron microscopy.\n- VNC (`vnc.py`): Segmentation of mitochondria in electron microscopy\n\n### Histopathology\n\n- BCSS (`bcss.py`): Segmentation of breast cancer tissue in histopathology.\n- Lizard* (`lizard.py`): Segmentation of nuclei in histopathology.\n- MoNuSaC (`monusac.py`): Segmentation of multi-organ nuclei in histopathology.\n- MoNuSeg (`monuseg.py`): Segmentation of multi-organ nuclei in histopathology.\n- PanNuke (`pannuke.py`): Segmentation of nuclei in histopathology.\n\n### Medical Imaging\n\n- AutoPET* (`medical/autopet.py`): Segmentation of lesions in whole-body FDG-PET/CT.\n- BTCV* (`medical/btcv.py`): Segmentation of multiple organs in CT.\n\n### NOTE:\n- \\* - These datasets cannot be used out of the box (mostly because of missing automatic downloading). Please take a look at the scripts and the dataset object for details.\n-->\n"}, {"fullname": "torch_em.classification", "modulename": "torch_em.classification", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification", "modulename": "torch_em.classification.classification", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification.ClassificationMetric", "modulename": "torch_em.classification.classification", "qualname": "ClassificationMetric", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification.ClassificationMetric.__init__", "modulename": "torch_em.classification.classification", "qualname": "ClassificationMetric.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">metric_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;accuracy_score&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">metric_kwargs</span></span>)</span>"}, {"fullname": "torch_em.classification.classification.ClassificationMetric.metric", "modulename": "torch_em.classification.classification", "qualname": "ClassificationMetric.metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification.ClassificationMetric.metric_kwargs", "modulename": "torch_em.classification.classification", "qualname": "ClassificationMetric.metric_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification.default_classification_loader", "modulename": "torch_em.classification.classification", "qualname": "default_classification_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span>,</span><span class=\"param\">\t<span class=\"n\">target</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">normalization</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">augmentation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">image_shape</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">loader_kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification.zarr_classification_loader", "modulename": "torch_em.classification.classification", "qualname": "zarr_classification_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification.default_classification_trainer", "modulename": "torch_em.classification.classification", "qualname": "default_classification_trainer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span>,</span><span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\tlogger=&lt;class &#x27;torch_em.classification.classification_logger.ClassificationLogger&#x27;&gt;,</span><span class=\"param\">\ttrainer_class=&lt;class &#x27;torch_em.classification.classification_trainer.ClassificationTrainer&#x27;&gt;,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification_dataset", "modulename": "torch_em.classification.classification_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><code>~torch.utils.data.DataLoader</code> by default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset.__init__", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">normalization</span>, </span><span class=\"param\"><span class=\"n\">augmentation</span>, </span><span class=\"param\"><span class=\"n\">image_shape</span></span>)</span>"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset.data", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset.data", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset.target", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset.target", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset.normalization", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset.normalization", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset.augmentation", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset.augmentation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset.image_shape", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset.image_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_dataset.ClassificationDataset.resize", "modulename": "torch_em.classification.classification_dataset", "qualname": "ClassificationDataset.resize", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification_logger", "modulename": "torch_em.classification.classification_logger", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_logger.confusion_matrix", "modulename": "torch_em.classification.classification_logger", "qualname": "confusion_matrix", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred</span>,</span><span class=\"param\">\t<span class=\"n\">class_labels</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">title</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">plot_kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification_logger.make_grid", "modulename": "torch_em.classification.classification_logger", "qualname": "make_grid", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">images</span>, </span><span class=\"param\"><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">prediction</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">images_per_row</span><span class=\"o\">=</span><span class=\"mi\">8</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger", "kind": "class", "doc": "<p></p>\n", "bases": "torch_em.trainer.logger_base.TorchEmLogger"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger.__init__", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">save_root</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">unused_kwargs</span></span>)</span>"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger.log_dir", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger.log_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger.tb", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger.tb", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger.log_image_interval", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger.log_image_interval", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger.add_image", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger.add_image", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">step</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger.log_train", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger.log_train", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">lr</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">log_gradients</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification_logger.ClassificationLogger.log_validation", "modulename": "torch_em.classification.classification_logger", "qualname": "ClassificationLogger.log_validation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">metric</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">y_true</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">y_pred</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.classification.classification_trainer", "modulename": "torch_em.classification.classification_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.classification.classification_trainer.ClassificationTrainer", "modulename": "torch_em.classification.classification_trainer", "qualname": "ClassificationTrainer", "kind": "class", "doc": "<p>Trainer class for 2d/3d training on a single GPU.</p>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "torch_em.cli", "modulename": "torch_em.cli", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.cli.train_2d_unet", "modulename": "torch_em.cli", "qualname": "train_2d_unet", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.cli.train_3d_unet", "modulename": "torch_em.cli", "qualname": "train_3d_unet", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.cli.predict", "modulename": "torch_em.cli", "qualname": "predict", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.cli.predict_with_tiling", "modulename": "torch_em.cli", "qualname": "predict_with_tiling", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data", "modulename": "torch_em.data", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.concat_dataset", "modulename": "torch_em.data.concat_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.concat_dataset.ConcatDataset", "modulename": "torch_em.data.concat_dataset", "qualname": "ConcatDataset", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><code>~torch.utils.data.DataLoader</code> by default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.data.concat_dataset.ConcatDataset.__init__", "modulename": "torch_em.data.concat_dataset", "qualname": "ConcatDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span>)</span>"}, {"fullname": "torch_em.data.concat_dataset.ConcatDataset.datasets", "modulename": "torch_em.data.concat_dataset", "qualname": "ConcatDataset.datasets", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.concat_dataset.ConcatDataset.ndim", "modulename": "torch_em.data.concat_dataset", "qualname": "ConcatDataset.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.concat_dataset.ConcatDataset.ds_lens", "modulename": "torch_em.data.concat_dataset", "qualname": "ConcatDataset.ds_lens", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.concat_dataset.ConcatDataset.ds_offsets", "modulename": "torch_em.data.concat_dataset", "qualname": "ConcatDataset.ds_offsets", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.dataset_wrapper", "modulename": "torch_em.data.dataset_wrapper", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.dataset_wrapper.DatasetWrapper", "modulename": "torch_em.data.dataset_wrapper", "qualname": "DatasetWrapper", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><code>~torch.utils.data.DataLoader</code> by default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.data.dataset_wrapper.DatasetWrapper.__init__", "modulename": "torch_em.data.dataset_wrapper", "qualname": "DatasetWrapper.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>, </span><span class=\"param\"><span class=\"n\">wrap_item</span><span class=\"p\">:</span> <span class=\"n\">Callable</span></span>)</span>"}, {"fullname": "torch_em.data.dataset_wrapper.DatasetWrapper.dataset", "modulename": "torch_em.data.dataset_wrapper", "qualname": "DatasetWrapper.dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.dataset_wrapper.DatasetWrapper.wrap_item", "modulename": "torch_em.data.dataset_wrapper", "qualname": "DatasetWrapper.wrap_item", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets", "modulename": "torch_em.data.datasets", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy", "modulename": "torch_em.data.datasets.electron_microscopy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.asem", "modulename": "torch_em.data.datasets.electron_microscopy.asem", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.asem.INCONSISTENT_VOLUMES", "modulename": "torch_em.data.datasets.electron_microscopy.asem", "qualname": "INCONSISTENT_VOLUMES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;mito&#x27;: [&#x27;cell_6.zarr&#x27;, &#x27;cell_13.zarr&#x27;, &#x27;cell_13a.zarr&#x27;], &#x27;golgi&#x27;: [&#x27;cell_3.zarr&#x27;, &#x27;cell_6.zarr&#x27;], &#x27;er&#x27;: [&#x27;cell_3.zarr&#x27;, &#x27;cell_6.zarr&#x27;, &#x27;cell_13.zarr&#x27;]}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.asem.VOLUMES", "modulename": "torch_em.data.datasets.electron_microscopy.asem", "qualname": "VOLUMES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;cell_1&#x27;: &#x27;cell_1/cell_1.zarr&#x27;, &#x27;cell_2&#x27;: &#x27;cell_2/cell_2.zarr&#x27;, &#x27;cell_3&#x27;: &#x27;cell_3/cell_3.zarr&#x27;, &#x27;cell_6&#x27;: &#x27;cell_6/cell_6.zarr&#x27;, &#x27;cell_12&#x27;: &#x27;cell_12/cell_12.zarr&#x27;, &#x27;cell_13&#x27;: &#x27;cell_13/cell_13.zarr&#x27;, &#x27;cell_13a&#x27;: &#x27;cell_13a/cell_13a.zarr&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.asem.ORGANELLES", "modulename": "torch_em.data.datasets.electron_microscopy.asem", "qualname": "ORGANELLES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;mito&#x27;: [&#x27;cell_1&#x27;, &#x27;cell_2&#x27;, &#x27;cell_3&#x27;, &#x27;cell_6&#x27;, &#x27;cell_13&#x27;, &#x27;cell_13a&#x27;], &#x27;golgi&#x27;: [&#x27;cell_1&#x27;, &#x27;cell_2&#x27;, &#x27;cell_3&#x27;, &#x27;cell_6&#x27;], &#x27;er&#x27;: [&#x27;cell_1&#x27;, &#x27;cell_2&#x27;, &#x27;cell_3&#x27;, &#x27;cell_6&#x27;], &#x27;ccp&#x27;: [&#x27;cell_12&#x27;, &#x27;cell_13&#x27;], &#x27;np&#x27;: [&#x27;cell_13a&#x27;], &#x27;np_bottom&#x27;: [&#x27;cell_13a&#x27;]}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.asem.get_asem_data", "modulename": "torch_em.data.datasets.electron_microscopy.asem", "qualname": "get_asem_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.asem.get_asem_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.asem", "qualname": "get_asem_dataset", "kind": "function", "doc": "<p>Dataset for the segmentation of organelles in FIB-SEM cells.</p>\n\n<p>This dataset provides access to 3d images of organelles (mitochondria, golgi, endoplasmic reticulum)\nsegmented in cells. If you use this data in your research, please cite: <a href=\"https://doi.org/10.1083/jcb.202208005\">https://doi.org/10.1083/jcb.202208005</a></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span>,</span><span class=\"param\">\t<span class=\"n\">download</span>,</span><span class=\"param\">\t<span class=\"n\">organelles</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">volume_ids</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.asem.get_asem_loader", "modulename": "torch_em.data.datasets.electron_microscopy.asem", "qualname": "get_asem_loader", "kind": "function", "doc": "<p>Dataloader for organelle segmentation in FIB-SEM cells. See <code>get_asem_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">organelles</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">volume_ids</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.axondeepseg", "modulename": "torch_em.data.datasets.electron_microscopy.axondeepseg", "kind": "module", "doc": "<p>AxonDeepSeg is a dataset for the segmentation of myelinated axons in EM.\nIt contains two different data types: TEM and SEM.\nThe dataset was published in <a href=\"https://doi.org/10.1038/s41598-018-22181-4\">https://doi.org/10.1038/s41598-018-22181-4</a>.\nPlease cite this publication if you use the dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.axondeepseg.URLS", "modulename": "torch_em.data.datasets.electron_microscopy.axondeepseg", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;sem&#x27;: &#x27;https://github.com/axondeepseg/data_axondeepseg_sem/archive/refs/heads/master.zip&#x27;, &#x27;tem&#x27;: &#x27;https://osf.io/download/uewd9&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.axondeepseg.CHECKSUMS", "modulename": "torch_em.data.datasets.electron_microscopy.axondeepseg", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;sem&#x27;: &#x27;d334cbacf548f78ce8dd4a597bf86b884bd15a47a230a0ccc46e1ffa94d58426&#x27;, &#x27;tem&#x27;: &#x27;e4657280808f3b80d3bf1fba87d1cbbf2455f519baf1a7b16d2ddf2e54739a95&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.axondeepseg.get_axondeepseg_data", "modulename": "torch_em.data.datasets.electron_microscopy.axondeepseg", "qualname": "get_axondeepseg_data", "kind": "function", "doc": "<p>Download the axondeepseg data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  The name of the dataset to download. Can be either 'sem' or 'tem'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath for the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.axondeepseg.get_axondeepseg_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.axondeepseg", "qualname": "get_axondeepseg_dataset", "kind": "function", "doc": "<p>Get dataset for segmnetation of myelinated axons.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  The name of the dataset to download. Can be either 'sem' or 'tem'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>one_hot_encoding:</strong>  Whether to return the labels as one hot encoding.</li>\n<li><strong>val_fraction:</strong>  The fraction of the data to use for validation.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'val'.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">one_hot_encoding</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">val_fraction</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.axondeepseg.get_axondeepseg_loader", "modulename": "torch_em.data.datasets.electron_microscopy.axondeepseg", "qualname": "get_axondeepseg_loader", "kind": "function", "doc": "<p>Get dataloader for the segmentation of myelinated axons.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  The name of the dataset to download. Can be either 'sem' or 'tem'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>one_hot_encoding:</strong>  Whether to return the labels as one hot encoding.</li>\n<li><strong>val_fraction:</strong>  The fraction of the data to use for validation.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'val'.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PyTorch DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">one_hot_encoding</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">val_fraction</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "kind": "module", "doc": "<p>The CEM, or MitoLab, dataset is a collection of data for\ntraining mitochondria generalist models. It consists of:</p>\n\n<ul>\n<li>CEM-MitoLab: annotated 2d data for training mitochondria segmentation models\n<ul>\n<li><a href=\"https://www.ebi.ac.uk/empiar/EMPIAR-11037/\"><a href=\"https://www.ebi.ac.uk/empiar/EMPIAR-11037/\">https://www.ebi.ac.uk/empiar/EMPIAR-11037/</a></a></li>\n</ul></li>\n<li>CEM-Mito-Benchmark: 7 Benchmark datasets for mitochondria segmentation\n<ul>\n<li><a href=\"https://www.ebi.ac.uk/empiar/EMPIAR-10982/\"><a href=\"https://www.ebi.ac.uk/empiar/EMPIAR-10982/\">https://www.ebi.ac.uk/empiar/EMPIAR-10982/</a></a></li>\n</ul></li>\n<li>CEM-1.5M: unlabeled EM images for pretraining: (Not yet implemented)\n<ul>\n<li><a href=\"https://www.ebi.ac.uk/empiar/EMPIAR-11035/\"><a href=\"https://www.ebi.ac.uk/empiar/EMPIAR-11035/\">https://www.ebi.ac.uk/empiar/EMPIAR-11035/</a></a></li>\n</ul></li>\n</ul>\n\n<p>These datasets are from the publication <a href=\"https://doi.org/10.1016/j.cels.2022.12.006\">https://doi.org/10.1016/j.cels.2022.12.006</a>.\nPlease cite this publication if you use this data in your research.</p>\n\n<p>The data itself can be downloaded from EMPIAR via aspera.</p>\n\n<ul>\n<li>You can install aspera via mamba. We recommend to do this in a separate environment\nto avoid dependency issues:\n<ul>\n<li><code>$ mamba create -c conda-forge -c hcc -n aspera aspera-cli</code></li>\n</ul></li>\n<li>After this you can run <code>$ mamba activate aspera</code> to have an environment with aspera installed.</li>\n<li>You can then download the data for one of the three datasets like this:\n<ul>\n<li>ascp -QT -l 200m -P33001 -i <PREFIX>/etc/asperaweb_id_dsa.openssh emp_ext2@fasp.ebi.ac.uk:/<EMPIAR_ID> <PATH></li>\n<li>Where <PREFIX> is the path to the mamba environment, <EMPIAR_ID> the id of one of the three datasets\nand <PATH> where you want to download the data.</li>\n</ul></li>\n<li>After this you can use the functions in this file if you use <PATH> as location for the data.</li>\n</ul>\n\n<p>Note that we have implemented automatic download, but this leads to dependency\nissues, so we recommend to download the data manually and then run the loaders with the correct path.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.BENCHMARK_DATASETS", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "BENCHMARK_DATASETS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{1: &#x27;mito_benchmarks/c_elegans&#x27;, 2: &#x27;mito_benchmarks/fly_brain&#x27;, 3: &#x27;mito_benchmarks/glycolytic_muscle&#x27;, 4: &#x27;mito_benchmarks/hela_cell&#x27;, 5: &#x27;mito_benchmarks/lucchi_pp&#x27;, 6: &#x27;mito_benchmarks/salivary_gland&#x27;, 7: &#x27;tem_benchmark&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.BENCHMARK_SHAPES", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "BENCHMARK_SHAPES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{1: (256, 256, 256), 2: (256, 255, 255), 3: (302, 383, 765), 4: (256, 256, 256), 5: (165, 768, 1024), 6: (1260, 1081, 1200), 7: (224, 224)}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_mitolab_data", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_mitolab_data", "kind": "function", "doc": "<p>Download the mitolab training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'val'.</li>\n<li><strong>val_fraction:</strong>  The fraction of the data to use for validation.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>discard_empty_images:</strong>  Whether to discard images without annotations.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>List of the image data paths.\n  List of the label data paths.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">val_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">discard_empty_images</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_benchmark_data", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_benchmark_data", "kind": "function", "doc": "<p>Download the mitolab benechmark data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>dataset_id:</strong>  The id of the benchmark dataset to download.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>List of the image data paths.\n  List of the label data paths.\n  The image data key.\n  The label data key.\n  Whether this is a segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_mitolab_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_mitolab_dataset", "kind": "function", "doc": "<p>Get the dataset for the mitolab training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'val'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>val_fraction:</strong>  The fraction of the data to use for validation.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>discard_empty_images:</strong>  Whether to discard images without annotations.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">val_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">discard_empty_images</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_cem15m_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_cem15m_dataset", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_benchmark_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_benchmark_dataset", "kind": "function", "doc": "<p>Get the dataset for one of the mitolab benchmark datasets.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>dataset_id:</strong>  The id of the benchmark dataset to download.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_id</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_mitolab_loader", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_mitolab_loader", "kind": "function", "doc": "<p>Get the dataloader for the mitolab training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'val'.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>discard_empty_images:</strong>  Whether to discard images without annotations.</li>\n<li><strong>val_fraction:</strong>  The fraction of the data to use for validation.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PyTorch DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">discard_empty_images</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">val_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_cem15m_loader", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_cem15m_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cem.get_benchmark_loader", "modulename": "torch_em.data.datasets.electron_microscopy.cem", "qualname": "get_benchmark_loader", "kind": "function", "doc": "<p>Get the datasloader for one of the mitolab benchmark datasets.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>dataset_id:</strong>  The id of the benchmark dataset to download.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cremi", "modulename": "torch_em.data.datasets.electron_microscopy.cremi", "kind": "module", "doc": "<p>CREMI is a dataset for neuron segmentation in EM.</p>\n\n<p>It contains three annotated volumes from the adult fruit-fly brain.\nIt was held as a challenge at MICCAI 2016. For details on the dataset check out <a href=\"https://cremi.org/\">https://cremi.org/</a>.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cremi.CREMI_URLS", "modulename": "torch_em.data.datasets.electron_microscopy.cremi", "qualname": "CREMI_URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;original&#x27;: {&#x27;A&#x27;: &#x27;https://cremi.org/static/data/sample_A_20160501.hdf&#x27;, &#x27;B&#x27;: &#x27;https://cremi.org/static/data/sample_B_20160501.hdf&#x27;, &#x27;C&#x27;: &#x27;https://cremi.org/static/data/sample_C_20160501.hdf&#x27;}, &#x27;realigned&#x27;: {}, &#x27;defects&#x27;: &#x27;https://zenodo.org/record/5767036/files/sample_ABC_padded_defects.h5&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cremi.CHECKSUMS", "modulename": "torch_em.data.datasets.electron_microscopy.cremi", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;original&#x27;: {&#x27;A&#x27;: &#x27;4c563d1b78acb2bcfb3ea958b6fe1533422f7f4a19f3e05b600bfa11430b510d&#x27;, &#x27;B&#x27;: &#x27;887e85521e00deead18c94a21ad71f278d88a5214c7edeed943130a1f4bb48b8&#x27;, &#x27;C&#x27;: &#x27;2874496f224d222ebc29d0e4753e8c458093e1d37bc53acd1b69b19ed1ae7052&#x27;}, &#x27;realigned&#x27;: {}, &#x27;defects&#x27;: &#x27;7b06ffa34733b2c32956ea5005e0cf345e7d3a27477f42f7c905701cdc947bd0&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cremi.get_cremi_data", "modulename": "torch_em.data.datasets.electron_microscopy.cremi", "qualname": "get_cremi_data", "kind": "function", "doc": "<p>Download the CREMI training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>samples:</strong>  The CREMI samples to use. The available samples are 'A', 'B', 'C'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>use_realigned:</strong>  Use the realigned instead of the original training data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepaths to the training data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_realigned</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cremi.get_cremi_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.cremi", "qualname": "get_cremi_dataset", "kind": "function", "doc": "<p>Get the CREMI dataset for the segmentation of neurons in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>samples:</strong>  The CREMI samples to use. The available samples are 'A', 'B', 'C'.</li>\n<li><strong>use_realigned:</strong>  Use the realigned instead of the original training data.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the samples.</li>\n<li><strong>defect_augmentation_kwargs:</strong>  Keyword arguments for defect augmentations.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s1\">&#39;A&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;B&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;C&#39;</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">use_realigned</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">defect_augmentation_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;p_drop_slice&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.025</span><span class=\"p\">,</span> <span class=\"s1\">&#39;p_low_contrast&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.025</span><span class=\"p\">,</span> <span class=\"s1\">&#39;p_deform_slice&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"s1\">&#39;deformation_mode&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;compress&#39;</span><span class=\"p\">}</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.cremi.get_cremi_loader", "modulename": "torch_em.data.datasets.electron_microscopy.cremi", "qualname": "get_cremi_loader", "kind": "function", "doc": "<p>Get the DataLoader for EM neuron segmentation in the CREMI dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>samples:</strong>  The CREMI samples to use. The available samples are 'A', 'B', 'C'.</li>\n<li><strong>use_realigned:</strong>  Use the realigned instead of the original training data.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the samples.</li>\n<li><strong>defect_augmentation_kwargs:</strong>  Keyword arguments for defect augmentations.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s1\">&#39;A&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;B&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;C&#39;</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">use_realigned</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">defect_augmentation_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;p_drop_slice&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.025</span><span class=\"p\">,</span> <span class=\"s1\">&#39;p_low_contrast&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.025</span><span class=\"p\">,</span> <span class=\"s1\">&#39;p_deform_slice&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"s1\">&#39;deformation_mode&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;compress&#39;</span><span class=\"p\">}</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.isbi2012", "modulename": "torch_em.data.datasets.electron_microscopy.isbi2012", "kind": "module", "doc": "<p>The ISBI2012 dataset was the first neuron segmentation challenge, held at the ISBI 2012 competition.</p>\n\n<p>It contains a small annotated EM volume from the fruit-fly brain. If you use this dataset in\nyour research please cite the following publication: <a href=\"https://doi.org/10.3389/fnana.2015.00142\">https://doi.org/10.3389/fnana.2015.00142</a>.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.isbi2012.ISBI_URL", "modulename": "torch_em.data.datasets.electron_microscopy.isbi2012", "qualname": "ISBI_URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://oc.embl.de/index.php/s/h0TkwqxU0PJDdMd/download&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.isbi2012.CHECKSUM", "modulename": "torch_em.data.datasets.electron_microscopy.isbi2012", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;0e10fe909a1243084d91773470856993b7d40126a12e85f0f1345a7a9e512f29&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.isbi2012.get_isbi_data", "modulename": "torch_em.data.datasets.electron_microscopy.isbi2012", "qualname": "get_isbi_data", "kind": "function", "doc": "<p>Download the ISBI2012 dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path to the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.isbi2012.get_isbi_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.isbi2012", "qualname": "get_isbi_dataset", "kind": "function", "doc": "<p>Get the dataset for EM neuron segmentation in ISBI 2012.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>use_original_labels:</strong>  Whether to use the original annotations or postprocessed 3d annotations.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_original_labels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.isbi2012.get_isbi_loader", "modulename": "torch_em.data.datasets.electron_microscopy.isbi2012", "qualname": "get_isbi_loader", "kind": "function", "doc": "<p>Get the DataLoader for EM neuron segmentation in ISBI 2012.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>use_original_labels:</strong>  Whether to use the original annotations or postprocessed 3d annotations.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_original_labels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.kasthuri", "modulename": "torch_em.data.datasets.electron_microscopy.kasthuri", "kind": "module", "doc": "<p>The Kasthuri dataset is a segmentation dataset for mitochondrion segmentation in electron microscopy.</p>\n\n<p>The dataset was published in <a href=\"https://doi.org/10.48550/arXiv.1812.06024\">https://doi.org/10.48550/arXiv.1812.06024</a>.\nPlease cite this publication if you use the dataset in your research.\nWe use the version of the dataset from <a href=\"https://sites.google.com/view/connectomics/\">https://sites.google.com/view/connectomics/</a>.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.kasthuri.URL", "modulename": "torch_em.data.datasets.electron_microscopy.kasthuri", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;http://www.casser.io/files/kasthuri_pp.zip &#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.kasthuri.CHECKSUM", "modulename": "torch_em.data.datasets.electron_microscopy.kasthuri", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;bbb78fd205ec9b57feb8f93ebbdf1666261cbc3e0305e7f11583ab5157a3d792&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.kasthuri.get_kasthuri_data", "modulename": "torch_em.data.datasets.electron_microscopy.kasthuri", "qualname": "get_kasthuri_data", "kind": "function", "doc": "<p>Download the kasthuri dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath for the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.kasthuri.get_kasthuri_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.kasthuri", "qualname": "get_kasthuri_dataset", "kind": "function", "doc": "<p>Get dataset for EM mitochondrion segmentation in the kasthuri dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.kasthuri.get_kasthuri_loader", "modulename": "torch_em.data.datasets.electron_microscopy.kasthuri", "qualname": "get_kasthuri_loader", "kind": "function", "doc": "<p>Get dataloader for EM mitochondrion segmentation in the kasthuri dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PyTorch DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.lucchi", "modulename": "torch_em.data.datasets.electron_microscopy.lucchi", "kind": "module", "doc": "<p>The Lucchi dataset is a segmentation dataset for mitochondrion segmentation in electron microscopy.</p>\n\n<p>The dataset was published in <a href=\"https://doi.org/10.48550/arXiv.1812.06024\">https://doi.org/10.48550/arXiv.1812.06024</a>.\nPlease cite this publication if you use the dataset in your research.\nWe use the version of the dataset from <a href=\"https://sites.google.com/view/connectomics/\">https://sites.google.com/view/connectomics/</a>.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.lucchi.URL", "modulename": "torch_em.data.datasets.electron_microscopy.lucchi", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;http://www.casser.io/files/lucchi_pp.zip&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.lucchi.CHECKSUM", "modulename": "torch_em.data.datasets.electron_microscopy.lucchi", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;770ce9e98fc6f29c1b1a250c637e6c5125f2b5f1260e5a7687b55a79e2e8844d&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.lucchi.get_lucchi_data", "modulename": "torch_em.data.datasets.electron_microscopy.lucchi", "qualname": "get_lucchi_data", "kind": "function", "doc": "<p>Download the lucchi dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The split to download, either 'train' or 'test'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath for the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.lucchi.get_lucchi_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.lucchi", "qualname": "get_lucchi_dataset", "kind": "function", "doc": "<p>Get dataset for EM mitochondrion segmentation in the lucchi dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.lucchi.get_lucchi_loader", "modulename": "torch_em.data.datasets.electron_microscopy.lucchi", "qualname": "get_lucchi_loader", "kind": "function", "doc": "<p>Get dataloader for EM mitochondrion segmentation in the lucchi dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split. Either 'train' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PyTorch DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.mitoem", "modulename": "torch_em.data.datasets.electron_microscopy.mitoem", "kind": "module", "doc": "<p>MitoEM is a dataset for segmenting mitochondria in electron microscopy.</p>\n\n<p>It contains two large annotated volumes, one from rat cortex, the other from human cortex.\nThis dataset was used for a segmentation challenge at ISBI 2022.\nIf you use it in your research then please cite <a href=\"https://doi.org/10.1007/978-3-030-59722-1_7\">https://doi.org/10.1007/978-3-030-59722-1_7</a>.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.mitoem.URLS", "modulename": "torch_em.data.datasets.electron_microscopy.mitoem", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;raw&#x27;: {&#x27;human&#x27;: &#x27;https://www.dropbox.com/s/z41qtu4y735j95e/EM30-H-im.zip?dl=1&#x27;, &#x27;rat&#x27;: &#x27;https://huggingface.co/datasets/pytc/EM30/resolve/main/EM30-R-im.zip&#x27;}, &#x27;labels&#x27;: {&#x27;human&#x27;: &#x27;https://www.dropbox.com/s/dhf89bc14kemw4e/EM30-H-mito-train-val-v2.zip?dl=1&#x27;, &#x27;rat&#x27;: &#x27;https://huggingface.co/datasets/pytc/MitoEM/blob/main/EM30-R-mito-train-val-v2.zip&#x27;}}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.mitoem.CHECKSUMS", "modulename": "torch_em.data.datasets.electron_microscopy.mitoem", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;raw&#x27;: {&#x27;human&#x27;: &#x27;98fe259f36a7d8d43f99981b7a0ef8cdeba2ce2615ff91595f428ae57207a041&#x27;, &#x27;rat&#x27;: &#x27;6a2cac68adde5d01984542d3ee1d7753d1fa3e6eb2a042ce15ce297c95885bbe&#x27;}, &#x27;labels&#x27;: {&#x27;human&#x27;: &#x27;0e8ed292cfcd0c58701d9f4299244a1b66d6aeb506c85754c34f98a4eda0ef1b&#x27;, &#x27;rat&#x27;: &#x27;c56380ac575428a818bd293ca3509d1249999846c3702ccbf11d308acdd2ae86&#x27;}}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.mitoem.get_slices", "modulename": "torch_em.data.datasets.electron_microscopy.mitoem", "qualname": "get_slices", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">folder</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.mitoem.get_mitoem_data", "modulename": "torch_em.data.datasets.electron_microscopy.mitoem", "qualname": "get_mitoem_data", "kind": "function", "doc": "<p>Download the MitoEM training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>samples:</strong>  The samples to download. The available samples are 'human' and 'rat'.</li>\n<li><strong>splits:</strong>  The data splits to download. The available splits are 'train', 'val' and 'test'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The paths to the downloaded and converted files.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">splits</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.mitoem.get_mitoem_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.mitoem", "qualname": "get_mitoem_dataset", "kind": "function", "doc": "<p>Get the MitoEM dataset for the segmentation of mitochondria in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>splits:</strong>  The splits to use for the dataset. Available values are 'train', 'val' and 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>samples:</strong>  The samples to use for the dataset. The available samples are 'human' and 'rat'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">splits</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s1\">&#39;human&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;rat&#39;</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.mitoem.get_mitoem_loader", "modulename": "torch_em.data.datasets.electron_microscopy.mitoem", "qualname": "get_mitoem_loader", "kind": "function", "doc": "<p>Get the MitoEM dataload for the segmentation of mitochondria in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>splits:</strong>  The splits to use for the dataset. Available values are 'train', 'val' and 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>samples:</strong>  The samples to use for the dataset. The available samples are 'human' and 'rat'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">splits</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s1\">&#39;human&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;rat&#39;</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.nuc_mm", "modulename": "torch_em.data.datasets.electron_microscopy.nuc_mm", "kind": "module", "doc": "<p>NucMM is a dataset for the segmentation of nuclei in EM and X-Ray.</p>\n\n<p>This dataset is from the publication <a href=\"https://doi.org/10.1007/978-3-030-87193-2_16\">https://doi.org/10.1007/978-3-030-87193-2_16</a>.\nPlease cite it if you use this dataset for a publication.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.nuc_mm.URL", "modulename": "torch_em.data.datasets.electron_microscopy.nuc_mm", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://drive.google.com/drive/folders/1_4CrlYvzx0ITnGlJOHdgcTRgeSkm9wT8&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.nuc_mm.get_nuc_mm_data", "modulename": "torch_em.data.datasets.electron_microscopy.nuc_mm", "qualname": "get_nuc_mm_data", "kind": "function", "doc": "<p>Download the NucMM training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>sample:</strong>  The NucMM samples to use. The available samples are 'mouse' and 'zebrafish'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath to the training data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.nuc_mm.get_nuc_mm_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.nuc_mm", "qualname": "get_nuc_mm_dataset", "kind": "function", "doc": "<p>Get the NucMM dataset for the segmentation of nuclei in X-Ray and EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>sample:</strong>  The CREMI samples to use. The available samples are 'A', 'B', 'C'.</li>\n<li><strong>split:</strong>  The split for the dataset, either 'train' or 'val'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.nuc_mm.get_nuc_mm_loader", "modulename": "torch_em.data.datasets.electron_microscopy.nuc_mm", "qualname": "get_nuc_mm_loader", "kind": "function", "doc": "<p>Get the NucMM dataset for the segmentation of nuclei in X-Ray and EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>sample:</strong>  The CREMI samples to use. The available samples are 'A', 'B', 'C'.</li>\n<li><strong>split:</strong>  The split for the dataset, either 'train' or 'val'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "kind": "module", "doc": "<p>Dataset for the segmentation of different structures in EM volume of a\nplatynereis larve. Contains annotations for the segmentation of:</p>\n\n<ul>\n<li>Cuticle</li>\n<li>Cilia</li>\n<li>Cells</li>\n<li>Nuclei</li>\n</ul>\n\n<p>This dataset is from the publication <a href=\"https://doi.org/10.1016/j.cell.2021.07.017\">https://doi.org/10.1016/j.cell.2021.07.017</a>.\nPlease cite it if you use this dataset for a publication.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.URLS", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;cells&#x27;: &#x27;https://zenodo.org/record/3675220/files/membrane.zip&#x27;, &#x27;nuclei&#x27;: &#x27;https://zenodo.org/record/3675220/files/nuclei.zip&#x27;, &#x27;cilia&#x27;: &#x27;https://zenodo.org/record/3675220/files/cilia.zip&#x27;, &#x27;cuticle&#x27;: &#x27;https://zenodo.org/record/3675220/files/cuticle.zip&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.CHECKSUMS", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;cells&#x27;: &#x27;30eb50c39e7e9883e1cd96e0df689fac37a56abb11e8ed088907c94a5980d6a3&#x27;, &#x27;nuclei&#x27;: &#x27;a05033c5fbc6a3069479ac6595b0a430070f83f5281f5b5c8913125743cf5510&#x27;, &#x27;cilia&#x27;: &#x27;6d2b47f63d39a671789c02d8b66cad5e4cf30eb14cdb073da1a52b7defcc5e24&#x27;, &#x27;cuticle&#x27;: &#x27;464f75d30133e8864958049647fe3c2216ddf2d4327569738ad72d299c991843&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platy_data", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platy_data", "kind": "function", "doc": "<p>Download the platynereis dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  Name of the segmentation task. Available tasks: 'cuticle', 'cilia', 'cells' or 'nuclei'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path to the folder where the data has been downloaded.\n  The number of files downloaded.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_cuticle_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_cuticle_dataset", "kind": "function", "doc": "<p>Get the dataset for cuticle segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_cuticle_loader", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_cuticle_loader", "kind": "function", "doc": "<p>Get the dataloader for cuticle segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_cilia_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_cilia_dataset", "kind": "function", "doc": "<p>Get the dataset for cilia segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_cilia_loader", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_cilia_loader", "kind": "function", "doc": "<p>Get the dataloader for cilia segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_cell_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_cell_dataset", "kind": "function", "doc": "<p>Get the dataset for cell segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_cell_loader", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_cell_loader", "kind": "function", "doc": "<p>Get the dataloader for cell segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_nuclei_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_nuclei_dataset", "kind": "function", "doc": "<p>Get the dataset for nucleus segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.platynereis.get_platynereis_nuclei_loader", "modulename": "torch_em.data.datasets.electron_microscopy.platynereis", "qualname": "get_platynereis_nuclei_loader", "kind": "function", "doc": "<p>Get the dataloader for nucleus segmentation in platynereis.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>sample_ids:</strong>  The sample ids to use for the dataset</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>rois:</strong>  The region of interests to use for the data blocks.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.snemi", "modulename": "torch_em.data.datasets.electron_microscopy.snemi", "kind": "module", "doc": "<p>SNEMI is a dataset for neuron segmentation in EM.</p>\n\n<p>It contains an annotated volumes from the mouse brain.\nThe data is part of the publication <a href=\"https://doi.org/10.1016/j.cell.2015.06.054\">https://doi.org/10.1016/j.cell.2015.06.054</a>.\nPlease cite it if you use this dataset for a publication.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.snemi.SNEMI_URLS", "modulename": "torch_em.data.datasets.electron_microscopy.snemi", "qualname": "SNEMI_URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;https://oc.embl.de/index.php/s/43iMotlXPyAB39z/download&#x27;, &#x27;test&#x27;: &#x27;https://oc.embl.de/index.php/s/aRhphk35H23De2s/download&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.snemi.CHECKSUMS", "modulename": "torch_em.data.datasets.electron_microscopy.snemi", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;5b130a24d9eb23d972fede0f1a403bc05f6808b361cfa22eff23b930b12f0615&#x27;, &#x27;test&#x27;: &#x27;3df3920a0ddec6897105845f842b2665d37a47c2d1b96d4f4565682e315a59fa&#x27;}"}, {"fullname": "torch_em.data.datasets.electron_microscopy.snemi.get_snemi_data", "modulename": "torch_em.data.datasets.electron_microscopy.snemi", "qualname": "get_snemi_data", "kind": "function", "doc": "<p>Download the SNEMI training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>sample:</strong>  The sample to download, either 'train' or 'test'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path to the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.snemi.get_snemi_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.snemi", "qualname": "get_snemi_dataset", "kind": "function", "doc": "<p>Get the SNEMI dataset for the segmentation of neurons in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>sample:</strong>  The sample to download, either 'train' or 'test'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.snemi.get_snemi_loader", "modulename": "torch_em.data.datasets.electron_microscopy.snemi", "qualname": "get_snemi_loader", "kind": "function", "doc": "<p>Get the DataLoader for EM neuron segmentation in the SNEMI dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>sample:</strong>  The sample to download, either 'train' or 'test'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.sponge_em", "modulename": "torch_em.data.datasets.electron_microscopy.sponge_em", "kind": "module", "doc": "<p>This dataset contains volume EM data of a sponge chamber with\nsegmentation annotations for cells, cilia and microvilli.</p>\n\n<p>It contains three annotated volumes. The dataset is part of the publication\n<a href=\"https://doi.org/10.1126/science.abj2949\">https://doi.org/10.1126/science.abj2949</a>. Please cite this publication of you use the\ndataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.sponge_em.URL", "modulename": "torch_em.data.datasets.electron_microscopy.sponge_em", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://zenodo.org/record/8150818/files/sponge_em_train_data.zip?download=1&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.sponge_em.CHECKSUM", "modulename": "torch_em.data.datasets.electron_microscopy.sponge_em", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;f1df616cd60f81b91d7642933e9edd74dc6c486b2e546186a7c1e54c67dd32a5&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.sponge_em.get_sponge_em_data", "modulename": "torch_em.data.datasets.electron_microscopy.sponge_em", "qualname": "get_sponge_em_data", "kind": "function", "doc": "<p>Download the SpongeEM training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path to the downloaded data.\n  The number of downloaded volumes.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.sponge_em.get_sponge_em_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.sponge_em", "qualname": "get_sponge_em_dataset", "kind": "function", "doc": "<p>Get the SpongeEM dataset for the segmentation of structures in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>mode:</strong>  Choose the segmentation task, either 'semantic' or 'instances'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>sample_ids:</strong>  The sample to download, valid ids are 1, 2 and 3.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.sponge_em.get_sponge_em_loader", "modulename": "torch_em.data.datasets.electron_microscopy.sponge_em", "qualname": "get_sponge_em_loader", "kind": "function", "doc": "<p>Get the SpongeEM dataloader for the segmentation of structures in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>mode:</strong>  Choose the segmentation task, either 'semantic' or 'instances'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>sample_ids:</strong>  The sample to download, valid ids are 1, 2 and 3.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sample_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.uro_cell", "modulename": "torch_em.data.datasets.electron_microscopy.uro_cell", "kind": "module", "doc": "<p>The UroCell dataset contains segmentation annotations for the following organelles:</p>\n\n<ul>\n<li>Food Vacuoles</li>\n<li>Golgi Apparatus</li>\n<li>Lysosomes</li>\n<li>Mitochondria</li>\n</ul>\n\n<p>It contains several FIB-SEM volumes with annotations.\nThis dataset is from the publication <a href=\"https://doi.org/10.1016/j.compbiomed.2020.103693\">https://doi.org/10.1016/j.compbiomed.2020.103693</a>.\nPlease cite it if you use this dataset for a publication.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.uro_cell.URL", "modulename": "torch_em.data.datasets.electron_microscopy.uro_cell", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://github.com/MancaZerovnikMekuc/UroCell/archive/refs/heads/master.zip&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.uro_cell.CHECKSUM", "modulename": "torch_em.data.datasets.electron_microscopy.uro_cell", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;a48cf31b06114d7def642742b4fcbe76103483c069122abe10f377d71a1acabc&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.uro_cell.get_urocell_data", "modulename": "torch_em.data.datasets.electron_microscopy.uro_cell", "qualname": "get_urocell_data", "kind": "function", "doc": "<p>Download the UroCell training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path to the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.uro_cell.get_uro_cell_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.uro_cell", "qualname": "get_uro_cell_dataset", "kind": "function", "doc": "<p>Get the UroCell dataset for organelle segmentation in FIB-SEM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>target:</strong>  The segmentation target, corresponding to the organelle to segment.\nAvailable organelles are 'fv', 'golgi', 'lyso' and 'mito'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.uro_cell.get_uro_cell_loader", "modulename": "torch_em.data.datasets.electron_microscopy.uro_cell", "qualname": "get_uro_cell_loader", "kind": "function", "doc": "<p>Get the UroCell dataloader for organelle segmentation in FIB-SEM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>target:</strong>  The segmentation target, corresponding to the organelle to segment.\nAvailable organelles are 'fv', 'golgi', 'lyso' and 'mito'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.vnc", "modulename": "torch_em.data.datasets.electron_microscopy.vnc", "kind": "module", "doc": "<p>The VNC dataset contains segmentation annotations for mitochondria in EM.</p>\n\n<p>It contains two volumes from TEM of the drosophila brain.\nPlease cite <a href=\"https://doi.org/10.6084/m9.figshare.856713.v1\">https://doi.org/10.6084/m9.figshare.856713.v1</a> if you use this dataset in your publication.</p>\n"}, {"fullname": "torch_em.data.datasets.electron_microscopy.vnc.URL", "modulename": "torch_em.data.datasets.electron_microscopy.vnc", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://github.com/unidesigner/groundtruth-drosophila-vnc/archive/refs/heads/master.zip&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.vnc.CHECKSUM", "modulename": "torch_em.data.datasets.electron_microscopy.vnc", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;f7bd0db03c86b64440a16b60360ad60c0a4411f89e2c021c7ee2c8d6af3d7e86&#x27;"}, {"fullname": "torch_em.data.datasets.electron_microscopy.vnc.get_vnc_data", "modulename": "torch_em.data.datasets.electron_microscopy.vnc", "qualname": "get_vnc_data", "kind": "function", "doc": "<p>Download the VNC training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path to the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.vnc.get_vnc_mito_dataset", "modulename": "torch_em.data.datasets.electron_microscopy.vnc", "qualname": "get_vnc_mito_dataset", "kind": "function", "doc": "<p>Get the VNC dataset for segmentating mitochondria in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.vnc.get_vnc_mito_loader", "modulename": "torch_em.data.datasets.electron_microscopy.vnc", "qualname": "get_vnc_mito_loader", "kind": "function", "doc": "<p>Get the VNC dataloader for segmentating mitochondria in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to return a binary segmentation target.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.electron_microscopy.vnc.get_vnc_neuron_loader", "modulename": "torch_em.data.datasets.electron_microscopy.vnc", "qualname": "get_vnc_neuron_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">patch_shape</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology", "modulename": "torch_em.data.datasets.histopathology", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.histopathology.bcss", "modulename": "torch_em.data.datasets.histopathology.bcss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.histopathology.bcss.URL", "modulename": "torch_em.data.datasets.histopathology.bcss", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://drive.google.com/drive/folders/1zqbdkQF8i5cEmZOGmbdQm-EP8dRYtvss?usp=sharing&#x27;"}, {"fullname": "torch_em.data.datasets.histopathology.bcss.CHECKSUM", "modulename": "torch_em.data.datasets.histopathology.bcss", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "None"}, {"fullname": "torch_em.data.datasets.histopathology.bcss.TEST_LIST", "modulename": "torch_em.data.datasets.histopathology.bcss", "qualname": "TEST_LIST", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;TCGA-A2-A0SX-DX1_xmin53791_ymin56683_MPP-0.2500&#x27;, &#x27;TCGA-BH-A0BG-DX1_xmin64019_ymin24975_MPP-0.2500&#x27;, &#x27;TCGA-AR-A1AI-DX1_xmin38671_ymin10616_MPP-0.2500&#x27;, &#x27;TCGA-E2-A574-DX1_xmin54962_ymin47475_MPP-0.2500&#x27;, &#x27;TCGA-GM-A3XL-DX1_xmin29910_ymin15820_MPP-0.2500&#x27;, &#x27;TCGA-E2-A14X-DX1_xmin88836_ymin66393_MPP-0.2500&#x27;, &#x27;TCGA-A2-A04P-DX1_xmin104246_ymin48517_MPP-0.2500&#x27;, &#x27;TCGA-E2-A14N-DX1_xmin21383_ymin66838_MPP-0.2500&#x27;, &#x27;TCGA-EW-A1OV-DX1_xmin126026_ymin65132_MPP-0.2500&#x27;, &#x27;TCGA-S3-AA15-DX1_xmin55486_ymin28926_MPP-0.2500&#x27;, &#x27;TCGA-LL-A5YO-DX1_xmin36631_ymin44396_MPP-0.2500&#x27;, &#x27;TCGA-GI-A2C9-DX1_xmin20882_ymin11843_MPP-0.2500&#x27;, &#x27;TCGA-BH-A0BW-DX1_xmin42346_ymin30843_MPP-0.2500&#x27;, &#x27;TCGA-E2-A1B6-DX1_xmin16266_ymin50634_MPP-0.2500&#x27;, &#x27;TCGA-AO-A0J2-DX1_xmin33561_ymin14515_MPP-0.2500&#x27;]"}, {"fullname": "torch_em.data.datasets.histopathology.bcss.get_bcss_dataset", "modulename": "torch_em.data.datasets.histopathology.bcss", "qualname": "get_bcss_dataset", "kind": "function", "doc": "<p>Dataset for breast cancer tissue segmentation in histopathology.</p>\n\n<p>This dataset is from <a href=\"https://bcsegmentation.grand-challenge.org/BCSS/\">https://bcsegmentation.grand-challenge.org/BCSS/</a>.\nPlease cite this paper (<a href=\"https://doi.org/10.1093/bioinformatics/btz083\">https://doi.org/10.1093/bioinformatics/btz083</a>) if you use this dataset for a publication.</p>\n\n<p>NOTE: There are multiple semantic instances in tissue labels. Below mentioned are their respective index details:\n    - 0: outside_roi (~background)\n    - 1: tumor\n    - 2: stroma\n    - 3: lymphocytic_infiltrate\n    - 4: necrosis_or_debris\n    - 5: glandular_secretions\n    - 6: blood\n    - 7: exclude\n    - 8: metaplasia_NOS\n    - 9: fat\n    - 10: plasma_cells\n    - 11: other_immune_infiltrate\n    - 12: mucoid_material\n    - 13: normal_acinus_or_duct\n    - 14: lymphatics\n    - 15: undetermined\n    - 16: nerve\n    - 17: skin_adnexa\n    - 18: blood_vessel\n    - 19: angioinvasion\n    - 20: dcis\n    - 21: other</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">val_fraction</span><span class=\"o\">=</span><span class=\"mf\">0.2</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int64</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.bcss.get_bcss_loader", "modulename": "torch_em.data.datasets.histopathology.bcss", "qualname": "get_bcss_loader", "kind": "function", "doc": "<p>Dataloader for breast cancer tissue segmentation in histopathology. See <code>get_bcss_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">val_fraction</span><span class=\"o\">=</span><span class=\"mf\">0.2</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int64</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.lizard", "modulename": "torch_em.data.datasets.histopathology.lizard", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.URL1", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "URL1", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://warwick.ac.uk/fac/cross_fac/tia/data/lizard/lizard_images1.zip&#x27;"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.URL2", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "URL2", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://warwick.ac.uk/fac/cross_fac/tia/data/lizard/lizard_images2.zip&#x27;"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.LABEL_URL", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "LABEL_URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://warwick.ac.uk/fac/cross_fac/tia/data/lizard/lizard_labels.zip&#x27;"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.CHECKSUM1", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "CHECKSUM1", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;d2c4e7c83dff634624c9c14d4a1a0b821d4e9ac41e05e3b36303d8f0c510113d&#x27;"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.CHECKSUM2", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "CHECKSUM2", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;9f529f30d9de66587167991a8bf75aaad07ce1d518b72e825c868ac7c33015ed&#x27;"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.LABEL_CHECKSUM", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "LABEL_CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;79f22ca83ca535682fba340cbc8bb66b74abd1ead4151ffc8593f204fcb97dec&#x27;"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.get_lizard_dataset", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "get_lizard_dataset", "kind": "function", "doc": "<p>Dataset for the segmentation of nuclei in histopathology.</p>\n\n<p>This dataset is from the publication <a href=\"https://doi.org/10.48550/arXiv.2108.11195\">https://doi.org/10.48550/arXiv.2108.11195</a>.\nPlease cite it if you use this dataset for a publication.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">patch_shape</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.lizard.get_lizard_loader", "modulename": "torch_em.data.datasets.histopathology.lizard", "qualname": "get_lizard_loader", "kind": "function", "doc": "<p>Dataloader for the segmentation of nuclei in histopathology. See 'get_lizard_dataset' for details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">patch_shape</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.monusac", "modulename": "torch_em.data.datasets.histopathology.monusac", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.histopathology.monusac.URL", "modulename": "torch_em.data.datasets.histopathology.monusac", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;https://drive.google.com/uc?export=download&amp;id=1lxMZaAPSpEHLSxGA9KKMt_r-4S8dwLhq&#x27;, &#x27;test&#x27;: &#x27;https://drive.google.com/uc?export=download&amp;id=1G54vsOdxWY1hG7dzmkeK3r0xz9s-heyQ&#x27;}"}, {"fullname": "torch_em.data.datasets.histopathology.monusac.CHECKSUM", "modulename": "torch_em.data.datasets.histopathology.monusac", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;5b7cbeb34817a8f880d3fddc28391e48d3329a91bf3adcbd131ea149a725cd92&#x27;, &#x27;test&#x27;: &#x27;bcbc38f6bf8b149230c90c29f3428cc7b2b76f8acd7766ce9fc908fc896c2674&#x27;}"}, {"fullname": "torch_em.data.datasets.histopathology.monusac.ORGAN_SPLITS", "modulename": "torch_em.data.datasets.histopathology.monusac", "qualname": "ORGAN_SPLITS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: {&#x27;lung&#x27;: [&#x27;TCGA-55-1594&#x27;, &#x27;TCGA-69-7760&#x27;, &#x27;TCGA-69-A59K&#x27;, &#x27;TCGA-73-4668&#x27;, &#x27;TCGA-78-7220&#x27;, &#x27;TCGA-86-7713&#x27;, &#x27;TCGA-86-8672&#x27;, &#x27;TCGA-L4-A4E5&#x27;, &#x27;TCGA-MP-A4SY&#x27;, &#x27;TCGA-MP-A4T7&#x27;], &#x27;kidney&#x27;: [&#x27;TCGA-5P-A9K0&#x27;, &#x27;TCGA-B9-A44B&#x27;, &#x27;TCGA-B9-A8YI&#x27;, &#x27;TCGA-DW-7841&#x27;, &#x27;TCGA-EV-5903&#x27;, &#x27;TCGA-F9-A97G&#x27;, &#x27;TCGA-G7-A8LD&#x27;, &#x27;TCGA-MH-A560&#x27;, &#x27;TCGA-P4-AAVK&#x27;, &#x27;TCGA-SX-A7SR&#x27;, &#x27;TCGA-UZ-A9PO&#x27;, &#x27;TCGA-UZ-A9PU&#x27;], &#x27;breast&#x27;: [&#x27;TCGA-A2-A0CV&#x27;, &#x27;TCGA-A2-A0ES&#x27;, &#x27;TCGA-B6-A0WZ&#x27;, &#x27;TCGA-BH-A18T&#x27;, &#x27;TCGA-D8-A1X5&#x27;, &#x27;TCGA-E2-A154&#x27;, &#x27;TCGA-E9-A22B&#x27;, &#x27;TCGA-E9-A22G&#x27;, &#x27;TCGA-EW-A6SD&#x27;, &#x27;TCGA-S3-AA11&#x27;], &#x27;prostate&#x27;: [&#x27;TCGA-EJ-5495&#x27;, &#x27;TCGA-EJ-5505&#x27;, &#x27;TCGA-EJ-5517&#x27;, &#x27;TCGA-G9-6342&#x27;, &#x27;TCGA-G9-6499&#x27;, &#x27;TCGA-J4-A67Q&#x27;, &#x27;TCGA-J4-A67T&#x27;, &#x27;TCGA-KK-A59X&#x27;, &#x27;TCGA-KK-A6E0&#x27;, &#x27;TCGA-KK-A7AW&#x27;, &#x27;TCGA-V1-A8WL&#x27;, &#x27;TCGA-V1-A9O9&#x27;, &#x27;TCGA-X4-A8KQ&#x27;, &#x27;TCGA-YL-A9WY&#x27;]}, &#x27;test&#x27;: {&#x27;lung&#x27;: [&#x27;TCGA-49-6743&#x27;, &#x27;TCGA-50-6591&#x27;, &#x27;TCGA-55-7570&#x27;, &#x27;TCGA-55-7573&#x27;, &#x27;TCGA-73-4662&#x27;, &#x27;TCGA-78-7152&#x27;, &#x27;TCGA-MP-A4T7&#x27;], &#x27;kidney&#x27;: [&#x27;TCGA-2Z-A9JG&#x27;, &#x27;TCGA-2Z-A9JN&#x27;, &#x27;TCGA-DW-7838&#x27;, &#x27;TCGA-DW-7963&#x27;, &#x27;TCGA-F9-A8NY&#x27;, &#x27;TCGA-IZ-A6M9&#x27;, &#x27;TCGA-MH-A55W&#x27;], &#x27;breast&#x27;: [&#x27;TCGA-A2-A04X&#x27;, &#x27;TCGA-A2-A0ES&#x27;, &#x27;TCGA-D8-A3Z6&#x27;, &#x27;TCGA-E2-A108&#x27;, &#x27;TCGA-EW-A6SB&#x27;], &#x27;prostate&#x27;: [&#x27;TCGA-G9-6356&#x27;, &#x27;TCGA-G9-6367&#x27;, &#x27;TCGA-VP-A87E&#x27;, &#x27;TCGA-VP-A87H&#x27;, &#x27;TCGA-X4-A8KS&#x27;, &#x27;TCGA-YL-A9WL&#x27;]}}"}, {"fullname": "torch_em.data.datasets.histopathology.monusac.get_patient_id", "modulename": "torch_em.data.datasets.histopathology.monusac", "qualname": "get_patient_id", "kind": "function", "doc": "<p>Gets us the patient id in the expected format\nInput Names: \"TCGA-<XX>-<XXXX>-01z-00-DX<X>-(<X>, &lt;00X>).tif\" (example: TCGA-2Z-A9JG-01Z-00-DX1_1.tif)\nExpected: \"TCGA-<XX>-<XXXX>\"                                  (example: TCGA-2Z-A9JG)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">split_wrt</span><span class=\"o\">=</span><span class=\"s1\">&#39;-01Z-00-&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.monusac.get_monusac_dataset", "modulename": "torch_em.data.datasets.histopathology.monusac", "qualname": "get_monusac_dataset", "kind": "function", "doc": "<p>Dataset from <a href=\"https://monusac-2020.grand-challenge.org/Data/\">https://monusac-2020.grand-challenge.org/Data/</a></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">split</span>,</span><span class=\"param\">\t<span class=\"n\">organ_type</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.monusac.get_monusac_loader", "modulename": "torch_em.data.datasets.histopathology.monusac", "qualname": "get_monusac_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">split</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">organ_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.monuseg", "modulename": "torch_em.data.datasets.histopathology.monuseg", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.histopathology.monuseg.URL", "modulename": "torch_em.data.datasets.histopathology.monuseg", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;https://drive.google.com/uc?export=download&amp;id=1ZgqFJomqQGNnsx7w7QBzQQMVA16lbVCA&#x27;, &#x27;test&#x27;: &#x27;https://drive.google.com/uc?export=download&amp;id=1NKkSQ5T0ZNQ8aUhh0a8Dt2YKYCQXIViw&#x27;}"}, {"fullname": "torch_em.data.datasets.histopathology.monuseg.CHECKSUM", "modulename": "torch_em.data.datasets.histopathology.monuseg", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;25d3d3185bb2970b397cafa72eb664c9b4d24294aee382e7e3df9885affce742&#x27;, &#x27;test&#x27;: &#x27;13e522387ae8b1bcc0530e13ff9c7b4d91ec74959ef6f6e57747368d7ee6f88a&#x27;}"}, {"fullname": "torch_em.data.datasets.histopathology.monuseg.ORGAN_SPLITS", "modulename": "torch_em.data.datasets.histopathology.monuseg", "qualname": "ORGAN_SPLITS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;breast&#x27;: [&#x27;TCGA-A7-A13E-01Z-00-DX1&#x27;, &#x27;TCGA-A7-A13F-01Z-00-DX1&#x27;, &#x27;TCGA-AR-A1AK-01Z-00-DX1&#x27;, &#x27;TCGA-AR-A1AS-01Z-00-DX1&#x27;, &#x27;TCGA-E2-A1B5-01Z-00-DX1&#x27;, &#x27;TCGA-E2-A14V-01Z-00-DX1&#x27;], &#x27;kidney&#x27;: [&#x27;TCGA-B0-5711-01Z-00-DX1&#x27;, &#x27;TCGA-HE-7128-01Z-00-DX1&#x27;, &#x27;TCGA-HE-7129-01Z-00-DX1&#x27;, &#x27;TCGA-HE-7130-01Z-00-DX1&#x27;, &#x27;TCGA-B0-5710-01Z-00-DX1&#x27;, &#x27;TCGA-B0-5698-01Z-00-DX1&#x27;], &#x27;liver&#x27;: [&#x27;TCGA-18-5592-01Z-00-DX1&#x27;, &#x27;TCGA-38-6178-01Z-00-DX1&#x27;, &#x27;TCGA-49-4488-01Z-00-DX1&#x27;, &#x27;TCGA-50-5931-01Z-00-DX1&#x27;, &#x27;TCGA-21-5784-01Z-00-DX1&#x27;, &#x27;TCGA-21-5786-01Z-00-DX1&#x27;], &#x27;prostate&#x27;: [&#x27;TCGA-G9-6336-01Z-00-DX1&#x27;, &#x27;TCGA-G9-6348-01Z-00-DX1&#x27;, &#x27;TCGA-G9-6356-01Z-00-DX1&#x27;, &#x27;TCGA-G9-6363-01Z-00-DX1&#x27;, &#x27;TCGA-CH-5767-01Z-00-DX1&#x27;, &#x27;TCGA-G9-6362-01Z-00-DX1&#x27;], &#x27;bladder&#x27;: [&#x27;TCGA-DK-A2I6-01A-01-TS1&#x27;, &#x27;TCGA-G2-A2EK-01A-02-TSB&#x27;], &#x27;colon&#x27;: [&#x27;TCGA-AY-A8YK-01A-01-TS1&#x27;, &#x27;TCGA-NH-A8F7-01A-01-TS1&#x27;], &#x27;stomach&#x27;: [&#x27;TCGA-KB-A93J-01A-01-TS1&#x27;, &#x27;TCGA-RD-A8N9-01A-01-TS1&#x27;]}"}, {"fullname": "torch_em.data.datasets.histopathology.monuseg.get_monuseg_dataset", "modulename": "torch_em.data.datasets.histopathology.monuseg", "qualname": "get_monuseg_dataset", "kind": "function", "doc": "<p>Dataset from <a href=\"https://monuseg.grand-challenge.org/Data/\">https://monuseg.grand-challenge.org/Data/</a></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">split</span>,</span><span class=\"param\">\t<span class=\"n\">organ_type</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.monuseg.get_monuseg_loader", "modulename": "torch_em.data.datasets.histopathology.monuseg", "qualname": "get_monuseg_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">split</span>,</span><span class=\"param\">\t<span class=\"n\">organ_type</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.pannuke", "modulename": "torch_em.data.datasets.histopathology.pannuke", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.histopathology.pannuke.URLS", "modulename": "torch_em.data.datasets.histopathology.pannuke", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;fold_1&#x27;: &#x27;https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke/fold_1.zip&#x27;, &#x27;fold_2&#x27;: &#x27;https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke/fold_2.zip&#x27;, &#x27;fold_3&#x27;: &#x27;https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke/fold_3.zip&#x27;}"}, {"fullname": "torch_em.data.datasets.histopathology.pannuke.CHECKSUM", "modulename": "torch_em.data.datasets.histopathology.pannuke", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;fold_1&#x27;: &#x27;6e19ad380300e8ce9480f9ab6a14cc91fa4b6a511609b40e3d70bdf9c881ed0b&#x27;, &#x27;fold_2&#x27;: &#x27;5bc540cc509f64b5f5a274d6e5a245527dbd3e6d3155d43555115c5d54709b07&#x27;, &#x27;fold_3&#x27;: &#x27;c14d372981c42f611ebc80afad01702b89cad8c1b3089daa31931cf5a4b1a39d&#x27;}"}, {"fullname": "torch_em.data.datasets.histopathology.pannuke.get_pannuke_dataset", "modulename": "torch_em.data.datasets.histopathology.pannuke", "qualname": "get_pannuke_dataset", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">folds</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;fold_1&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;fold_2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;fold_3&#39;</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">with_label_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">custom_label_choice</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;instances&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.histopathology.pannuke.get_pannuke_loader", "modulename": "torch_em.data.datasets.histopathology.pannuke", "qualname": "get_pannuke_loader", "kind": "function", "doc": "<p>TODO</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">folds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;fold_1&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;fold_2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;fold_3&#39;</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">custom_label_choice</span><span class=\"o\">=</span><span class=\"s1\">&#39;instances&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy", "modulename": "torch_em.data.datasets.light_microscopy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.covid_if", "modulename": "torch_em.data.datasets.light_microscopy.covid_if", "kind": "module", "doc": "<p>This dataset contains annotation for cell and nucleus segmentation\nin immunofluorescence microscopy.</p>\n\n<p>This dataset is from the publication <a href=\"https://doi.org/10.1002/bies.202000257\">https://doi.org/10.1002/bies.202000257</a>.\nPlease cite it if you use this dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.covid_if.COVID_IF_URL", "modulename": "torch_em.data.datasets.light_microscopy.covid_if", "qualname": "COVID_IF_URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://zenodo.org/record/5092850/files/covid-if-groundtruth.zip?download=1&#x27;"}, {"fullname": "torch_em.data.datasets.light_microscopy.covid_if.CHECKSUM", "modulename": "torch_em.data.datasets.light_microscopy.covid_if", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;d9cd6c85a19b802c771fb4ff928894b19a8fab0e0af269c49235fdac3f7a60e1&#x27;"}, {"fullname": "torch_em.data.datasets.light_microscopy.covid_if.get_covid_if_data", "modulename": "torch_em.data.datasets.light_microscopy.covid_if", "qualname": "get_covid_if_data", "kind": "function", "doc": "<p>Download the CovidIF training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath to the training data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.covid_if.get_covid_if_dataset", "modulename": "torch_em.data.datasets.light_microscopy.covid_if", "qualname": "get_covid_if_dataset", "kind": "function", "doc": "<p>Get the CovidIF dataset for segmenting nuclei or cells in immunofluorescence microscopy.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>sample_range:</strong>  Id range of samples to load from the training dataset.</li>\n<li><strong>target:</strong>  The segmentation task. Either 'cells' or 'nuclei'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sample_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;cells&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.covid_if.get_covid_if_loader", "modulename": "torch_em.data.datasets.light_microscopy.covid_if", "qualname": "get_covid_if_loader", "kind": "function", "doc": "<p>Get the CovidIF dataloder for segmenting nuclei or cells in immunofluorescence microscopy.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>sample_range:</strong>  Id range of samples to load from the training dataset.</li>\n<li><strong>target:</strong>  The segmentation task. Either 'cells' or 'nuclei'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sample_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;cells&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.ctc", "modulename": "torch_em.data.datasets.light_microscopy.ctc", "kind": "module", "doc": "<p>The Cell Tracking Challenge contains annotated data for cell segmentation and tracking.</p>\n\n<p>We currently cprovide the 2d datasets with segmentation annotations.\nIf you use this data in your research please cite <a href=\"https://doi.org/10.1038/nmeth.4473\">https://doi.org/10.1038/nmeth.4473</a>.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.ctc.CTC_CHECKSUMS", "modulename": "torch_em.data.datasets.light_microscopy.ctc", "qualname": "CTC_CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: {&#x27;BF-C2DL-HSC&#x27;: &#x27;0aa68ec37a9b06e72a5dfa07d809f56e1775157fb674bb75ff904936149657b1&#x27;, &#x27;BF-C2DL-MuSC&#x27;: &#x27;ca72b59042809120578a198ba236e5ed3504dd6a122ef969428b7c64f0a5e67d&#x27;, &#x27;DIC-C2DH-HeLa&#x27;: &#x27;832fed2d05bb7488cf9c51a2994b75f8f3f53b3c3098856211f2d39023c34e1a&#x27;, &#x27;Fluo-C2DL-Huh7&#x27;: &#x27;1912658c1b3d8b38b314eb658b559e7b39c256917150e9b3dd8bfdc77347617d&#x27;, &#x27;Fluo-C2DL-MSC&#x27;: &#x27;a083521f0cb673ae02d4957c5e6580c2e021943ef88101f6a2f61b944d671af2&#x27;, &#x27;Fluo-N2DH-GOWT1&#x27;: &#x27;1a7bd9a7d1d10c4122c7782427b437246fb69cc3322a975485c04e206f64fc2c&#x27;, &#x27;Fluo-N2DH-SIM+&#x27;: &#x27;3e809148c87ace80c72f563b56c35e0d9448dcdeb461a09c83f61e93f5e40ec8&#x27;, &#x27;Fluo-N2DL-HeLa&#x27;: &#x27;35dd99d58e071aba0b03880128d920bd1c063783cc280f9531fbdc5be614c82e&#x27;, &#x27;PhC-C2DH-U373&#x27;: &#x27;b18185c18fce54e8eeb93e4bbb9b201d757add9409bbf2283b8114185a11bc9e&#x27;, &#x27;PhC-C2DL-PSC&#x27;: &#x27;9d54bb8febc8798934a21bf92e05d92f5e8557c87e28834b2832591cdda78422&#x27;}, &#x27;test&#x27;: {&#x27;BF-C2DL-HSC&#x27;: &#x27;fd1c05ec625fd0526c8369d1139babe137e885457eee98c10d957da578d0d5bc&#x27;, &#x27;BF-C2DL-MuSC&#x27;: &#x27;c5cae259e6090e82a2596967fb54c8a768717c1772398f8546ad1c8df0820450&#x27;, &#x27;DIC-C2DH-HeLa&#x27;: &#x27;5e5d5f2aa90aef99d750cf03f5c12d799d50b892f98c86950e07a2c5955ac01f&#x27;, &#x27;Fluo-C2DL-Huh7&#x27;: &#x27;cc7359f8fb6b0c43995365e83ce0116d32f477ac644b2ca02b98bc253e2bcbbe&#x27;, &#x27;Fluo-C2DL-MSC&#x27;: &#x27;c90b13e603dde52f17801d4f0cadde04ed7f21cc05296b1f0957d92dbfc8ffa6&#x27;, &#x27;Fluo-N2DH-GOWT1&#x27;: &#x27;c6893ec2d63459de49d4dc21009b04275573403c62cc02e6ee8d0cb1a5068add&#x27;, &#x27;Fluo-N2DH-SIM+&#x27;: &#x27;c4f257add739b284d02176057814de345dee2ac1a7438e360ccd2df73618db68&#x27;, &#x27;Fluo-N2DL-HeLa&#x27;: &#x27;45cf3daf05e8495aa2ce0febacca4cf0928fab808c0b14ed2eb7289a819e6bb8&#x27;, &#x27;PhC-C2DH-U373&#x27;: &#x27;7aa3162e4363a416b259149adc13c9b09cb8aecfe8165eb1428dd534b66bec8a&#x27;, &#x27;PhC-C2DL-PSC&#x27;: &#x27;8c98ac6203e7490157ceb6aa1131d60a3863001b61fb75e784bc49d47ee264d5&#x27;}}"}, {"fullname": "torch_em.data.datasets.light_microscopy.ctc.get_ctc_data", "modulename": "torch_em.data.datasets.light_microscopy.ctc", "qualname": "get_ctc_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.ctc.get_ctc_segmentation_dataset", "modulename": "torch_em.data.datasets.light_microscopy.ctc", "qualname": "get_ctc_segmentation_dataset", "kind": "function", "doc": "<p>Get the CTC dataset for cell segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>dataset_name:</strong>  Name of the dataset to be downloaded. The available datasets are:\n{', '.join(CTC_CHECKSUMS['train'].keys())}</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>split:</strong>  The split to download. Currently only supports 'train'.</li>\n<li><strong>vol_id:</strong>  The train id to load.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">vol_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.ctc.get_ctc_segmentation_loader", "modulename": "torch_em.data.datasets.light_microscopy.ctc", "qualname": "get_ctc_segmentation_loader", "kind": "function", "doc": "<p>Get the CTC dataloader for cell segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>dataset_name:</strong>  Name of the dataset to be downloaded. The available datasets are:\n{', '.join(CTC_CHECKSUMS['train'].keys())}</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>split:</strong>  The split to download. Currently only supports 'train'.</li>\n<li><strong>vol_id:</strong>  The train id to load.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">vol_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.deepbacs", "modulename": "torch_em.data.datasets.light_microscopy.deepbacs", "kind": "module", "doc": "<p>DeepBacs is a dataset for segmenting bacteria in label-free light microscopy.</p>\n\n<p>This dataset is from the publication <a href=\"https://doi.org/10.1038/s42003-022-03634-z\">https://doi.org/10.1038/s42003-022-03634-z</a>.\nPlease cite it if you use this dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.deepbacs.URLS", "modulename": "torch_em.data.datasets.light_microscopy.deepbacs", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;s_aureus&#x27;: &#x27;https://zenodo.org/record/5550933/files/DeepBacs_Data_Segmentation_Staph_Aureus_dataset.zip?download=1&#x27;, &#x27;e_coli&#x27;: &#x27;https://zenodo.org/record/5550935/files/DeepBacs_Data_Segmentation_E.coli_Brightfield_dataset.zip?download=1&#x27;, &#x27;b_subtilis&#x27;: &#x27;https://zenodo.org/record/5639253/files/Multilabel_U-Net_dataset_B.subtilis.zip?download=1&#x27;, &#x27;mixed&#x27;: &#x27;https://zenodo.org/record/5551009/files/DeepBacs_Data_Segmentation_StarDist_MIXED_dataset.zip?download=1&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.deepbacs.CHECKSUMS", "modulename": "torch_em.data.datasets.light_microscopy.deepbacs", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;s_aureus&#x27;: &#x27;4047792f1248ee82fce34121d0ade84828e55db5a34656cc25beec46eacaf307&#x27;, &#x27;e_coli&#x27;: &#x27;f812a2f814c3875c78fcc1609a2e9b34c916c7a9911abbf8117f423536ef1c17&#x27;, &#x27;b_subtilis&#x27;: &#x27;1&#x27;, &#x27;mixed&#x27;: &#x27;2730e6b391637d6dc05bbc7b8c915fd8184d835ac3611e13f23ac6f10f86c2a0&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.deepbacs.get_deebacs_data", "modulename": "torch_em.data.datasets.light_microscopy.deepbacs", "qualname": "get_deebacs_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">bac_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.deepbacs.get_deepbacs_dataset", "modulename": "torch_em.data.datasets.light_microscopy.deepbacs", "qualname": "get_deepbacs_dataset", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">bac_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.deepbacs.get_deepbacs_loader", "modulename": "torch_em.data.datasets.light_microscopy.deepbacs", "qualname": "get_deepbacs_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">bac_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.dsb", "modulename": "torch_em.data.datasets.light_microscopy.dsb", "kind": "module", "doc": "<p>This Dataset was used in a Kaggle Data Science Bowl. It contains light microscopy\nimages with annotations for nucleus segmentation.</p>\n\n<p>The dataset is described in the publication <a href=\"https://doi.org/10.1038/s41592-019-0612-7\">https://doi.org/10.1038/s41592-019-0612-7</a>.\nPlease cite it if you use this dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.dsb.DSB_URLS", "modulename": "torch_em.data.datasets.light_microscopy.dsb", "qualname": "DSB_URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;full&#x27;: &#x27;&#x27;, &#x27;reduced&#x27;: &#x27;https://github.com/stardist/stardist/releases/download/0.1.0/dsb2018.zip&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.dsb.CHECKSUMS", "modulename": "torch_em.data.datasets.light_microscopy.dsb", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;full&#x27;: None, &#x27;reduced&#x27;: &#x27;e44921950edce378063aa4457e625581ba35b4c2dbd9a07c19d48900129f386f&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.dsb.get_dsb_data", "modulename": "torch_em.data.datasets.light_microscopy.dsb", "qualname": "get_dsb_data", "kind": "function", "doc": "<p>Download the DeepBacs training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>source:</strong>  The source of the dataset. Can either be 'full' for the complete dataset,\nor 'reduced' for the dataset excluding histopathology images.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath to the training data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">source</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.dsb.get_dsb_dataset", "modulename": "torch_em.data.datasets.light_microscopy.dsb", "qualname": "get_dsb_dataset", "kind": "function", "doc": "<p>Get the DSB dataset for nucleus segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The split to use for the dataset. Either 'train' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>source:</strong>  The source of the dataset. Can either be 'full' for the complete dataset,\nor 'reduced' for the dataset excluding histopathology images.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">source</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;reduced&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.dsb.get_dsb_loader", "modulename": "torch_em.data.datasets.light_microscopy.dsb", "qualname": "get_dsb_loader", "kind": "function", "doc": "<p>Get the DSB dataloader for nucleus segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The split to use for the dataset. Either 'train' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>source:</strong>  The source of the dataset. Can either be 'full' for the complete dataset,\nor 'reduced' for the dataset excluding histopathology images.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">source</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;reduced&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.dynamicnuclearnet", "modulename": "torch_em.data.datasets.light_microscopy.dynamicnuclearnet", "kind": "module", "doc": "<p>The DynamicNuclearNet dataset contains annotations for nucleus segmentation\nand tracking in fluorescence light microscopy, for five different cell lines.</p>\n\n<p>This dataset is from the publication <a href=\"https://doi.org/10.1101/803205\">https://doi.org/10.1101/803205</a>.\nPlease cite it if you use this dataset for your research.</p>\n\n<p>This dataset cannot be downloaded automatically, please visit <a href=\"https://datasets.deepcell.org/data\">https://datasets.deepcell.org/data</a>\nand download it yourself.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.dynamicnuclearnet.get_dynamicnuclearnet_dataset", "modulename": "torch_em.data.datasets.light_microscopy.dynamicnuclearnet", "qualname": "get_dynamicnuclearnet_dataset", "kind": "function", "doc": "<p>Get the DynamicNuclearNet dataset for nucleus segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The split to use for the dataset. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.dynamicnuclearnet.get_dynamicnuclearnet_loader", "modulename": "torch_em.data.datasets.light_microscopy.dynamicnuclearnet", "qualname": "get_dynamicnuclearnet_loader", "kind": "function", "doc": "<p>Get the DynamicNuclearNet dataloader for nucleus segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The split to use for the dataset. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.hpa", "modulename": "torch_em.data.datasets.light_microscopy.hpa", "kind": "module", "doc": "<p>This dataset was part of the HPA Kaggle challenge for protein identification.\nIt contains confocal microscopy images and annotations for cell segmentation.</p>\n\n<p>The dataset is described in the publication <a href=\"https://doi.org/10.1038/s41592-019-0658-6\">https://doi.org/10.1038/s41592-019-0658-6</a>.\nPlease cite it if you use this dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.hpa.URLS", "modulename": "torch_em.data.datasets.light_microscopy.hpa", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;segmentation&#x27;: &#x27;https://zenodo.org/record/4665863/files/hpa_dataset_v2.zip&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.hpa.CHECKSUMS", "modulename": "torch_em.data.datasets.light_microscopy.hpa", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;segmentation&#x27;: &#x27;dcd6072293d88d49c71376d3d99f3f4f102e4ee83efb0187faa89c95ec49faa9&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.hpa.get_hpa_segmentation_data", "modulename": "torch_em.data.datasets.light_microscopy.hpa", "qualname": "get_hpa_segmentation_data", "kind": "function", "doc": "<p>Download the HPA training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>channels:</strong>  The image channels to extract. Available channels are\n'microtubules', 'protein', 'nuclei' or 'er'.</li>\n<li><strong>n_workers_preproc:</strong>  The number of workers to use for preprocessing.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath to the training data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">channels</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">n_workers_preproc</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.hpa.get_hpa_segmentation_dataset", "modulename": "torch_em.data.datasets.light_microscopy.hpa", "qualname": "get_hpa_segmentation_dataset", "kind": "function", "doc": "<p>Get the HPA dataset for segmenting cells in confocal microscopy.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The split for the dataset. Available splits are 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>channels:</strong>  The image channels to extract. Available channels are\n'microtubules', 'protein', 'nuclei' or 'er'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>n_workers_preproc:</strong>  The number of workers to use for preprocessing.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">channels</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;microtubules&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;protein&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;nuclei&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;er&#39;</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_workers_preproc</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.hpa.get_hpa_segmentation_loader", "modulename": "torch_em.data.datasets.light_microscopy.hpa", "qualname": "get_hpa_segmentation_loader", "kind": "function", "doc": "<p>Get the HPA dataloader for segmenting cells in confocal microscopy.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The split for the dataset. Available splits are 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>channels:</strong>  The image channels to extract. Available channels are\n'microtubules', 'protein', 'nuclei' or 'er'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>n_workers_preproc:</strong>  The number of workers to use for preprocessing.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">channels</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;microtubules&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;protein&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;nuclei&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;er&#39;</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_workers_preproc</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.livecell", "modulename": "torch_em.data.datasets.light_microscopy.livecell", "kind": "module", "doc": "<p>The LIVECell dataset contains phase-contrast microscopy images\nand annotations for cell segmentations for 8 different cell lines.</p>\n\n<p>This dataset is desceibed in the publication <a href=\"https://doi.org/10.1038/s41592-021-01249-6\">https://doi.org/10.1038/s41592-021-01249-6</a>.\nPlease cite it if you use this dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.livecell.URLS", "modulename": "torch_em.data.datasets.light_microscopy.livecell", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;images&#x27;: &#x27;http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/images.zip&#x27;, &#x27;train&#x27;: &#x27;http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json&#x27;, &#x27;val&#x27;: &#x27;http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_val.json&#x27;, &#x27;test&#x27;: &#x27;http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test.json&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.livecell.CHECKSUM", "modulename": "torch_em.data.datasets.light_microscopy.livecell", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "None"}, {"fullname": "torch_em.data.datasets.light_microscopy.livecell.get_livecell_data", "modulename": "torch_em.data.datasets.light_microscopy.livecell", "qualname": "get_livecell_data", "kind": "function", "doc": "<p>Download the LIVECell dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split to use. Either 'train', 'val' or 'test'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>cell_types:</strong>  The cell types for which to get the data paths.</li>\n<li><strong>label_path:</strong>  Optional path for loading the label data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The paths to the image data.\n  The paths to the label data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">cell_types</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.livecell.get_livecell_dataset", "modulename": "torch_em.data.datasets.light_microscopy.livecell", "qualname": "get_livecell_dataset", "kind": "function", "doc": "<p>Get the LIVECell dataset for segmenting cells in phase-contrast microscopy.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split to use. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>cell_types:</strong>  The cell types for which to get the data paths.</li>\n<li><strong>label_path:</strong>  Optional path for loading the label data.</li>\n<li><strong>label_dtype:</strong>  The datatype of the label data.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">cell_types</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int64</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.livecell.get_livecell_loader", "modulename": "torch_em.data.datasets.light_microscopy.livecell", "qualname": "get_livecell_loader", "kind": "function", "doc": "<p>Get the LIVECell dataloader for segmenting cells in phase-contrast microscopy.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split to use. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>cell_types:</strong>  The cell types for which to get the data paths.</li>\n<li><strong>label_path:</strong>  Optional path for loading the label data.</li>\n<li><strong>label_dtype:</strong>  The datatype of the label data.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">cell_types</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int64</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.mouse_embryo", "modulename": "torch_em.data.datasets.light_microscopy.mouse_embryo", "kind": "module", "doc": "<p>This dataset contains confocal microscopy stacks of a mouse embryo\nwith annotations for cell and nucleus segmentation.</p>\n\n<p>This dataset is part of the publication <a href=\"https://doi.org/10.15252/embj.2022113280\">https://doi.org/10.15252/embj.2022113280</a>.\nPlease cite it if you use this data in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.mouse_embryo.URL", "modulename": "torch_em.data.datasets.light_microscopy.mouse_embryo", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://zenodo.org/record/6546550/files/MouseEmbryos.zip?download=1&#x27;"}, {"fullname": "torch_em.data.datasets.light_microscopy.mouse_embryo.CHECKSUM", "modulename": "torch_em.data.datasets.light_microscopy.mouse_embryo", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;bf24df25e5f919489ce9e674876ff27e06af84445c48cf2900f1ab590a042622&#x27;"}, {"fullname": "torch_em.data.datasets.light_microscopy.mouse_embryo.get_mouse_embryo_data", "modulename": "torch_em.data.datasets.light_microscopy.mouse_embryo", "qualname": "get_mouse_embryo_data", "kind": "function", "doc": "<p>Download the mouse embryo dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath for the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.mouse_embryo.get_mouse_embryo_dataset", "modulename": "torch_em.data.datasets.light_microscopy.mouse_embryo", "qualname": "get_mouse_embryo_dataset", "kind": "function", "doc": "<p>Get the mouse embryo dataset for cell or nucleus segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  The name of the segmentation task. Either 'membrane' or 'nuclei'.</li>\n<li><strong>split:</strong>  The split to use for the dataset. Either 'train' or 'val'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.mouse_embryo.get_mouse_embryo_loader", "modulename": "torch_em.data.datasets.light_microscopy.mouse_embryo", "qualname": "get_mouse_embryo_loader", "kind": "function", "doc": "<p>Get the mouse embryo dataset for cell or nucleus segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  The name of the segmentation task. Either 'membrane' or 'nuclei'.</li>\n<li><strong>split:</strong>  The split to use for the dataset. Either 'train' or 'val'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "kind": "module", "doc": "<p>This dataset comes from the Neurips Cell Segmentation Challenge,\nwhich collects microscopy images and annotations for cell segmentation.</p>\n\n<p>The dataset contains both images with annotations for cell segmentation\nand unlabed images for self-supervised or semi-supervised learning.\nSee also the challenge website for details: <a href=\"https://neurips22-cellseg.grand-challenge.org/\">https://neurips22-cellseg.grand-challenge.org/</a>.\nThe dataset os decribed in the publication <a href=\"https://doi.org/10.1038/s41592-024-02233-6\">https://doi.org/10.1038/s41592-024-02233-6</a>.\nPlease cite it if you use the dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.URL", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;https://zenodo.org/records/10719375/files/Training-labeled.zip&#x27;, &#x27;val&#x27;: &#x27;https://zenodo.org/records/10719375/files/Tuning.zip&#x27;, &#x27;test&#x27;: &#x27;https://zenodo.org/records/10719375/files/Testing.zip&#x27;, &#x27;unlabeled&#x27;: &#x27;https://zenodo.org/records/10719375/files/train-unlabeled-part1.zip&#x27;, &#x27;unlabeled_wsi&#x27;: &#x27;https://zenodo.org/records/10719375/files/train-unlabeled-part2.zip&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.CHECKSUM", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;b2383929eb8e99b2716fa0d4e2f6e03983e626a57cf00fe85175869c54aa3592&#x27;, &#x27;val&#x27;: &#x27;849423d36bb8fcc2d91a5b189a3b6d93c3d4071c9701eaaa44ba393a510459c4&#x27;, &#x27;test&#x27;: &#x27;3379730221f43830d30fddf131750e967c9c9bdf04f98811e852a050eb659ccc&#x27;, &#x27;unlabeled&#x27;: &#x27;390b38b398b05e9e5306a024a3bd48ab22e49592cfab3c1a119eab3636b38e0d&#x27;, &#x27;unlabeled_wsi&#x27;: &#x27;d1e68eba2918305eab8b846e7578ac14683de970e3fa6a7c2a4a55753be56204&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.DIR_NAMES", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "DIR_NAMES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;Training-labeled&#x27;, &#x27;val&#x27;: &#x27;Tuning&#x27;, &#x27;test&#x27;: &#x27;Testing/Public&#x27;, &#x27;unlabeled&#x27;: &#x27;release-part1&#x27;, &#x27;unlabeled_wsi&#x27;: &#x27;train-unlabeled-part2&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.ZIP_PATH", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "ZIP_PATH", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;Training-labeled.zip&#x27;, &#x27;val&#x27;: &#x27;Tuning.zip&#x27;, &#x27;test&#x27;: &#x27;Testing.zip&#x27;, &#x27;unlabeled&#x27;: &#x27;train-unlabeled-part1.zip&#x27;, &#x27;unlabeled_wsi&#x27;: &#x27;train-unlabeled-part2.zip&#x27;}"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.to_rgb", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "to_rgb", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.get_neurips_cellseg_data", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "get_neurips_cellseg_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.get_neurips_cellseg_supervised_dataset", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "get_neurips_cellseg_supervised_dataset", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">make_rgb</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform2</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.get_neurips_cellseg_supervised_loader", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "get_neurips_cellseg_supervised_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">make_rgb</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform2</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">loader_kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.get_neurips_cellseg_unsupervised_dataset", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "get_neurips_cellseg_unsupervised_dataset", "kind": "function", "doc": "<p>Get the unsupervised dataset from the NeurIPS Cell Seg Challenge.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>root:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>make_rgb:</strong>  Whether to map all data to RGB or treat it as grayscale.</li>\n<li><strong>raw_transform:</strong>  Transformation of the raw data.</li>\n<li><strong>transform:</strong>  Transformation applied to raw and label data.</li>\n<li><strong>dtype:</strong>  The data type of the image data.</li>\n<li><strong>sampler:</strong>  Sampler for rejecting batches.</li>\n<li><strong>use_images:</strong>  Whether to use the normal image data.</li>\n<li><strong>use_wholeslide:</strong>  Whether to use the wholeslide image data.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">make_rgb</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_images</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_wholeslide</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.neurips_cell_seg.get_neurips_cellseg_unsupervised_loader", "modulename": "torch_em.data.datasets.light_microscopy.neurips_cell_seg", "qualname": "get_neurips_cellseg_unsupervised_loader", "kind": "function", "doc": "<p>Get the unsupervised dataset from the NeurIPS Cell Seg Challenge.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>root:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>make_rgb:</strong>  Whether to map all data to RGB or treat it as grayscale.</li>\n<li><strong>raw_transform:</strong>  Transformation of the raw data.</li>\n<li><strong>transform:</strong>  Transformation applied to raw and label data.</li>\n<li><strong>dtype:</strong>  The data type of the image data.</li>\n<li><strong>sampler:</strong>  Sampler for rejecting batches.</li>\n<li><strong>use_images:</strong>  Whether to use the normal image data.</li>\n<li><strong>use_wholeslide:</strong>  Whether to use the wholeslide image data.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>loader_kwargs:</strong>  Keyword arguments for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">make_rgb</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_images</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_wholeslide</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">loader_kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.plantseg", "modulename": "torch_em.data.datasets.light_microscopy.plantseg", "kind": "module", "doc": "<p>This dataset contains confocal and lightsheet microscopy images of plant cells\nwith annotations for cell and nucleus segmentation.</p>\n\n<p>The dataset part of the publication <a href=\"https://doi.org/10.7554/eLife.57613\">https://doi.org/10.7554/eLife.57613</a>.\nPlease cite it if you use this dataset in your research.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.plantseg.URLS", "modulename": "torch_em.data.datasets.light_microscopy.plantseg", "qualname": "URLS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;root&#x27;: {&#x27;train&#x27;: &#x27;https://files.de-1.osf.io/v1/resources/9x3g2/providers/osfstorage/?zip=&#x27;, &#x27;val&#x27;: &#x27;https://files.de-1.osf.io/v1/resources/vs6gb/providers/osfstorage/?zip=&#x27;, &#x27;test&#x27;: &#x27;https://files.de-1.osf.io/v1/resources/tn4xj/providers/osfstorage/?zip=&#x27;}, &#x27;nuclei&#x27;: {&#x27;train&#x27;: &#x27;https://files.de-1.osf.io/v1/resources/thxzn/providers/osfstorage/?zip=&#x27;}, &#x27;ovules&#x27;: {&#x27;train&#x27;: &#x27;https://files.de-1.osf.io/v1/resources/x9yns/providers/osfstorage/?zip=&#x27;, &#x27;val&#x27;: &#x27;https://files.de-1.osf.io/v1/resources/xp5uf/providers/osfstorage/?zip=&#x27;, &#x27;test&#x27;: &#x27;https://files.de-1.osf.io/v1/resources/8jz7e/providers/osfstorage/?zip=&#x27;}}"}, {"fullname": "torch_em.data.datasets.light_microscopy.plantseg.CHECKSUMS", "modulename": "torch_em.data.datasets.light_microscopy.plantseg", "qualname": "CHECKSUMS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;root&#x27;: {&#x27;train&#x27;: None, &#x27;val&#x27;: None, &#x27;test&#x27;: None}, &#x27;nuclei&#x27;: {&#x27;train&#x27;: None}, &#x27;ovules&#x27;: {&#x27;train&#x27;: None, &#x27;val&#x27;: None, &#x27;test&#x27;: None}}"}, {"fullname": "torch_em.data.datasets.light_microscopy.plantseg.get_plantseg_data", "modulename": "torch_em.data.datasets.light_microscopy.plantseg", "qualname": "get_plantseg_data", "kind": "function", "doc": "<p>Download the PlantSeg training data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>name:</strong>  The name of the data to load. Either 'root', 'nuclei' or 'ovules'.</li>\n<li><strong>split:</strong>  The split to download. Either 'train', 'val' or 'test'.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The filepath to the training data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.plantseg.get_plantseg_dataset", "modulename": "torch_em.data.datasets.light_microscopy.plantseg", "qualname": "get_plantseg_dataset", "kind": "function", "doc": "<p>Get the PlantSeg dataset for segmenting nuclei or cells.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  The name of the data to load. Either 'root', 'nuclei' or 'ovules'.</li>\n<li><strong>split:</strong>  The split to download. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.plantseg.get_plantseg_loader", "modulename": "torch_em.data.datasets.light_microscopy.plantseg", "qualname": "get_plantseg_loader", "kind": "function", "doc": "<p>Get the PlantSeg dataloader for segmenting nuclei or cells.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>name:</strong>  The name of the data to load. Either 'root', 'nuclei' or 'ovules'.</li>\n<li><strong>split:</strong>  The split to download. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>offsets:</strong>  Offset values for affinity computation used as target.</li>\n<li><strong>boundaries:</strong>  Whether to compute boundaries as the target.</li>\n<li><strong>binary:</strong>  Whether to use a binary segmentation target.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.pnas_arabidopsis", "modulename": "torch_em.data.datasets.light_microscopy.pnas_arabidopsis", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.pnas_arabidopsis.URL", "modulename": "torch_em.data.datasets.light_microscopy.pnas_arabidopsis", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://www.repository.cam.ac.uk/bitstream/handle/1810/262530/PNAS.zip?sequence=4&amp;isAllowed=y&#x27;"}, {"fullname": "torch_em.data.datasets.light_microscopy.pnas_arabidopsis.CHECKSUM", "modulename": "torch_em.data.datasets.light_microscopy.pnas_arabidopsis", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;&#x27;"}, {"fullname": "torch_em.data.datasets.light_microscopy.tissuenet", "modulename": "torch_em.data.datasets.light_microscopy.tissuenet", "kind": "module", "doc": "<p>The TissueNet dataset contains annotations for cell segmentation in microscopy images of different tissue types.</p>\n\n<p>This dataset is from the publication <a href=\"https://doi.org/10.1038/s41587-021-01094-0\">https://doi.org/10.1038/s41587-021-01094-0</a>.\nPlease cite it if you use this dataset for your research.</p>\n\n<p>This dataset cannot be downloaded automatically, please visit <a href=\"https://datasets.deepcell.org/data\">https://datasets.deepcell.org/data</a>\nand download it yourself.</p>\n"}, {"fullname": "torch_em.data.datasets.light_microscopy.tissuenet.get_tissuenet_dataset", "modulename": "torch_em.data.datasets.light_microscopy.tissuenet", "qualname": "get_tissuenet_dataset", "kind": "function", "doc": "<p>Get the TissueNet dataset for segmenting cells in microscopy tissue images.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split to use. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>raw_channel:</strong>  The channel to load for the raw data. Either 'nucleus', 'cell' or 'rgb'.</li>\n<li><strong>label_channel:</strong>  The channel to load for the label data. Either 'nucleus' or 'cell'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_channel</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">label_channel</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.light_microscopy.tissuenet.get_tissuenet_loader", "modulename": "torch_em.data.datasets.light_microscopy.tissuenet", "qualname": "get_tissuenet_loader", "kind": "function", "doc": "<p>Get the TissueNet dataloader for segmenting cells in microscopy tissue images.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Filepath to a folder where the downloaded data will be saved.</li>\n<li><strong>split:</strong>  The data split to use. Either 'train', 'val' or 'test'.</li>\n<li><strong>patch_shape:</strong>  The patch shape to use for training.</li>\n<li><strong>batch_size:</strong>  The batch size for training.</li>\n<li><strong>raw_channel:</strong>  The channel to load for the raw data. Either 'nucleus', 'cell' or 'rgb'.</li>\n<li><strong>label_channel:</strong>  The channel to load for the label data. Either 'nucleus' or 'cell'.</li>\n<li><strong>download:</strong>  Whether to download the data if it is not present.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">raw_channel</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">label_channel</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical", "modulename": "torch_em.data.datasets.medical", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.autopet", "modulename": "torch_em.data.datasets.medical.autopet", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.autopet.AUTOPET_DATA", "modulename": "torch_em.data.datasets.medical.autopet", "qualname": "AUTOPET_DATA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;http://193.196.20.155/data/autoPET/data/nifti.zip&#x27;"}, {"fullname": "torch_em.data.datasets.medical.autopet.CHECKSUM", "modulename": "torch_em.data.datasets.medical.autopet", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;0ac2186ea6d936ff41ce605c6a9588aeb20f031085589897dbab22fc82a12972&#x27;"}, {"fullname": "torch_em.data.datasets.medical.autopet.get_autopet_dataset", "modulename": "torch_em.data.datasets.medical.autopet", "qualname": "get_autopet_dataset", "kind": "function", "doc": "<p>Dataset for lesion segmentation in whole-body FDG-PET/CT scans.</p>\n\n<p>This dataset is fromt the <code>AutoPET II - Automated Lesion Segmentation in PET/CT - Domain Generalization</code> challenge.\nLink: <a href=\"https://autopet-ii.grand-challenge.org/\">https://autopet-ii.grand-challenge.org/</a>\nPlease cite it if you use this dataset for publication.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The path where the zip files / the prepared dataset exists.\n<ul>\n<li>Expected initial structure: <code>path</code> should have ...</li>\n</ul></li>\n<li><strong>patch_shape:</strong>  The patch shape (for 2d or 3d patches)</li>\n<li><strong>ndim:</strong>  The dimensions of the inputs (use <code>2</code> for getting 2d patches, and <code>3</code> for getting 3d patches)</li>\n<li><strong>modality:</strong>  The modality for using the AutoPET dataset.\n<ul>\n<li>(default: None) If passed <code>None</code>, it takes both the modalities as inputs</li>\n</ul></li>\n<li><strong>download:</strong>  Downloads the dataset</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dataset: The segmentation dataset for the respective modalities.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">modality</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.autopet.get_autopet_loader", "modulename": "torch_em.data.datasets.medical.autopet", "qualname": "get_autopet_loader", "kind": "function", "doc": "<p>Dataloader for lesion segmentation in whole-body FDG-PET/CT scans. See <code>get_autopet_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span>,</span><span class=\"param\">\t<span class=\"n\">modality</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.btcv", "modulename": "torch_em.data.datasets.medical.btcv", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.btcv.ABDOMEN_ORGANS", "modulename": "torch_em.data.datasets.medical.btcv", "qualname": "ABDOMEN_ORGANS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;spleen&#x27;: 1, &#x27;right kidney&#x27;: 2, &#x27;left kidney&#x27;: 3, &#x27;gallbladder&#x27;: 4, &#x27;esophagus&#x27;: 5, &#x27;liver&#x27;: 6, &#x27;stomach&#x27;: 7, &#x27;aorta&#x27;: 8, &#x27;inferior vena cava&#x27;: 9, &#x27;portal vein and splenic vein&#x27;: 10, &#x27;pancreas&#x27;: 11, &#x27;right adrenal gland&#x27;: 12, &#x27;left adrenal gland&#x27;: 13}"}, {"fullname": "torch_em.data.datasets.medical.btcv.CERVICAL_ORGANS", "modulename": "torch_em.data.datasets.medical.btcv", "qualname": "CERVICAL_ORGANS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;bladder&#x27;: 1, &#x27;uterus&#x27;: 2, &#x27;rectum&#x27;: 3, &#x27;small bowel&#x27;: 4}"}, {"fullname": "torch_em.data.datasets.medical.btcv.InstancesFromOneHot", "modulename": "torch_em.data.datasets.medical.btcv", "qualname": "InstancesFromOneHot", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.btcv.InstancesFromOneHot.__init__", "modulename": "torch_em.data.datasets.medical.btcv", "qualname": "InstancesFromOneHot.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_ids</span>, </span><span class=\"param\"><span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.data.datasets.medical.btcv.InstancesFromOneHot.class_ids", "modulename": "torch_em.data.datasets.medical.btcv", "qualname": "InstancesFromOneHot.class_ids", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.btcv.get_btcv_dataset", "modulename": "torch_em.data.datasets.medical.btcv", "qualname": "get_btcv_dataset", "kind": "function", "doc": "<p>Dataset for multi-organ segmentation in CT scans.</p>\n\n<p>This dataset is from the Multi-Atlas Labeling Beyond the Cranial Vault - Workshop and Challenge\nLink: <a href=\"https://www.synapse.org/#!Synapse:syn3193805/wiki/89480\">https://www.synapse.org/#!Synapse:syn3193805/wiki/89480</a>\nPlease cite it if you use this dataset for a publication.</p>\n\n<p>Steps on how to get the dataset?\n    1. Join the challenge using their official website: <a href=\"https://www.synapse.org/#!Synapse:syn3193805\">https://www.synapse.org/#!Synapse:syn3193805</a>\n    2. Next, go to \"Files\" -> (download the respective zip files)\n        - \"Abdomen\" -> \"RawData.zip\" downloads all the abdominal CT scans\n        - \"Cervix\" -> \"CervixRawData.zip\" downloads all the cervical CT scans\n    3. Provide the path to the parent directory, where the zipped file(s) have been downloaded.\n       The dataset would take care of the rest.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The path where the zip files / the prepared datasets exist.\n<ul>\n<li>Expected initial structure: <code>path</code> should have two zip files, namely <code>RawData.zip</code> and <code>CervixRawData.zip</code></li>\n</ul></li>\n<li><strong>patch_shape:</strong>  The patch shape (for 2d or 3d patches)</li>\n<li><strong>ndim:</strong>  The dimensions of the inputs (use <code>2</code> for getting 2d patches,  and <code>3</code> for getting 3d patches)</li>\n<li><strong>organ:</strong>  The organs in the respective anatomical regions of choice\n<ul>\n<li>default: None (i.e., returns labels with all organ types)</li>\n</ul></li>\n<li><strong>anatomy:</strong>  The anatomical regions of choice from the provided scans\n<ul>\n<li>default: None (i.e., returns both the available anatomies - abdomen and cervix)</li>\n</ul></li>\n<li><strong>download:</strong>  (NOT SUPPORTED) Downloads the dataset</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dataset: The dataset for the respective splits</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">organs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">anatomy</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_foreground_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.btcv.get_btcv_loader", "modulename": "torch_em.data.datasets.medical.btcv", "qualname": "get_btcv_loader", "kind": "function", "doc": "<p>Dataloader for multi-organ segmentation in CT scans. See <code>get_btcv_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span>,</span><span class=\"param\">\t<span class=\"n\">organs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">anatomy</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_foreground_fraction</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.busi", "modulename": "torch_em.data.datasets.medical.busi", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.busi.URL", "modulename": "torch_em.data.datasets.medical.busi", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://scholar.cu.edu.eg/Dataset_BUSI.zip&#x27;"}, {"fullname": "torch_em.data.datasets.medical.busi.CHECKSUM", "modulename": "torch_em.data.datasets.medical.busi", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;b2ce09f6063a31a73f628b6a6ee1245187cbaec225e93e563735691d68654de7&#x27;"}, {"fullname": "torch_em.data.datasets.medical.busi.get_busi_data", "modulename": "torch_em.data.datasets.medical.busi", "qualname": "get_busi_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">download</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.busi.get_busi_dataset", "modulename": "torch_em.data.datasets.medical.busi", "qualname": "get_busi_dataset", "kind": "function", "doc": "<p>\"Dataset for segmentation of breast cancer in ultrasound images.</p>\n\n<p>This database is located at <a href=\"https://scholar.cu.edu.eg/?q=afahmy/pages/dataset\">https://scholar.cu.edu.eg/?q=afahmy/pages/dataset</a></p>\n\n<p>The dataset is from Al-Dhabyani et al. - <a href=\"https://doi.org/10.1016/j.dib.2019.104863\">https://doi.org/10.1016/j.dib.2019.104863</a>\nPlease cite it if you use this dataset for a publication.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">category</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.busi.get_busi_loader", "modulename": "torch_em.data.datasets.medical.busi", "qualname": "get_busi_loader", "kind": "function", "doc": "<p>Dataloader for segmentation of breast cancer in ultrasound images. See <code>get_busi_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">category</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.camus", "modulename": "torch_em.data.datasets.medical.camus", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.camus.URL", "modulename": "torch_em.data.datasets.medical.camus", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://humanheart-project.creatis.insa-lyon.fr/database/api/v1/folder/63fde55f73e9f004868fb7ac/download&#x27;"}, {"fullname": "torch_em.data.datasets.medical.camus.CHECKSUM", "modulename": "torch_em.data.datasets.medical.camus", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;43745d640db5d979332bda7f00f4746747a2591b46efc8f1966b573ce8d65655&#x27;"}, {"fullname": "torch_em.data.datasets.medical.camus.get_camus_data", "modulename": "torch_em.data.datasets.medical.camus", "qualname": "get_camus_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">download</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.camus.get_camus_dataset", "modulename": "torch_em.data.datasets.medical.camus", "qualname": "get_camus_dataset", "kind": "function", "doc": "<p>Dataset for segmenting cardiac structures in 2d echocardiography images.</p>\n\n<p>The database is located at:\n<a href=\"https://humanheart-project.creatis.insa-lyon.fr/database/#collection/6373703d73e9f0047faa1bc8\">https://humanheart-project.creatis.insa-lyon.fr/database/#collection/6373703d73e9f0047faa1bc8</a></p>\n\n<p>This dataset is from the CAMUS challenge - <a href=\"https://doi.org/10.1109/TMI.2019.2900516\">https://doi.org/10.1109/TMI.2019.2900516</a>.\nPlease cite it if you use this dataset for a publication.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">chamber</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.camus.get_camus_loader", "modulename": "torch_em.data.datasets.medical.camus", "qualname": "get_camus_loader", "kind": "function", "doc": "<p>Dataloader for segmenting cardiac structures in 2d echocardiography images. See <code>get_camus_dataset</code> for details</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">chamber</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.drive", "modulename": "torch_em.data.datasets.medical.drive", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.drive.URL", "modulename": "torch_em.data.datasets.medical.drive", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;https://www.dropbox.com/sh/z4hbbzqai0ilqht/AADp_8oefNFs2bjC2kzl2_Fqa/training.zip?dl=1&#x27;, &#x27;test&#x27;: &#x27;https://www.dropbox.com/sh/z4hbbzqai0ilqht/AABuUJQJ5yG5oCuziYzYu8jWa/test.zip?dl=1&#x27;}"}, {"fullname": "torch_em.data.datasets.medical.drive.CHECKSUM", "modulename": "torch_em.data.datasets.medical.drive", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;train&#x27;: &#x27;7101e19598e2b7aacdbd5e6e7575057b9154a4aaec043e0f4e28902bf4e2e209&#x27;, &#x27;test&#x27;: &#x27;d76c95c98a0353487ffb63b3bb2663c00ed1fde7d8fdfd8c3282c6e310a02731&#x27;}"}, {"fullname": "torch_em.data.datasets.medical.drive.get_drive_data", "modulename": "torch_em.data.datasets.medical.drive", "qualname": "get_drive_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">download</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.drive.get_drive_dataset", "modulename": "torch_em.data.datasets.medical.drive", "qualname": "get_drive_dataset", "kind": "function", "doc": "<p>Dataset for segmentation of retinal blood vessels in fundus images.</p>\n\n<p>This dataset is from the \"DRIVE\" challenge:</p>\n\n<ul>\n<li><a href=\"https://drive.grand-challenge.org/\">https://drive.grand-challenge.org/</a></li>\n<li><a href=\"https://doi.org/10.1109/TMI.2004.825627\">https://doi.org/10.1109/TMI.2004.825627</a></li>\n</ul>\n\n<p>Please cite it if you use this dataset for a publication.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.drive.get_drive_loader", "modulename": "torch_em.data.datasets.medical.drive", "qualname": "get_drive_loader", "kind": "function", "doc": "<p>Dataloader for segmentation of retinal blood vessels in fundus images. See <code>get_drive_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.papila", "modulename": "torch_em.data.datasets.medical.papila", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.papila.URL", "modulename": "torch_em.data.datasets.medical.papila", "qualname": "URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://figshare.com/ndownloader/files/35013982&#x27;"}, {"fullname": "torch_em.data.datasets.medical.papila.CHECKSUM", "modulename": "torch_em.data.datasets.medical.papila", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;15b053dff496bc8e53eb8a8d0707ef73ba3d56c988eea92b65832c9c82852a7d&#x27;"}, {"fullname": "torch_em.data.datasets.medical.papila.get_papila_data", "modulename": "torch_em.data.datasets.medical.papila", "qualname": "get_papila_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">download</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.papila.contour_to_mask", "modulename": "torch_em.data.datasets.medical.papila", "qualname": "contour_to_mask", "kind": "function", "doc": "<p>Return mask given a contour and the shape of image</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cont</span>, </span><span class=\"param\"><span class=\"n\">img_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.papila.get_papila_dataset", "modulename": "torch_em.data.datasets.medical.papila", "qualname": "get_papila_dataset", "kind": "function", "doc": "<p>Dataset for segmentation of optic cup and optic disc in fundus images.</p>\n\n<p>The database is located at <a href=\"https://figshare.com/articles/dataset/PAPILA/14798004/2\">https://figshare.com/articles/dataset/PAPILA/14798004/2</a></p>\n\n<p>The dataset is from Kovalyk et al. - <a href=\"https://doi.org/10.1038/s41597-022-01388-1\">https://doi.org/10.1038/s41597-022-01388-1</a>.\nPlease cite it if you use this dataset for a publication.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">task</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;disc&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">expert_choice</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;exp1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.papila.get_papila_loader", "modulename": "torch_em.data.datasets.medical.papila", "qualname": "get_papila_loader", "kind": "function", "doc": "<p>Dataloader for segmentation of optic cup and optic disc in fundus images. See <code>get_papila_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">task</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;disc&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">expert_choice</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;exp1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.siim_acr", "modulename": "torch_em.data.datasets.medical.siim_acr", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.medical.siim_acr.KAGGLE_DATASET_NAME", "modulename": "torch_em.data.datasets.medical.siim_acr", "qualname": "KAGGLE_DATASET_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;vbookshelf/pneumothorax-chest-xray-images-and-masks&#x27;"}, {"fullname": "torch_em.data.datasets.medical.siim_acr.CHECKSUM", "modulename": "torch_em.data.datasets.medical.siim_acr", "qualname": "CHECKSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;1ade68d31adb996c531bb686fb9d02fe11876ddf6f25594ab725e18c69d81538&#x27;"}, {"fullname": "torch_em.data.datasets.medical.siim_acr.get_siim_acr_data", "modulename": "torch_em.data.datasets.medical.siim_acr", "qualname": "get_siim_acr_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">download</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.siim_acr.get_siim_acr_dataset", "modulename": "torch_em.data.datasets.medical.siim_acr", "qualname": "get_siim_acr_dataset", "kind": "function", "doc": "<p>Dataset for pneumothorax segmentation in CXR.</p>\n\n<p>The database is located at <a href=\"https://www.kaggle.com/datasets/vbookshelf/pneumothorax-chest-xray-images-and-masks/data\">https://www.kaggle.com/datasets/vbookshelf/pneumothorax-chest-xray-images-and-masks/data</a></p>\n\n<p>This dataset is from the \"SIIM-ACR Pneumothorax Segmentation\" competition:\n<a href=\"https://kaggle.com/competitions/siim-acr-pneumothorax-segmentation\">https://kaggle.com/competitions/siim-acr-pneumothorax-segmentation</a></p>\n\n<p>Please cite it if you use this dataset for a publication.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.medical.siim_acr.get_siim_acr_loader", "modulename": "torch_em.data.datasets.medical.siim_acr", "qualname": "get_siim_acr_loader", "kind": "function", "doc": "<p>Dataloader for pneumothorax segmentation in CXR. See <code>get_siim_acr_dataset</code> for details.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">download</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">resize_inputs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util", "modulename": "torch_em.data.datasets.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.datasets.util.BIOIMAGEIO_IDS", "modulename": "torch_em.data.datasets.util", "qualname": "BIOIMAGEIO_IDS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;covid_if&#x27;: &#x27;ilastik/covid_if_training_data&#x27;, &#x27;cremi&#x27;: &#x27;ilastik/cremi_training_data&#x27;, &#x27;dsb&#x27;: &#x27;ilastik/stardist_dsb_training_data&#x27;, &#x27;hpa&#x27;: &#x27;&#x27;, &#x27;isbi2012&#x27;: &#x27;ilastik/isbi2012_neuron_segmentation_challenge&#x27;, &#x27;kasthuri&#x27;: &#x27;&#x27;, &#x27;livecell&#x27;: &#x27;ilastik/livecell_dataset&#x27;, &#x27;lucchi&#x27;: &#x27;&#x27;, &#x27;mitoem&#x27;: &#x27;ilastik/mitoem_segmentation_challenge&#x27;, &#x27;monuseg&#x27;: &#x27;deepimagej/monuseg_digital_pathology_miccai2018&#x27;, &#x27;ovules&#x27;: &#x27;&#x27;, &#x27;plantseg_root&#x27;: &#x27;ilastik/plantseg_root&#x27;, &#x27;plantseg_ovules&#x27;: &#x27;ilastik/plantseg_ovules&#x27;, &#x27;platynereis&#x27;: &#x27;ilastik/platynereis_em_training_data&#x27;, &#x27;snemi&#x27;: &#x27;&#x27;, &#x27;uro_cell&#x27;: &#x27;&#x27;, &#x27;vnc&#x27;: &#x27;ilastik/vnc&#x27;}"}, {"fullname": "torch_em.data.datasets.util.get_bioimageio_dataset_id", "modulename": "torch_em.data.datasets.util", "qualname": "get_bioimageio_dataset_id", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset_name</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.get_checksum", "modulename": "torch_em.data.datasets.util", "qualname": "get_checksum", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">filename</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.download_source", "modulename": "torch_em.data.datasets.util", "qualname": "download_source", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">url</span>, </span><span class=\"param\"><span class=\"n\">download</span>, </span><span class=\"param\"><span class=\"n\">checksum</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">verify</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.download_source_gdrive", "modulename": "torch_em.data.datasets.util", "qualname": "download_source_gdrive", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span>,</span><span class=\"param\">\t<span class=\"n\">url</span>,</span><span class=\"param\">\t<span class=\"n\">download</span>,</span><span class=\"param\">\t<span class=\"n\">checksum</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">download_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;zip&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">expected_samples</span><span class=\"o\">=</span><span class=\"mi\">10000</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.download_source_empiar", "modulename": "torch_em.data.datasets.util", "qualname": "download_source_empiar", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">access_id</span>, </span><span class=\"param\"><span class=\"n\">download</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.download_source_kaggle", "modulename": "torch_em.data.datasets.util", "qualname": "download_source_kaggle", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">dataset_name</span>, </span><span class=\"param\"><span class=\"n\">download</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.update_kwargs", "modulename": "torch_em.data.datasets.util", "qualname": "update_kwargs", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">kwargs</span>, </span><span class=\"param\"><span class=\"n\">key</span>, </span><span class=\"param\"><span class=\"n\">value</span>, </span><span class=\"param\"><span class=\"n\">msg</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.unzip", "modulename": "torch_em.data.datasets.util", "qualname": "unzip", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">zip_path</span>, </span><span class=\"param\"><span class=\"n\">dst</span>, </span><span class=\"param\"><span class=\"n\">remove</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.split_kwargs", "modulename": "torch_em.data.datasets.util", "qualname": "split_kwargs", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">function</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.ensure_transforms", "modulename": "torch_em.data.datasets.util", "qualname": "ensure_transforms", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ndim</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.add_instance_label_transform", "modulename": "torch_em.data.datasets.util", "qualname": "add_instance_label_transform", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">kwargs</span>,</span><span class=\"param\">\t<span class=\"n\">add_binary_target</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">boundaries</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">binary_is_exclusive</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.generate_labeled_array_from_xml", "modulename": "torch_em.data.datasets.util", "qualname": "generate_labeled_array_from_xml", "kind": "function", "doc": "<p>Function taken from: <a href=\"https://github.com/rshwndsz/hover-net/blob/master/lightning_hovernet.ipynb\">https://github.com/rshwndsz/hover-net/blob/master/lightning_hovernet.ipynb</a></p>\n\n<p>Given image shape and path to annotations (xml file), generatebit mask with the region inside a contour being white\n    shape: The image shape on which bit mask will be made\n    xml_file: path relative to the current working directory where the xml file is present</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>An image of given shape with region inside contour being white..</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">xml_file</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.datasets.util.convert_svs_to_array", "modulename": "torch_em.data.datasets.util", "qualname": "convert_svs_to_array", "kind": "function", "doc": "<p>Converts .svs files to numpy array format</p>\n\n<h6 id=\"argument\">Argument:</h6>\n\n<blockquote>\n  <ul>\n  <li>path: [str] - Path to the svs file\n  (below mentioned arguments are used for multi-resolution images)</li>\n  <li>location: tuple[int, int] - pixel location (x, y) in level 0 of the image (default: (0, 0))</li>\n  <li>level: [int] -  target level used to read the image (default: 0)</li>\n  <li>img_size: tuple[int, int] - expected size of the image\n  (default: None -> obtains the original shape at the expected level)</li>\n  </ul>\n</blockquote>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>the image as numpy array</p>\n</blockquote>\n\n<p>TODO: it can be extended to convert WSIs (or modalities with multiple resolutions)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">location</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">img_size</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.image_collection_dataset", "modulename": "torch_em.data.image_collection_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><code>~torch.utils.data.DataLoader</code> by default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.__init__", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">label_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform2</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">full_check</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.max_sampling_attempts", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.max_sampling_attempts", "kind": "variable", "doc": "<p></p>\n", "default_value": "500"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.max_sampling_attempts_image", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.max_sampling_attempts_image", "kind": "variable", "doc": "<p></p>\n", "default_value": "50"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.raw_images", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.raw_images", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.label_images", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.label_images", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.patch_shape", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.patch_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.raw_transform", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.raw_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.label_transform", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.label_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.label_transform2", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.label_transform2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.transform", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.sampler", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.sampler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.dtype", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.label_dtype", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.label_dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.image_collection_dataset.ImageCollectionDataset.ndim", "modulename": "torch_em.data.image_collection_dataset", "qualname": "ImageCollectionDataset.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.pseudo_label_dataset", "modulename": "torch_em.data.pseudo_label_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.pseudo_label_dataset.PseudoLabelDataset", "modulename": "torch_em.data.pseudo_label_dataset", "qualname": "PseudoLabelDataset", "kind": "class", "doc": "<p></p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.data.pseudo_label_dataset.PseudoLabelDataset.__init__", "modulename": "torch_em.data.pseudo_label_dataset", "qualname": "PseudoLabelDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pseudo_labeler</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">roi</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">labeler_device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.data.pseudo_label_dataset.PseudoLabelDataset.pseudo_labeler", "modulename": "torch_em.data.pseudo_label_dataset", "qualname": "PseudoLabelDataset.pseudo_labeler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.pseudo_label_dataset.PseudoLabelDataset.label_transform", "modulename": "torch_em.data.pseudo_label_dataset", "qualname": "PseudoLabelDataset.label_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.pseudo_label_dataset.PseudoLabelDataset.labeler_device", "modulename": "torch_em.data.pseudo_label_dataset", "qualname": "PseudoLabelDataset.labeler_device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset", "modulename": "torch_em.data.raw_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset", "kind": "class", "doc": "<p></p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.__init__", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">roi</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">augmentations</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.max_sampling_attempts", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.max_sampling_attempts", "kind": "variable", "doc": "<p></p>\n", "default_value": "500"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.compute_len", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.compute_len", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">patch_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.raw_path", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.raw_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.raw_key", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.raw_key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.raw", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.raw", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.shape", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.roi", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.roi", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.patch_shape", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.patch_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.raw_transform", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.raw_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.transform", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.sampler", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.sampler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.dtype", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.augmentations", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.augmentations", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.sample_shape", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.sample_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.trafo_halo", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.trafo_halo", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.ndim", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_dataset.RawDataset.crop", "modulename": "torch_em.data.raw_dataset", "qualname": "RawDataset.crop", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.raw_image_collection_dataset", "modulename": "torch_em.data.raw_image_collection_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><code>~torch.utils.data.DataLoader</code> by default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.__init__", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_image_paths</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">augmentations</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">full_check</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.max_sampling_attempts", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.max_sampling_attempts", "kind": "variable", "doc": "<p></p>\n", "default_value": "500"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.raw_images", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.raw_images", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.patch_shape", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.patch_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.raw_transform", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.raw_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.transform", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.dtype", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.sampler", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.sampler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.augmentations", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.augmentations", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.raw_image_collection_dataset.RawImageCollectionDataset.ndim", "modulename": "torch_em.data.raw_image_collection_dataset", "qualname": "RawImageCollectionDataset.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler", "modulename": "torch_em.data.sampler", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinForegroundSampler", "modulename": "torch_em.data.sampler", "qualname": "MinForegroundSampler", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinForegroundSampler.__init__", "modulename": "torch_em.data.sampler", "qualname": "MinForegroundSampler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">background_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">p_reject</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "torch_em.data.sampler.MinForegroundSampler.min_fraction", "modulename": "torch_em.data.sampler", "qualname": "MinForegroundSampler.min_fraction", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinForegroundSampler.background_id", "modulename": "torch_em.data.sampler", "qualname": "MinForegroundSampler.background_id", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinForegroundSampler.p_reject", "modulename": "torch_em.data.sampler", "qualname": "MinForegroundSampler.p_reject", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinSemanticLabelForegroundSampler", "modulename": "torch_em.data.sampler", "qualname": "MinSemanticLabelForegroundSampler", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinSemanticLabelForegroundSampler.__init__", "modulename": "torch_em.data.sampler", "qualname": "MinSemanticLabelForegroundSampler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">semantic_ids</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">min_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">min_fraction_per_id</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">p_reject</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "torch_em.data.sampler.MinSemanticLabelForegroundSampler.semantic_ids", "modulename": "torch_em.data.sampler", "qualname": "MinSemanticLabelForegroundSampler.semantic_ids", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinSemanticLabelForegroundSampler.min_fraction", "modulename": "torch_em.data.sampler", "qualname": "MinSemanticLabelForegroundSampler.min_fraction", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinSemanticLabelForegroundSampler.p_reject", "modulename": "torch_em.data.sampler", "qualname": "MinSemanticLabelForegroundSampler.p_reject", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinSemanticLabelForegroundSampler.min_fraction_per_id", "modulename": "torch_em.data.sampler", "qualname": "MinSemanticLabelForegroundSampler.min_fraction_per_id", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinIntensitySampler", "modulename": "torch_em.data.sampler", "qualname": "MinIntensitySampler", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinIntensitySampler.__init__", "modulename": "torch_em.data.sampler", "qualname": "MinIntensitySampler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_intensity</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">function</span><span class=\"o\">=</span><span class=\"s1\">&#39;median&#39;</span>, </span><span class=\"param\"><span class=\"n\">p_reject</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "torch_em.data.sampler.MinIntensitySampler.min_intensity", "modulename": "torch_em.data.sampler", "qualname": "MinIntensitySampler.min_intensity", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinIntensitySampler.function", "modulename": "torch_em.data.sampler", "qualname": "MinIntensitySampler.function", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinIntensitySampler.p_reject", "modulename": "torch_em.data.sampler", "qualname": "MinIntensitySampler.p_reject", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinInstanceSampler", "modulename": "torch_em.data.sampler", "qualname": "MinInstanceSampler", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinInstanceSampler.__init__", "modulename": "torch_em.data.sampler", "qualname": "MinInstanceSampler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_num_instances</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">p_reject</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "torch_em.data.sampler.MinInstanceSampler.min_num_instances", "modulename": "torch_em.data.sampler", "qualname": "MinInstanceSampler.min_num_instances", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinInstanceSampler.p_reject", "modulename": "torch_em.data.sampler", "qualname": "MinInstanceSampler.p_reject", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinTwoInstanceSampler", "modulename": "torch_em.data.sampler", "qualname": "MinTwoInstanceSampler", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinTwoInstanceSampler.__init__", "modulename": "torch_em.data.sampler", "qualname": "MinTwoInstanceSampler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">p_reject</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "torch_em.data.sampler.MinTwoInstanceSampler.p_reject", "modulename": "torch_em.data.sampler", "qualname": "MinTwoInstanceSampler.p_reject", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinNoToBackgroundBoundarySampler", "modulename": "torch_em.data.sampler", "qualname": "MinNoToBackgroundBoundarySampler", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinNoToBackgroundBoundarySampler.__init__", "modulename": "torch_em.data.sampler", "qualname": "MinNoToBackgroundBoundarySampler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trafo</span>, </span><span class=\"param\"><span class=\"n\">min_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>, </span><span class=\"param\"><span class=\"n\">p_reject</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "torch_em.data.sampler.MinNoToBackgroundBoundarySampler.trafo", "modulename": "torch_em.data.sampler", "qualname": "MinNoToBackgroundBoundarySampler.trafo", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinNoToBackgroundBoundarySampler.bg_label", "modulename": "torch_em.data.sampler", "qualname": "MinNoToBackgroundBoundarySampler.bg_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinNoToBackgroundBoundarySampler.mask_label", "modulename": "torch_em.data.sampler", "qualname": "MinNoToBackgroundBoundarySampler.mask_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinNoToBackgroundBoundarySampler.min_fraction", "modulename": "torch_em.data.sampler", "qualname": "MinNoToBackgroundBoundarySampler.min_fraction", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.sampler.MinNoToBackgroundBoundarySampler.p_reject", "modulename": "torch_em.data.sampler", "qualname": "MinNoToBackgroundBoundarySampler.p_reject", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset", "modulename": "torch_em.data.segmentation_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset", "kind": "class", "doc": "<p></p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.__init__", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">label_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform2</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">roi</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">with_label_channels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.max_sampling_attempts", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.max_sampling_attempts", "kind": "variable", "doc": "<p></p>\n", "default_value": "500"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.compute_len", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.compute_len", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">patch_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.raw_path", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.raw_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.raw_key", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.raw_key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.raw", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.raw", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.label_path", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.label_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.label_key", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.label_key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.labels", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.labels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.shape", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.roi", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.roi", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.patch_shape", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.patch_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.raw_transform", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.raw_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.label_transform", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.label_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.label_transform2", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.label_transform2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.transform", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.sampler", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.sampler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.dtype", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.label_dtype", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.label_dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.sample_shape", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.sample_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.trafo_halo", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.trafo_halo", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.ndim", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.data.segmentation_dataset.SegmentationDataset.crop", "modulename": "torch_em.data.segmentation_dataset", "qualname": "SegmentationDataset.crop", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss", "modulename": "torch_em.loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.EMBEDDING_LOSSES", "modulename": "torch_em.loss", "qualname": "EMBEDDING_LOSSES", "kind": "variable", "doc": "<p></p>\n", "default_value": "(&lt;class &#x27;torch_em.loss.contrastive.ContrastiveLoss&#x27;&gt;, &lt;class &#x27;torch_em.loss.spoco_loss.SPOCOLoss&#x27;&gt;)"}, {"fullname": "torch_em.loss.affinity_side_loss", "modulename": "torch_em.loss.affinity_side_loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.affinity_side_loss.shift_tensor", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "shift_tensor", "kind": "function", "doc": "<p>Shift a tensor by the given (spatial) offset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>tensor [torch.Tensor] - 4D (=2 spatial dims) or 5D (=3 spatial dims) tensor.\nNeeds to be of float type.</li>\n<li>offset (tuple) - 2d or 3d spatial offset used for shifting the tensor</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tensor</span>, </span><span class=\"param\"><span class=\"n\">offset</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.affinity_side_loss.invert_offsets", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "invert_offsets", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offsets</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.affinity_side_loss.segmentation_to_affinities", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "segmentation_to_affinities", "kind": "function", "doc": "<p>Transform segmentation to affinities.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>segmentation [torch.tensor] - 4D (2 spatial dims) or 5D (3 spatial dims) segmentation tensor.\nThe channel axis (= dimension 1) needs to be a singleton.</li>\n<li>offsets [list[tuple]] - list of offsets for which to compute the affinities.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">segmentation</span>, </span><span class=\"param\"><span class=\"n\">offsets</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.affinity_side_loss.embeddings_to_affinities", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "embeddings_to_affinities", "kind": "function", "doc": "<p>Transform embeddings to affinities.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">delta</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.affinity_side_loss.AffinitySideLoss", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "AffinitySideLoss", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.affinity_side_loss.AffinitySideLoss.__init__", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "AffinitySideLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offset_ranges</span>, </span><span class=\"param\"><span class=\"n\">n_samples</span>, </span><span class=\"param\"><span class=\"n\">delta</span></span>)</span>"}, {"fullname": "torch_em.loss.affinity_side_loss.AffinitySideLoss.ndim", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "AffinitySideLoss.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.affinity_side_loss.AffinitySideLoss.offset_ranges", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "AffinitySideLoss.offset_ranges", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.affinity_side_loss.AffinitySideLoss.n_samples", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "AffinitySideLoss.n_samples", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.affinity_side_loss.AffinitySideLoss.delta", "modulename": "torch_em.loss.affinity_side_loss", "qualname": "AffinitySideLoss.delta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.combined_loss", "modulename": "torch_em.loss.combined_loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.combined_loss.CombinedLoss", "modulename": "torch_em.loss.combined_loss", "qualname": "CombinedLoss", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.combined_loss.CombinedLoss.__init__", "modulename": "torch_em.loss.combined_loss", "qualname": "CombinedLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">losses</span>, </span><span class=\"param\"><span class=\"n\">loss_weights</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.loss.combined_loss.CombinedLoss.losses", "modulename": "torch_em.loss.combined_loss", "qualname": "CombinedLoss.losses", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.combined_loss.CombinedLoss.forward", "modulename": "torch_em.loss.combined_loss", "qualname": "CombinedLoss.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.contrastive", "modulename": "torch_em.loss.contrastive", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.check_consecutive", "modulename": "torch_em.loss.contrastive", "qualname": "check_consecutive", "kind": "function", "doc": "<p>Check that the input labels are consecutive and start at zero.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss", "kind": "class", "doc": "<p>Implementation of contrastive loss defined in <a href=\"https://arxiv.org/pdf/1708.02551.pdf\">https://arxiv.org/pdf/1708.02551.pdf</a>\nSemantic Instance Segmentation with a Discriminative Loss Function</p>\n\n<p>This class contians different implementations for the discrimnative loss:</p>\n\n<ul>\n<li>based on pure pytorch, expanding the instance dimension, this is not memory efficient</li>\n<li>based on pytorch_scatter (https://github.com/rusty1s/pytorch_scatter), this is memory efficient</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>delta_var [float] -</li>\n<li>delta_dist [float] -</li>\n<li>norm [str] -</li>\n<li>aplpha [float] -</li>\n<li>beta [float] -</li>\n<li>gamma [float] -</li>\n<li>ignore_label [int] -</li>\n<li>impl [str] -</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.__init__", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">delta_var</span>,</span><span class=\"param\">\t<span class=\"n\">delta_dist</span>,</span><span class=\"param\">\t<span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;fro&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_label</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">impl</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.implementations", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.implementations", "kind": "variable", "doc": "<p></p>\n", "default_value": "(None, &#x27;scatter&#x27;, &#x27;expand&#x27;)"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.delta_var", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.delta_var", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.delta_dist", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.delta_dist", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.norm", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.norm", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.alpha", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.beta", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.beta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.gamma", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.gamma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.ignore_label", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.ignore_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.init_kwargs", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.has_torch_scatter", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.has_torch_scatter", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.contrastive.ContrastiveLoss.forward", "modulename": "torch_em.loss.contrastive", "qualname": "ContrastiveLoss.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.contrastive_impl", "modulename": "torch_em.loss.contrastive_impl", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.contrastive_impl.expand_as_one_hot", "modulename": "torch_em.loss.contrastive_impl", "qualname": "expand_as_one_hot", "kind": "function", "doc": "<p>Converts NxSPATIAL label image to NxCxSPATIAL, where each label gets converted to its corresponding one-hot vector.\nNOTE: make sure that the input_ contains consecutive numbers starting from 0, otherwise the scatter_ function\nwon't work.</p>\n\n<p>SPATIAL = DxHxW in case of 3D or SPATIAL = HxW in case of 2D</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>input_</strong>:  3D or 4D label image (NxSPATIAL)</li>\n<li><strong>C</strong>:  number of channels/labels</li>\n<li><strong>ignore_label</strong>:  ignore index to be kept during the expansion</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>4D or 5D output image (NxCxSPATIAL)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">C</span>, </span><span class=\"param\"><span class=\"n\">ignore_label</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.dice", "modulename": "torch_em.loss.dice", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.flatten_samples", "modulename": "torch_em.loss.dice", "qualname": "flatten_samples", "kind": "function", "doc": "<p>Flattens a tensor or a variable such that the channel axis is first and the sample axis\nis second. The shapes are transformed as follows:\n    (N, C, H, W) --> (C, N * H * W)\n    (N, C, D, H, W) --> (C, N * D * H * W)\n    (N, C) --> (C, N)\nThe input must be atleast 2d.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.dice.dice_score", "modulename": "torch_em.loss.dice", "qualname": "dice_score", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_</span>,</span><span class=\"param\">\t<span class=\"n\">target</span>,</span><span class=\"param\">\t<span class=\"n\">invert</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">channelwise</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">reduce_channel</span><span class=\"o\">=</span><span class=\"s1\">&#39;sum&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.dice.DiceLoss", "modulename": "torch_em.loss.dice", "qualname": "DiceLoss", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.dice.DiceLoss.__init__", "modulename": "torch_em.loss.dice", "qualname": "DiceLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">channelwise</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span>, </span><span class=\"param\"><span class=\"n\">reduce_channel</span><span class=\"o\">=</span><span class=\"s1\">&#39;sum&#39;</span></span>)</span>"}, {"fullname": "torch_em.loss.dice.DiceLoss.channelwise", "modulename": "torch_em.loss.dice", "qualname": "DiceLoss.channelwise", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLoss.eps", "modulename": "torch_em.loss.dice", "qualname": "DiceLoss.eps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLoss.reduce_channel", "modulename": "torch_em.loss.dice", "qualname": "DiceLoss.reduce_channel", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLoss.init_kwargs", "modulename": "torch_em.loss.dice", "qualname": "DiceLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLoss.forward", "modulename": "torch_em.loss.dice", "qualname": "DiceLoss.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.dice.DiceLossWithLogits", "modulename": "torch_em.loss.dice", "qualname": "DiceLossWithLogits", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.dice.DiceLossWithLogits.__init__", "modulename": "torch_em.loss.dice", "qualname": "DiceLossWithLogits.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">channelwise</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span>, </span><span class=\"param\"><span class=\"n\">reduce_channel</span><span class=\"o\">=</span><span class=\"s1\">&#39;sum&#39;</span></span>)</span>"}, {"fullname": "torch_em.loss.dice.DiceLossWithLogits.channelwise", "modulename": "torch_em.loss.dice", "qualname": "DiceLossWithLogits.channelwise", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLossWithLogits.eps", "modulename": "torch_em.loss.dice", "qualname": "DiceLossWithLogits.eps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLossWithLogits.reduce_channel", "modulename": "torch_em.loss.dice", "qualname": "DiceLossWithLogits.reduce_channel", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLossWithLogits.init_kwargs", "modulename": "torch_em.loss.dice", "qualname": "DiceLossWithLogits.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.DiceLossWithLogits.forward", "modulename": "torch_em.loss.dice", "qualname": "DiceLossWithLogits.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss.__init__", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">channelwise</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span></span>)</span>"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss.alpha", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss.beta", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss.beta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss.channelwise", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss.channelwise", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss.eps", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss.eps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss.init_kwargs", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLoss.forward", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLoss.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits.__init__", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">channelwise</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span></span>)</span>"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits.alpha", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits.beta", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits.beta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits.channelwise", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits.channelwise", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits.eps", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits.eps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits.init_kwargs", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.dice.BCEDiceLossWithLogits.forward", "modulename": "torch_em.loss.dice", "qualname": "BCEDiceLossWithLogits.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.distance_based", "modulename": "torch_em.loss.distance_based", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.distance_based.DistanceLoss", "modulename": "torch_em.loss.distance_based", "qualname": "DistanceLoss", "kind": "class", "doc": "<p>Loss for distance based instance segmentation.</p>\n\n<p>Expects input and targets with three channels: foreground and two distance channels.\nTypically the distance channels are centroid and inverted boundary distance.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask_distances_in_bg:</strong>  whether to mask the loss for distance predictions in the background.</li>\n<li><strong>foreground_loss:</strong>  the loss for comparing foreground predictions and target.</li>\n<li><strong>distance_loss:</strong>  the loss for comparing distance predictions and target.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.distance_based.DistanceLoss.__init__", "modulename": "torch_em.loss.distance_based", "qualname": "DistanceLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mask_distances_in_bg</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">=</span> <span class=\"n\">DiceLoss</span><span class=\"p\">()</span>,</span><span class=\"param\">\t<span class=\"n\">distance_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">=</span> <span class=\"n\">MSELoss</span><span class=\"p\">()</span></span>)</span>"}, {"fullname": "torch_em.loss.distance_based.DistanceLoss.foreground_loss", "modulename": "torch_em.loss.distance_based", "qualname": "DistanceLoss.foreground_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.distance_based.DistanceLoss.distance_loss", "modulename": "torch_em.loss.distance_based", "qualname": "DistanceLoss.distance_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.distance_based.DistanceLoss.mask_distances_in_bg", "modulename": "torch_em.loss.distance_based", "qualname": "DistanceLoss.mask_distances_in_bg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.distance_based.DistanceLoss.init_kwargs", "modulename": "torch_em.loss.distance_based", "qualname": "DistanceLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.distance_based.DistanceLoss.forward", "modulename": "torch_em.loss.distance_based", "qualname": "DistanceLoss.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.distance_based.DiceBasedDistanceLoss", "modulename": "torch_em.loss.distance_based", "qualname": "DiceBasedDistanceLoss", "kind": "class", "doc": "<p>Similar to DistanceLoss and uses dice for all losses.</p>\n", "bases": "DistanceLoss"}, {"fullname": "torch_em.loss.distance_based.DiceBasedDistanceLoss.__init__", "modulename": "torch_em.loss.distance_based", "qualname": "DiceBasedDistanceLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mask_distances_in_bg</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span>)</span>"}, {"fullname": "torch_em.loss.spoco_loss", "modulename": "torch_em.loss.spoco_loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.compute_cluster_means", "modulename": "torch_em.loss.spoco_loss", "qualname": "compute_cluster_means", "kind": "function", "doc": "<p>Computes mean embeddings per instance.\nE - embedding dimension</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings:</strong>  tensor of pixel embeddings, shape: ExSPATIAL</li>\n<li><strong>target:</strong>  one-hot encoded target instances, shape: SPATIAL</li>\n<li><strong>n_instances:</strong>  number of instances</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">n_instances</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.select_stable_anchor", "modulename": "torch_em.loss.spoco_loss", "qualname": "select_stable_anchor", "kind": "function", "doc": "<p>Anchor sampling procedure. Given a binary mask of an object (<code>object_mask</code>) and a <code>mean_embedding</code> vector within\nthe mask, the function selects a pixel from the mask at random and returns its embedding only if it\"s closer than\n<code>delta_var</code> from the <code>mean_embedding</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings (torch.Tensor):</strong>  ExSpatial vector field of an image</li>\n<li><strong>mean_embedding (torch.Tensor):</strong>  E-dimensional mean of embeddings lying within the <code>object_mask</code></li>\n<li><strong>object_mask (torch.Tensor):</strong>  binary image of a selected object</li>\n<li><strong>delta_var (float):</strong>  contrastive loss, pull force margin</li>\n<li><strong>norm (str):</strong>  vector norm used, default: Frobenius norm</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>embedding of a selected pixel within the mask or the mean embedding if stable anchor could be found</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">mean_embedding</span>, </span><span class=\"param\"><span class=\"n\">object_mask</span>, </span><span class=\"param\"><span class=\"n\">delta_var</span>, </span><span class=\"param\"><span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;fro&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.GaussianKernel", "modulename": "torch_em.loss.spoco_loss", "qualname": "GaussianKernel", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.spoco_loss.GaussianKernel.__init__", "modulename": "torch_em.loss.spoco_loss", "qualname": "GaussianKernel.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">delta_var</span>, </span><span class=\"param\"><span class=\"n\">pmaps_threshold</span></span>)</span>"}, {"fullname": "torch_em.loss.spoco_loss.GaussianKernel.delta_var", "modulename": "torch_em.loss.spoco_loss", "qualname": "GaussianKernel.delta_var", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.GaussianKernel.two_sigma", "modulename": "torch_em.loss.spoco_loss", "qualname": "GaussianKernel.two_sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.GaussianKernel.forward", "modulename": "torch_em.loss.spoco_loss", "qualname": "GaussianKernel.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dist_map</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.CombinedAuxLoss", "modulename": "torch_em.loss.spoco_loss", "qualname": "CombinedAuxLoss", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.spoco_loss.CombinedAuxLoss.__init__", "modulename": "torch_em.loss.spoco_loss", "qualname": "CombinedAuxLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">losses</span>, </span><span class=\"param\"><span class=\"n\">weights</span></span>)</span>"}, {"fullname": "torch_em.loss.spoco_loss.CombinedAuxLoss.losses", "modulename": "torch_em.loss.spoco_loss", "qualname": "CombinedAuxLoss.losses", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.CombinedAuxLoss.weights", "modulename": "torch_em.loss.spoco_loss", "qualname": "CombinedAuxLoss.weights", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.CombinedAuxLoss.forward", "modulename": "torch_em.loss.spoco_loss", "qualname": "CombinedAuxLoss.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">instance_pmaps</span>, </span><span class=\"param\"><span class=\"n\">instance_masks</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase", "kind": "class", "doc": "<p>Base class for the spoco losses.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.__init__", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">delta_var</span>,</span><span class=\"param\">\t<span class=\"n\">delta_dist</span>,</span><span class=\"param\">\t<span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;fro&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">unlabeled_push_weight</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">instance_term_weight</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">impl</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.delta_var", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.delta_var", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.delta_dist", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.delta_dist", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.norm", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.norm", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.alpha", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.beta", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.beta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.gamma", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.gamma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.unlabeled_push_weight", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.unlabeled_push_weight", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.unlabeled_push", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.unlabeled_push", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.instance_term_weight", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.instance_term_weight", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.compute_instance_term", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.compute_instance_term", "kind": "function", "doc": "<p>Computes auxiliary loss based on embeddings and a given list of target\ninstances together with their mean embeddings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings (torch.tensor):</strong>  pixel embeddings (ExSPATIAL)</li>\n<li><strong>cluster_means (torch.tensor):</strong>  mean embeddings per instance (CxExSINGLETON_SPATIAL)</li>\n<li><strong>target (torch.tensor):</strong>  ground truth instance segmentation (SPATIAL)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>float: value of the instance-based term</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">cluster_means</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.ContrastiveLossBase.forward", "modulename": "torch_em.loss.spoco_loss", "qualname": "ContrastiveLossBase.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_ (torch.tensor):</strong>  embeddings predicted by the network (NxExDxHxW) (E - embedding dims)\nexpects float32 tensor</li>\n<li><strong>target (torch.tensor):</strong>  ground truth instance segmentation (Nx1DxHxW)\nexpects int64 tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Combined loss defined as: alpha * variance_term + beta * distance_term + gamma * regularization_term\n      + instance_term_weight * instance_term + unlabeled_push_weight * unlabeled_push_term</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.ExtendedContrastiveLoss", "modulename": "torch_em.loss.spoco_loss", "qualname": "ExtendedContrastiveLoss", "kind": "class", "doc": "<p>Contrastive loss extended with instance-based loss term and background push term.</p>\n\n<p>Based on:\n\"Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings\": <a href=\"https://arxiv.org/abs/2103.14572\">https://arxiv.org/abs/2103.14572</a></p>\n", "bases": "ContrastiveLossBase"}, {"fullname": "torch_em.loss.spoco_loss.ExtendedContrastiveLoss.__init__", "modulename": "torch_em.loss.spoco_loss", "qualname": "ExtendedContrastiveLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">delta_var</span>,</span><span class=\"param\">\t<span class=\"n\">delta_dist</span>,</span><span class=\"param\">\t<span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;fro&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">unlabeled_push_weight</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">instance_term_weight</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">aux_loss</span><span class=\"o\">=</span><span class=\"s1\">&#39;dice&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">pmaps_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.9</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.loss.spoco_loss.ExtendedContrastiveLoss.dist_to_mask", "modulename": "torch_em.loss.spoco_loss", "qualname": "ExtendedContrastiveLoss.dist_to_mask", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ExtendedContrastiveLoss.init_kwargs", "modulename": "torch_em.loss.spoco_loss", "qualname": "ExtendedContrastiveLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.ExtendedContrastiveLoss.compute_instance_term", "modulename": "torch_em.loss.spoco_loss", "qualname": "ExtendedContrastiveLoss.compute_instance_term", "kind": "function", "doc": "<p>Computes auxiliary loss based on embeddings and a given list of target\ninstances together with their mean embeddings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings (torch.tensor):</strong>  pixel embeddings (ExSPATIAL)</li>\n<li><strong>cluster_means (torch.tensor):</strong>  mean embeddings per instance (CxExSINGLETON_SPATIAL)</li>\n<li><strong>target (torch.tensor):</strong>  ground truth instance segmentation (SPATIAL)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>float: value of the instance-based term</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">cluster_means</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss", "kind": "class", "doc": "<p>The full SPOCO Loss for instance segmentation training with sparse instance labels.</p>\n\n<p>Extends the \"classic\" contrastive loss with an instance-based term and a embedding consistency term.\n(The unlabeled push term is turned off by default, since we assume sparse instance labels).</p>\n\n<p>Based on:\n\"Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings\": <a href=\"https://arxiv.org/abs/2103.14572\">https://arxiv.org/abs/2103.14572</a></p>\n", "bases": "ExtendedContrastiveLoss"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.__init__", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">delta_var</span>,</span><span class=\"param\">\t<span class=\"n\">delta_dist</span>,</span><span class=\"param\">\t<span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;fro&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">unlabeled_push_weight</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">instance_term_weight</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">consistency_term_weight</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">aux_loss</span><span class=\"o\">=</span><span class=\"s1\">&#39;dice&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">pmaps_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.9</span>,</span><span class=\"param\">\t<span class=\"n\">max_anchors</span><span class=\"o\">=</span><span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">volume_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.05</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.consistency_term_weight", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.consistency_term_weight", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.max_anchors", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.max_anchors", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.volume_threshold", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.volume_threshold", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.consistency_loss", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.consistency_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.init_kwargs", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.emb_consistency", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.emb_consistency", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">emb_q</span>, </span><span class=\"param\"><span class=\"n\">emb_k</span>, </span><span class=\"param\"><span class=\"n\">mask</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOLoss.forward", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOLoss.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_ (torch.tensor):</strong>  embeddings predicted by the network (NxExDxHxW) (E - embedding dims)\nexpects float32 tensor</li>\n<li><strong>target (torch.tensor):</strong>  ground truth instance segmentation (Nx1DxHxW)\nexpects int64 tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Combined loss defined as: alpha * variance_term + beta * distance_term + gamma * regularization_term\n      + instance_term_weight * instance_term + unlabeled_push_weight * unlabeled_push_term</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.__init__", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">delta_var</span>, </span><span class=\"param\"><span class=\"n\">pmaps_threshold</span>, </span><span class=\"param\"><span class=\"n\">max_anchors</span><span class=\"o\">=</span><span class=\"mi\">30</span>, </span><span class=\"param\"><span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;fro&#39;</span></span>)</span>"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.max_anchors", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.max_anchors", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.consistency_loss", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.consistency_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.norm", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.norm", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.dist_to_mask", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.dist_to_mask", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.init_kwargs", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.emb_consistency", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.emb_consistency", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">emb_q</span>, </span><span class=\"param\"><span class=\"n\">emb_k</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.spoco_loss.SPOCOConsistencyLoss.forward", "modulename": "torch_em.loss.spoco_loss", "qualname": "SPOCOConsistencyLoss.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">emb_q</span>, </span><span class=\"param\"><span class=\"n\">emb_k</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.wrapper", "modulename": "torch_em.loss.wrapper", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.LossWrapper", "modulename": "torch_em.loss.wrapper", "qualname": "LossWrapper", "kind": "class", "doc": "<p>Wrapper around a torch loss function.</p>\n\n<p>Applies transformations to prediction and/or target before passing it to the loss.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.loss.wrapper.LossWrapper.__init__", "modulename": "torch_em.loss.wrapper", "qualname": "LossWrapper.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">transform</span></span>)</span>"}, {"fullname": "torch_em.loss.wrapper.LossWrapper.loss", "modulename": "torch_em.loss.wrapper", "qualname": "LossWrapper.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.LossWrapper.transform", "modulename": "torch_em.loss.wrapper", "qualname": "LossWrapper.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.LossWrapper.init_kwargs", "modulename": "torch_em.loss.wrapper", "qualname": "LossWrapper.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.LossWrapper.apply_transform", "modulename": "torch_em.loss.wrapper", "qualname": "LossWrapper.apply_transform", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.wrapper.LossWrapper.forward", "modulename": "torch_em.loss.wrapper", "qualname": "LossWrapper.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.loss.wrapper.ApplyMask", "modulename": "torch_em.loss.wrapper", "qualname": "ApplyMask", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.ApplyMask.__init__", "modulename": "torch_em.loss.wrapper", "qualname": "ApplyMask.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">masking_method</span><span class=\"o\">=</span><span class=\"s1\">&#39;crop&#39;</span>, </span><span class=\"param\"><span class=\"n\">channel_dim</span><span class=\"o\">=</span><span class=\"mi\">1</span></span>)</span>"}, {"fullname": "torch_em.loss.wrapper.ApplyMask.MASKING_FUNCS", "modulename": "torch_em.loss.wrapper", "qualname": "ApplyMask.MASKING_FUNCS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;crop&#x27;: &lt;function ApplyMask._crop&gt;, &#x27;multiply&#x27;: &lt;function ApplyMask._multiply&gt;}"}, {"fullname": "torch_em.loss.wrapper.ApplyMask.masking_func", "modulename": "torch_em.loss.wrapper", "qualname": "ApplyMask.masking_func", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.ApplyMask.channel_dim", "modulename": "torch_em.loss.wrapper", "qualname": "ApplyMask.channel_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.ApplyMask.init_kwargs", "modulename": "torch_em.loss.wrapper", "qualname": "ApplyMask.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.loss.wrapper.ApplyAndRemoveMask", "modulename": "torch_em.loss.wrapper", "qualname": "ApplyAndRemoveMask", "kind": "class", "doc": "<p></p>\n", "bases": "ApplyMask"}, {"fullname": "torch_em.loss.wrapper.MaskIgnoreLabel", "modulename": "torch_em.loss.wrapper", "qualname": "MaskIgnoreLabel", "kind": "class", "doc": "<p></p>\n", "bases": "ApplyMask"}, {"fullname": "torch_em.loss.wrapper.MaskIgnoreLabel.__init__", "modulename": "torch_em.loss.wrapper", "qualname": "MaskIgnoreLabel.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ignore_label</span><span class=\"o\">=-</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">masking_method</span><span class=\"o\">=</span><span class=\"s1\">&#39;crop&#39;</span>, </span><span class=\"param\"><span class=\"n\">channel_dim</span><span class=\"o\">=</span><span class=\"mi\">1</span></span>)</span>"}, {"fullname": "torch_em.loss.wrapper.MaskIgnoreLabel.ignore_label", "modulename": "torch_em.loss.wrapper", "qualname": "MaskIgnoreLabel.ignore_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric", "modulename": "torch_em.metric", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric", "modulename": "torch_em.metric.instance_segmentation_metric", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.BaseInstanceSegmentationMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "BaseInstanceSegmentationMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.metric.instance_segmentation_metric.BaseInstanceSegmentationMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "BaseInstanceSegmentationMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">segmenter</span>, </span><span class=\"param\"><span class=\"n\">metric</span>, </span><span class=\"param\"><span class=\"n\">to_numpy</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.BaseInstanceSegmentationMetric.segmenter", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "BaseInstanceSegmentationMetric.segmenter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.BaseInstanceSegmentationMetric.metric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "BaseInstanceSegmentationMetric.metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.BaseInstanceSegmentationMetric.to_numpy", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "BaseInstanceSegmentationMetric.to_numpy", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.BaseInstanceSegmentationMetric.forward", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "BaseInstanceSegmentationMetric.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.metric.instance_segmentation_metric.filter_sizes", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "filter_sizes", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">seg</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">hmap</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWS", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWS", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWS.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">with_background</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWS.offsets", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWS.offsets", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWS.with_background", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWS.with_background", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWS.min_seg_size", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWS.min_seg_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWS.strides", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWS.strides", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">delta</span>, </span><span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">with_background</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS.delta", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS.delta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS.offsets", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS.offsets", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS.with_background", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS.with_background", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS.min_seg_size", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS.min_seg_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS.strides", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS.strides", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWS.merge_background", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWS.merge_background", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">seg</span>, </span><span class=\"param\"><span class=\"n\">embeddings</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.metric.instance_segmentation_metric.Multicut", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "Multicut", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.Multicut.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "Multicut.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">min_seg_size</span>,</span><span class=\"param\">\t<span class=\"n\">anisotropic</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">dt_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.25</span>,</span><span class=\"param\">\t<span class=\"n\">sigma_seeds</span><span class=\"o\">=</span><span class=\"mf\">2.0</span>,</span><span class=\"param\">\t<span class=\"n\">solver</span><span class=\"o\">=</span><span class=\"s1\">&#39;decomposition&#39;</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.Multicut.min_seg_size", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "Multicut.min_seg_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.Multicut.anisotropic", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "Multicut.anisotropic", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.Multicut.dt_threshold", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "Multicut.dt_threshold", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.Multicut.sigma_seeds", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "Multicut.sigma_seeds", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.Multicut.solver", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "Multicut.solver", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScan", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScan", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScan.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScan.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_size</span>, </span><span class=\"param\"><span class=\"n\">eps</span>, </span><span class=\"param\"><span class=\"n\">remove_largest</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScan.min_size", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScan.min_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScan.eps", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScan.eps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScan.remove_largest", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScan.remove_largest", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.IOUError", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "IOUError", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.IOUError.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "IOUError.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>, </span><span class=\"param\"><span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"s1\">&#39;precision&#39;</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.IOUError.threshold", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "IOUError.threshold", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.IOUError.metric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "IOUError.metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.VariationOfInformation", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "VariationOfInformation", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.AdaptedRandError", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "AdaptedRandError", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.SymmetricBestDice", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "SymmetricBestDice", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSIOUMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSIOUMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSIOUMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSIOUMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">delta</span>, </span><span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">iou_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSIOUMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSIOUMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSSBDMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSSBDMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSSBDMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSSBDMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">delta</span>, </span><span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSSBDMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSSBDMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSVOIMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSVOIMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSVOIMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSVOIMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">delta</span>, </span><span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSVOIMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSVOIMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSRandMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSRandMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSRandMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSRandMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">delta</span>, </span><span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.EmbeddingMWSRandMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "EmbeddingMWSRandMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanIOUMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanIOUMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanIOUMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanIOUMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_size</span>, </span><span class=\"param\"><span class=\"n\">eps</span>, </span><span class=\"param\"><span class=\"n\">iou_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanIOUMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanIOUMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanSBDMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanSBDMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanSBDMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanSBDMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_size</span>, </span><span class=\"param\"><span class=\"n\">eps</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanSBDMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanSBDMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanRandMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanRandMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanRandMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanRandMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_size</span>, </span><span class=\"param\"><span class=\"n\">eps</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanRandMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanRandMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanVOIMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanVOIMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanVOIMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanVOIMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_size</span>, </span><span class=\"param\"><span class=\"n\">eps</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.HDBScanVOIMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "HDBScanVOIMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MulticutVOIMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MulticutVOIMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MulticutVOIMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MulticutVOIMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">anisotropic</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">dt_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.25</span>, </span><span class=\"param\"><span class=\"n\">sigma_seeds</span><span class=\"o\">=</span><span class=\"mf\">2.0</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MulticutVOIMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MulticutVOIMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MulticutRandMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MulticutRandMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MulticutRandMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MulticutRandMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">anisotropic</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">dt_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.25</span>, </span><span class=\"param\"><span class=\"n\">sigma_seeds</span><span class=\"o\">=</span><span class=\"mf\">2.0</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MulticutRandMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MulticutRandMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSIOUMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSIOUMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSIOUMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSIOUMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">iou_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSIOUMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSIOUMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSSBDMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSSBDMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSSBDMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSSBDMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSSBDMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSSBDMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSVOIMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSVOIMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSVOIMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSVOIMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSVOIMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSVOIMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSRandMetric", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSRandMetric", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "BaseInstanceSegmentationMetric"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSRandMetric.__init__", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSRandMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">min_seg_size</span>, </span><span class=\"param\"><span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.metric.instance_segmentation_metric.MWSRandMetric.init_kwargs", "modulename": "torch_em.metric.instance_segmentation_metric", "qualname": "MWSRandMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model", "modulename": "torch_em.model", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet", "modulename": "torch_em.model.probabilistic_unet", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.truncated_normal_", "modulename": "torch_em.model.probabilistic_unet", "qualname": "truncated_normal_", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tensor</span>, </span><span class=\"param\"><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.init_weights", "modulename": "torch_em.model.probabilistic_unet", "qualname": "init_weights", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">m</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.init_weights_orthogonal_normal", "modulename": "torch_em.model.probabilistic_unet", "qualname": "init_weights_orthogonal_normal", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">m</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.Encoder", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Encoder", "kind": "class", "doc": "<p>A convolutional neural network, consisting of len(num_filters) times a block of no_convs_per_block\nconvolutional layers, after each block a pooling operation is performed.\nAnd after each convolutional layer a non-linear (ReLU) activation function is applied.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.probabilistic_unet.Encoder.__init__", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Encoder.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span>,</span><span class=\"param\">\t<span class=\"n\">no_convs_per_block</span>,</span><span class=\"param\">\t<span class=\"n\">initializers</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">posterior</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.model.probabilistic_unet.Encoder.contracting_path", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Encoder.contracting_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Encoder.input_channels", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Encoder.input_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Encoder.num_filters", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Encoder.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Encoder.layers", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Encoder.layers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Encoder.forward", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Encoder.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian", "kind": "class", "doc": "<p>A convolutional net that parametrizes a Gaussian distribution with axis aligned covariance matrix.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.__init__", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span>,</span><span class=\"param\">\t<span class=\"n\">no_convs_per_block</span>,</span><span class=\"param\">\t<span class=\"n\">latent_dim</span>,</span><span class=\"param\">\t<span class=\"n\">initializers</span>,</span><span class=\"param\">\t<span class=\"n\">posterior</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.input_channels", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.input_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.channel_axis", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.channel_axis", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.num_filters", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.no_convs_per_block", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.no_convs_per_block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.latent_dim", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.latent_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.posterior", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.posterior", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.encoder", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.conv_layer", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.conv_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.show_img", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.show_img", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.show_seg", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.show_seg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.show_concat", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.show_concat", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.show_enc", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.show_enc", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.sum_input", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.sum_input", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.AxisAlignedConvGaussian.forward", "modulename": "torch_em.model.probabilistic_unet", "qualname": "AxisAlignedConvGaussian.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span>, </span><span class=\"param\"><span class=\"n\">segm</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb", "kind": "class", "doc": "<p>A function composed of no_convs_fcomb times a 1x1 convolution that combines the sample taken from the latent space,\nand output of the UNet (the feature map) by concatenating them along their channel axis.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.__init__", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">num_filters</span>,</span><span class=\"param\">\t<span class=\"n\">latent_dim</span>,</span><span class=\"param\">\t<span class=\"n\">num_output_channels</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span>,</span><span class=\"param\">\t<span class=\"n\">no_convs_fcomb</span>,</span><span class=\"param\">\t<span class=\"n\">initializers</span>,</span><span class=\"param\">\t<span class=\"n\">use_tile</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.num_channels", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.num_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.num_classes", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.num_classes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.channel_axis", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.channel_axis", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.spatial_axes", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.spatial_axes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.num_filters", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.latent_dim", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.latent_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.use_tile", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.use_tile", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.no_convs_fcomb", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.no_convs_fcomb", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.name", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.tile", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.tile", "kind": "function", "doc": "<p>This function is taken form PyTorch forum and mimics the behavior of tf.tile.\nSource: <a href=\"https://discuss.pytorch.org/t/how-to-tile-a-tensor/13853/3\">https://discuss.pytorch.org/t/how-to-tile-a-tensor/13853/3</a></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">a</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">n_tile</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.Fcomb.forward", "modulename": "torch_em.model.probabilistic_unet", "qualname": "Fcomb.forward", "kind": "function", "doc": "<p>Z is (batch_size x latent_dim) and feature_map is (batch_size x no_channels x H x W).\nSo broadcast Z to batch_sizexlatent_dimxHxW. Behavior is exactly the same as tf.tile (verified)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature_map</span>, </span><span class=\"param\"><span class=\"n\">z</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet", "kind": "class", "doc": "<p>This network implementation for the Probabilistic UNet of Kohl et al. (https://arxiv.org/abs/1806.05034).\nThis generative segmentation heuristic uses UNet combined with a conditional variational\nautoencoder enabling to efficiently produce an unlimited number of plausible hypotheses.</p>\n\n<p>The following elements are initialized to get our desired network:\ninput_channels: the number of channels in the image (1 for grayscale and 3 for RGB)\nnum_classes: the number of classes to predict\nnum_filters: is a list consisting of the amount of filters layer\nlatent_dim: dimension of the latent space\nno_cons_per_block: no convs per block in the (convolutional) encoder of prior and posterior\nbeta: KL and reconstruction loss are weighted using a KL weighting factor (\u03b2)\nconsensus_masking: activates consensus masking in the reconstruction loss\nrl_swap: switches the reconstruction loss to dice loss from the default (binary cross-entroy loss)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_channels [int] - (default:</strong>  1)</li>\n<li><strong>num_classes [int] - (default:</strong>  1)</li>\n<li><strong>num_filters [list] - (default:</strong>  [32, 64, 128, 192])</li>\n<li><strong>latent_dim [int] - (default:</strong>  6)</li>\n<li><strong>no_convs_fcomb [int] - (default:</strong>  4)</li>\n<li><strong>beta [float] - (default:</strong>  10.0)</li>\n<li><strong>consensus_masking [bool] - (default:</strong>  False)</li>\n<li><strong>rl_swap [bool] - (default:</strong>  False)</li>\n<li><strong>device [torch.device] - (default:</strong>  None)</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.__init__", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">192</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">latent_dim</span><span class=\"o\">=</span><span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">no_convs_fcomb</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">10.0</span>,</span><span class=\"param\">\t<span class=\"n\">consensus_masking</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rl_swap</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.input_channels", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.input_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.num_classes", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.num_classes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.num_filters", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.num_filters", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.latent_dim", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.latent_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.no_convs_per_block", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.no_convs_per_block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.no_convs_fcomb", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.no_convs_fcomb", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.initializers", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.initializers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.beta", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.beta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.z_prior_sample", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.z_prior_sample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.consensus_masking", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.consensus_masking", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.rl_swap", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.rl_swap", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.unet", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.unet", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.prior", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.prior", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.posterior", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.posterior", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.fcomb", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.fcomb", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.forward", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.forward", "kind": "function", "doc": "<p>Construct prior latent space for patch and run patch through UNet,\nin case training is True also construct posterior latent space</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">patch</span>, </span><span class=\"param\"><span class=\"n\">segm</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.sample", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.sample", "kind": "function", "doc": "<p>Sample a segmentation by reconstructing from a prior sample and combining this with UNet features</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">testing</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.reconstruct", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.reconstruct", "kind": "function", "doc": "<p>Reconstruct a segmentation from a posterior sample (decoding a posterior sample) and UNet feature map\nuse_posterior_mean: use posterior_mean instead of sampling z_q\ncalculate_posterior: use a provided sample or sample from posterior latent space</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">use_posterior_mean</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">calculate_posterior</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">z_posterior</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.kl_divergence", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.kl_divergence", "kind": "function", "doc": "<p>Calculate the KL divergence between the posterior and prior KL(Q||P)\nanalytic: calculate KL analytically or via sampling from the posterior\ncalculate_posterior: if we use samapling to approximate KL we can sample here or supply a sample</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">analytic</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">calculate_posterior</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">z_posterior</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.probabilistic_unet.ProbabilisticUNet.elbo", "modulename": "torch_em.model.probabilistic_unet", "qualname": "ProbabilisticUNet.elbo", "kind": "function", "doc": "<p>Calculate the evidence lower bound of the log-likelihood of P(Y|X)\nconsm: consensus response</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">segm</span>,</span><span class=\"param\">\t<span class=\"n\">consm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">analytic_kl</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">reconstruct_posterior_mean</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d", "modulename": "torch_em.model.resnet3d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.__init__", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">BasicBlock</span><span class=\"p\">,</span> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">Bottleneck</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">layers</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">zero_init_residual</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">groups</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">width_per_group</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">replace_stride_with_dilation</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">norm_layer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stride_conv1</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.in_channels", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.in_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.out_channels", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.out_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.inplanes", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.inplanes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.dilation", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.dilation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.groups", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.groups", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.base_width", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.base_width", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.conv1", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.conv1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.bn1", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.bn1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.relu", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.relu", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.maxpool", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.maxpool", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.layer1", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.layer1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.layer2", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.layer2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.layer3", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.layer3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.layer4", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.layer4", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.avgpool", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.avgpool", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.fc", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.fc", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.resnet3d.ResNet3d.forward", "modulename": "torch_em.model.resnet3d", "qualname": "ResNet3d.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnet3d_18", "modulename": "torch_em.model.resnet3d", "qualname": "resnet3d_18", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnet3d_34", "modulename": "torch_em.model.resnet3d", "qualname": "resnet3d_34", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnet3d_50", "modulename": "torch_em.model.resnet3d", "qualname": "resnet3d_50", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnet3d_101", "modulename": "torch_em.model.resnet3d", "qualname": "resnet3d_101", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnet3d_152", "modulename": "torch_em.model.resnet3d", "qualname": "resnet3d_152", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnext3d_50_32x4d", "modulename": "torch_em.model.resnet3d", "qualname": "resnext3d_50_32x4d", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnext3d_101_32x8d", "modulename": "torch_em.model.resnet3d", "qualname": "resnext3d_101_32x8d", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.resnext3d_101_64x4d", "modulename": "torch_em.model.resnet3d", "qualname": "resnext3d_101_64x4d", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.wide_resnet3d_50_2", "modulename": "torch_em.model.resnet3d", "qualname": "wide_resnet3d_50_2", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.resnet3d.wide_resnet3d_101_2", "modulename": "torch_em.model.resnet3d", "qualname": "wide_resnet3d_101_2", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resnet3d</span><span class=\"o\">.</span><span class=\"n\">ResNet3d</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet", "modulename": "torch_em.model.unet", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.AccumulateChannels", "modulename": "torch_em.model.unet", "qualname": "AccumulateChannels", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unet.AccumulateChannels.__init__", "modulename": "torch_em.model.unet", "qualname": "AccumulateChannels.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">invariant_channels</span>, </span><span class=\"param\"><span class=\"n\">accumulate_channels</span>, </span><span class=\"param\"><span class=\"n\">accumulator</span></span>)</span>"}, {"fullname": "torch_em.model.unet.AccumulateChannels.invariant_channels", "modulename": "torch_em.model.unet", "qualname": "AccumulateChannels.invariant_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.AccumulateChannels.accumulate_channels", "modulename": "torch_em.model.unet", "qualname": "AccumulateChannels.accumulate_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.AccumulateChannels.accumulator", "modulename": "torch_em.model.unet", "qualname": "AccumulateChannels.accumulator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.AccumulateChannels.forward", "modulename": "torch_em.model.unet", "qualname": "AccumulateChannels.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.affinities_to_boundaries", "modulename": "torch_em.model.unet", "qualname": "affinities_to_boundaries", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">aff_channels</span>, </span><span class=\"param\"><span class=\"n\">accumulator</span><span class=\"o\">=</span><span class=\"s1\">&#39;max&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.affinities_with_foreground_to_boundaries", "modulename": "torch_em.model.unet", "qualname": "affinities_with_foreground_to_boundaries", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">aff_channels</span>, </span><span class=\"param\"><span class=\"n\">fg_channel</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">accumulator</span><span class=\"o\">=</span><span class=\"s1\">&#39;max&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.affinities_to_boundaries2d", "modulename": "torch_em.model.unet", "qualname": "affinities_to_boundaries2d", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.affinities_with_foreground_to_boundaries2d", "modulename": "torch_em.model.unet", "qualname": "affinities_with_foreground_to_boundaries2d", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.affinities_to_boundaries3d", "modulename": "torch_em.model.unet", "qualname": "affinities_to_boundaries3d", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.affinities_with_foreground_to_boundaries3d", "modulename": "torch_em.model.unet", "qualname": "affinities_with_foreground_to_boundaries3d", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.affinities_to_boundaries_anisotropic", "modulename": "torch_em.model.unet", "qualname": "affinities_to_boundaries_anisotropic", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.POSTPROCESSING", "modulename": "torch_em.model.unet", "qualname": "POSTPROCESSING", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;affinities_to_boundaries_anisotropic&#x27;: &lt;function affinities_to_boundaries_anisotropic&gt;, &#x27;affinities_to_boundaries2d&#x27;: &lt;function affinities_to_boundaries2d&gt;, &#x27;affinities_with_foreground_to_boundaries2d&#x27;: &lt;function affinities_with_foreground_to_boundaries2d&gt;, &#x27;affinities_to_boundaries3d&#x27;: &lt;function affinities_to_boundaries3d&gt;, &#x27;affinities_with_foreground_to_boundaries3d&#x27;: &lt;function affinities_with_foreground_to_boundaries3d&gt;}"}, {"fullname": "torch_em.model.unet.UNetBase", "modulename": "torch_em.model.unet", "qualname": "UNetBase", "kind": "class", "doc": "<p></p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unet.UNetBase.__init__", "modulename": "torch_em.model.unet", "qualname": "UNetBase.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">encoder</span>,</span><span class=\"param\">\t<span class=\"n\">base</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span>,</span><span class=\"param\">\t<span class=\"n\">out_conv</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">final_activation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">postprocessing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">check_shape</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.model.unet.UNetBase.encoder", "modulename": "torch_em.model.unet", "qualname": "UNetBase.encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.base", "modulename": "torch_em.model.unet", "qualname": "UNetBase.base", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.decoder", "modulename": "torch_em.model.unet", "qualname": "UNetBase.decoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.out_conv", "modulename": "torch_em.model.unet", "qualname": "UNetBase.out_conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.check_shape", "modulename": "torch_em.model.unet", "qualname": "UNetBase.check_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.final_activation", "modulename": "torch_em.model.unet", "qualname": "UNetBase.final_activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.postprocessing", "modulename": "torch_em.model.unet", "qualname": "UNetBase.postprocessing", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.in_channels", "modulename": "torch_em.model.unet", "qualname": "UNetBase.in_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.out_channels", "modulename": "torch_em.model.unet", "qualname": "UNetBase.out_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.depth", "modulename": "torch_em.model.unet", "qualname": "UNetBase.depth", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNetBase.load_encoder_state", "modulename": "torch_em.model.unet", "qualname": "UNetBase.load_encoder_state", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.UNetBase.load_decoder_state", "modulename": "torch_em.model.unet", "qualname": "UNetBase.load_decoder_state", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.UNetBase.load_base_state", "modulename": "torch_em.model.unet", "qualname": "UNetBase.load_base_state", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.UNetBase.forward", "modulename": "torch_em.model.unet", "qualname": "UNetBase.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.Encoder", "modulename": "torch_em.model.unet", "qualname": "Encoder", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unet.Encoder.__init__", "modulename": "torch_em.model.unet", "qualname": "Encoder.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">scale_factors</span>,</span><span class=\"param\">\t<span class=\"n\">conv_block_impl</span>,</span><span class=\"param\">\t<span class=\"n\">pooler_impl</span>,</span><span class=\"param\">\t<span class=\"n\">anisotropic_kernel</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">conv_block_kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.unet.Encoder.blocks", "modulename": "torch_em.model.unet", "qualname": "Encoder.blocks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Encoder.poolers", "modulename": "torch_em.model.unet", "qualname": "Encoder.poolers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Encoder.return_outputs", "modulename": "torch_em.model.unet", "qualname": "Encoder.return_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Encoder.in_channels", "modulename": "torch_em.model.unet", "qualname": "Encoder.in_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Encoder.out_channels", "modulename": "torch_em.model.unet", "qualname": "Encoder.out_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Encoder.forward", "modulename": "torch_em.model.unet", "qualname": "Encoder.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.Decoder", "modulename": "torch_em.model.unet", "qualname": "Decoder", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unet.Decoder.__init__", "modulename": "torch_em.model.unet", "qualname": "Decoder.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">scale_factors</span>,</span><span class=\"param\">\t<span class=\"n\">conv_block_impl</span>,</span><span class=\"param\">\t<span class=\"n\">sampler_impl</span>,</span><span class=\"param\">\t<span class=\"n\">anisotropic_kernel</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">conv_block_kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.unet.Decoder.blocks", "modulename": "torch_em.model.unet", "qualname": "Decoder.blocks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Decoder.samplers", "modulename": "torch_em.model.unet", "qualname": "Decoder.samplers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Decoder.return_outputs", "modulename": "torch_em.model.unet", "qualname": "Decoder.return_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Decoder.in_channels", "modulename": "torch_em.model.unet", "qualname": "Decoder.in_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Decoder.out_channels", "modulename": "torch_em.model.unet", "qualname": "Decoder.out_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Decoder.forward", "modulename": "torch_em.model.unet", "qualname": "Decoder.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">encoder_inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.get_norm_layer", "modulename": "torch_em.model.unet", "qualname": "get_norm_layer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">norm</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">channels</span>, </span><span class=\"param\"><span class=\"n\">n_groups</span><span class=\"o\">=</span><span class=\"mi\">32</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.ConvBlock", "modulename": "torch_em.model.unet", "qualname": "ConvBlock", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unet.ConvBlock.__init__", "modulename": "torch_em.model.unet", "qualname": "ConvBlock.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span>,</span><span class=\"param\">\t<span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"s1\">&#39;InstanceNorm&#39;</span></span>)</span>"}, {"fullname": "torch_em.model.unet.ConvBlock.in_channels", "modulename": "torch_em.model.unet", "qualname": "ConvBlock.in_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.ConvBlock.out_channels", "modulename": "torch_em.model.unet", "qualname": "ConvBlock.out_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.ConvBlock.forward", "modulename": "torch_em.model.unet", "qualname": "ConvBlock.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.Upsampler", "modulename": "torch_em.model.unet", "qualname": "Upsampler", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unet.Upsampler.__init__", "modulename": "torch_em.model.unet", "qualname": "Upsampler.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scale_factor</span>, </span><span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">mode</span></span>)</span>"}, {"fullname": "torch_em.model.unet.Upsampler.mode", "modulename": "torch_em.model.unet", "qualname": "Upsampler.mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Upsampler.scale_factor", "modulename": "torch_em.model.unet", "qualname": "Upsampler.scale_factor", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Upsampler.conv", "modulename": "torch_em.model.unet", "qualname": "Upsampler.conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.Upsampler.forward", "modulename": "torch_em.model.unet", "qualname": "Upsampler.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unet.ConvBlock2d", "modulename": "torch_em.model.unet", "qualname": "ConvBlock2d", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "ConvBlock"}, {"fullname": "torch_em.model.unet.ConvBlock2d.__init__", "modulename": "torch_em.model.unet", "qualname": "ConvBlock2d.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.unet.Upsampler2d", "modulename": "torch_em.model.unet", "qualname": "Upsampler2d", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "Upsampler"}, {"fullname": "torch_em.model.unet.Upsampler2d.__init__", "modulename": "torch_em.model.unet", "qualname": "Upsampler2d.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scale_factor</span>, </span><span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;bilinear&#39;</span></span>)</span>"}, {"fullname": "torch_em.model.unet.UNet2d", "modulename": "torch_em.model.unet", "qualname": "UNet2d", "kind": "class", "doc": "<p></p>\n", "bases": "UNetBase"}, {"fullname": "torch_em.model.unet.UNet2d.__init__", "modulename": "torch_em.model.unet", "qualname": "UNet2d.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span>,</span><span class=\"param\">\t<span class=\"n\">depth</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">initial_features</span><span class=\"o\">=</span><span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">gain</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">final_activation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_side_outputs</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\tconv_block_impl=&lt;class &#x27;torch_em.model.unet.ConvBlock2d&#x27;&gt;,</span><span class=\"param\">\tpooler_impl=&lt;class &#x27;torch.nn.modules.pooling.MaxPool2d&#x27;&gt;,</span><span class=\"param\">\tsampler_impl=&lt;class &#x27;torch_em.model.unet.Upsampler2d&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">postprocessing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">check_shape</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">conv_block_kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.unet.UNet2d.init_kwargs", "modulename": "torch_em.model.unet", "qualname": "UNet2d.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.ConvBlock3d", "modulename": "torch_em.model.unet", "qualname": "ConvBlock3d", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "ConvBlock"}, {"fullname": "torch_em.model.unet.ConvBlock3d.__init__", "modulename": "torch_em.model.unet", "qualname": "ConvBlock3d.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.unet.Upsampler3d", "modulename": "torch_em.model.unet", "qualname": "Upsampler3d", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "Upsampler"}, {"fullname": "torch_em.model.unet.Upsampler3d.__init__", "modulename": "torch_em.model.unet", "qualname": "Upsampler3d.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scale_factor</span>, </span><span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;trilinear&#39;</span></span>)</span>"}, {"fullname": "torch_em.model.unet.AnisotropicUNet", "modulename": "torch_em.model.unet", "qualname": "AnisotropicUNet", "kind": "class", "doc": "<p></p>\n", "bases": "UNetBase"}, {"fullname": "torch_em.model.unet.AnisotropicUNet.__init__", "modulename": "torch_em.model.unet", "qualname": "AnisotropicUNet.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span>,</span><span class=\"param\">\t<span class=\"n\">scale_factors</span>,</span><span class=\"param\">\t<span class=\"n\">initial_features</span><span class=\"o\">=</span><span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">gain</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">final_activation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_side_outputs</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\tconv_block_impl=&lt;class &#x27;torch_em.model.unet.ConvBlock3d&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">anisotropic_kernel</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">postprocessing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">check_shape</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">conv_block_kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.unet.AnisotropicUNet.init_kwargs", "modulename": "torch_em.model.unet", "qualname": "AnisotropicUNet.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unet.UNet3d", "modulename": "torch_em.model.unet", "qualname": "UNet3d", "kind": "class", "doc": "<p></p>\n", "bases": "AnisotropicUNet"}, {"fullname": "torch_em.model.unet.UNet3d.__init__", "modulename": "torch_em.model.unet", "qualname": "UNet3d.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span>,</span><span class=\"param\">\t<span class=\"n\">depth</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">initial_features</span><span class=\"o\">=</span><span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">gain</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">final_activation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_side_outputs</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\tconv_block_impl=&lt;class &#x27;torch_em.model.unet.ConvBlock3d&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">postprocessing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">check_shape</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">conv_block_kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.unet.UNet3d.init_kwargs", "modulename": "torch_em.model.unet", "qualname": "UNet3d.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr", "modulename": "torch_em.model.unetr", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR", "modulename": "torch_em.model.unetr", "qualname": "UNETR", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unetr.UNETR.__init__", "modulename": "torch_em.model.unetr", "qualname": "UNETR.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">img_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>,</span><span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sam&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">encoder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_b&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">use_sam_stats</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_mae_stats</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">resize_input</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">encoder_checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">final_activation</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_skip_connection</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">embed_dim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_conv_transpose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.model.unetr.UNETR.use_sam_stats", "modulename": "torch_em.model.unetr", "qualname": "UNETR.use_sam_stats", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.use_mae_stats", "modulename": "torch_em.model.unetr", "qualname": "UNETR.use_mae_stats", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.use_skip_connection", "modulename": "torch_em.model.unetr", "qualname": "UNETR.use_skip_connection", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.resize_input", "modulename": "torch_em.model.unetr", "qualname": "UNETR.resize_input", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.out_channels", "modulename": "torch_em.model.unetr", "qualname": "UNETR.out_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.base", "modulename": "torch_em.model.unetr", "qualname": "UNETR.base", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.out_conv", "modulename": "torch_em.model.unetr", "qualname": "UNETR.out_conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.deconv_out", "modulename": "torch_em.model.unetr", "qualname": "UNETR.deconv_out", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.decoder_head", "modulename": "torch_em.model.unetr", "qualname": "UNETR.decoder_head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.final_activation", "modulename": "torch_em.model.unetr", "qualname": "UNETR.final_activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.UNETR.get_preprocess_shape", "modulename": "torch_em.model.unetr", "qualname": "UNETR.get_preprocess_shape", "kind": "function", "doc": "<p>Compute the output size given input size and target long side length.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">oldh</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">oldw</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">long_side_length</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.UNETR.resize_longest_side", "modulename": "torch_em.model.unetr", "qualname": "UNETR.resize_longest_side", "kind": "function", "doc": "<p>Resizes the image so that the longest side has the correct length.</p>\n\n<p>Expects batched images with shape BxCxHxW and float format.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.UNETR.preprocess", "modulename": "torch_em.model.unetr", "qualname": "UNETR.preprocess", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.UNETR.postprocess_masks", "modulename": "torch_em.model.unetr", "qualname": "UNETR.postprocess_masks", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">masks</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">input_size</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">original_size</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.UNETR.forward", "modulename": "torch_em.model.unetr", "qualname": "UNETR.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.SingleDeconv2DBlock", "modulename": "torch_em.model.unetr", "qualname": "SingleDeconv2DBlock", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unetr.SingleDeconv2DBlock.__init__", "modulename": "torch_em.model.unetr", "qualname": "SingleDeconv2DBlock.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scale_factor</span>, </span><span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span></span>)</span>"}, {"fullname": "torch_em.model.unetr.SingleDeconv2DBlock.block", "modulename": "torch_em.model.unetr", "qualname": "SingleDeconv2DBlock.block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.SingleDeconv2DBlock.forward", "modulename": "torch_em.model.unetr", "qualname": "SingleDeconv2DBlock.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.SingleConv2DBlock", "modulename": "torch_em.model.unetr", "qualname": "SingleConv2DBlock", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unetr.SingleConv2DBlock.__init__", "modulename": "torch_em.model.unetr", "qualname": "SingleConv2DBlock.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"n\">kernel_size</span></span>)</span>"}, {"fullname": "torch_em.model.unetr.SingleConv2DBlock.block", "modulename": "torch_em.model.unetr", "qualname": "SingleConv2DBlock.block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.SingleConv2DBlock.forward", "modulename": "torch_em.model.unetr", "qualname": "SingleConv2DBlock.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.Conv2DBlock", "modulename": "torch_em.model.unetr", "qualname": "Conv2DBlock", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unetr.Conv2DBlock.__init__", "modulename": "torch_em.model.unetr", "qualname": "Conv2DBlock.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span></span>)</span>"}, {"fullname": "torch_em.model.unetr.Conv2DBlock.block", "modulename": "torch_em.model.unetr", "qualname": "Conv2DBlock.block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.Conv2DBlock.forward", "modulename": "torch_em.model.unetr", "qualname": "Conv2DBlock.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.unetr.Deconv2DBlock", "modulename": "torch_em.model.unetr", "qualname": "Deconv2DBlock", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.model.unetr.Deconv2DBlock.__init__", "modulename": "torch_em.model.unetr", "qualname": "Deconv2DBlock.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">in_channels</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span>, </span><span class=\"param\"><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">use_conv_transpose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.model.unetr.Deconv2DBlock.block", "modulename": "torch_em.model.unetr", "qualname": "Deconv2DBlock.block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.unetr.Deconv2DBlock.forward", "modulename": "torch_em.model.unetr", "qualname": "Deconv2DBlock.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vim", "modulename": "torch_em.model.vim", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vim.ViM", "modulename": "torch_em.model.vim", "qualname": "ViM", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vim.ViM.__init__", "modulename": "torch_em.model.vim", "qualname": "ViM.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.vim.ViM.convert_to_expected_dim", "modulename": "torch_em.model.vim", "qualname": "ViM.convert_to_expected_dim", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs_</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vim.ViM.forward_features", "modulename": "torch_em.model.vim", "qualname": "ViM.forward_features", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">inference_params</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vim.ViM.forward", "modulename": "torch_em.model.vim", "qualname": "ViM.forward", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">inference_params</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vim.get_vim_encoder", "modulename": "torch_em.model.vim", "qualname": "get_vim_encoder", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;vim_t&#39;</span>, </span><span class=\"param\"><span class=\"n\">with_cls_token</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vim.get_vimunet_model", "modulename": "torch_em.model.vim", "qualname": "get_vimunet_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">out_channels</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;vim_t&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">with_cls_token</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vit", "modulename": "torch_em.model.vit", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vit.ViT_Sam", "modulename": "torch_em.model.vit", "qualname": "ViT_Sam", "kind": "class", "doc": "<p>Vision Transformer derived from the Segment Anything Codebase (https://arxiv.org/abs/2304.02643):\n<a href=\"https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py\">https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py</a></p>\n", "bases": "segment_anything.modeling.image_encoder.ImageEncoderViT"}, {"fullname": "torch_em.model.vit.ViT_Sam.__init__", "modulename": "torch_em.model.vit", "qualname": "ViT_Sam.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>img_size (int):</strong>  Input image size.</li>\n<li><strong>patch_size (int):</strong>  Patch size.</li>\n<li><strong>in_chans (int):</strong>  Number of input image channels.</li>\n<li><strong>embed_dim (int):</strong>  Patch embedding dimension.</li>\n<li><strong>depth (int):</strong>  Depth of ViT.</li>\n<li><strong>num_heads (int):</strong>  Number of attention heads in each ViT block.</li>\n<li><strong>mlp_ratio (float):</strong>  Ratio of mlp hidden dim to embedding dim.</li>\n<li><strong>qkv_bias (bool):</strong>  If True, add a learnable bias to query, key, value.</li>\n<li><strong>norm_layer (nn.Module):</strong>  Normalization layer.</li>\n<li><strong>act_layer (nn.Module):</strong>  Activation layer.</li>\n<li><strong>use_abs_pos (bool):</strong>  If True, use absolute positional embeddings.</li>\n<li><strong>use_rel_pos (bool):</strong>  If True, add relative positional embeddings to the attention map.</li>\n<li><strong>rel_pos_zero_init (bool):</strong>  If True, zero initialize relative positional parameters.</li>\n<li><strong>window_size (int):</strong>  Window size for window attention blocks.</li>\n<li><strong>global_attn_indexes (list):</strong>  Indexes for blocks using global attention.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_chans</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">embed_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">768</span>,</span><span class=\"param\">\t<span class=\"n\">global_attn_indexes</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">Ellipsis</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.vit.ViT_Sam.chunks_for_projection", "modulename": "torch_em.model.vit", "qualname": "ViT_Sam.chunks_for_projection", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vit.ViT_Sam.in_chans", "modulename": "torch_em.model.vit", "qualname": "ViT_Sam.in_chans", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vit.ViT_Sam.embed_dim", "modulename": "torch_em.model.vit", "qualname": "ViT_Sam.embed_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vit.ViT_Sam.forward", "modulename": "torch_em.model.vit", "qualname": "ViT_Sam.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vit.ViT_MAE", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE", "kind": "class", "doc": "<p>Vision Transformer derived from the Masked Auto Encoder Codebase (<a href=\"https://arxiv.org/abs/2111.06377\">https://arxiv.org/abs/2111.06377</a>)\n<a href=\"https://github.com/facebookresearch/mae/blob/main/models_vit.py#L20-L53\">https://github.com/facebookresearch/mae/blob/main/models_vit.py#L20-L53</a></p>\n", "bases": "timm.models.vision_transformer.VisionTransformer"}, {"fullname": "torch_em.model.vit.ViT_MAE.__init__", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>img_size:</strong>  Input image size.</li>\n<li><strong>patch_size:</strong>  Patch size.</li>\n<li><strong>in_chans:</strong>  Number of image input channels.</li>\n<li><strong>num_classes:</strong>  Mumber of classes for classification head.</li>\n<li><strong>global_pool:</strong>  Type of global pooling for final sequence (default: 'token').</li>\n<li><strong>embed_dim:</strong>  Transformer embedding dimension.</li>\n<li><strong>depth:</strong>  Depth of transformer.</li>\n<li><strong>num_heads:</strong>  Number of attention heads.</li>\n<li><strong>mlp_ratio:</strong>  Ratio of mlp hidden dim to embedding dim.</li>\n<li><strong>qkv_bias:</strong>  Enable bias for qkv projections if True.</li>\n<li><strong>init_values:</strong>  Layer-scale init values (layer-scale enabled if not None).</li>\n<li><strong>class_token:</strong>  Use class token.</li>\n<li><strong>no_embed_class:</strong>  Don't include position embeddings for class (or reg) tokens.</li>\n<li><strong>reg_tokens:</strong>  Number of register tokens.</li>\n<li><strong>fc_norm:</strong>  Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.</li>\n<li><strong>drop_rate:</strong>  Head dropout rate.</li>\n<li><strong>pos_drop_rate:</strong>  Position embedding dropout rate.</li>\n<li><strong>attn_drop_rate:</strong>  Attention dropout rate.</li>\n<li><strong>drop_path_rate:</strong>  Stochastic depth rate.</li>\n<li><strong>weight_init:</strong>  Weight initialization scheme.</li>\n<li><strong>embed_layer:</strong>  Patch embedding layer.</li>\n<li><strong>norm_layer:</strong>  Normalization layer.</li>\n<li><strong>act_layer:</strong>  MLP activation layer.</li>\n<li><strong>block_fn:</strong>  Transformer block layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">img_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span>, </span><span class=\"param\"><span class=\"n\">in_chans</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">depth</span><span class=\"o\">=</span><span class=\"mi\">12</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.model.vit.ViT_MAE.img_size", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE.img_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vit.ViT_MAE.in_chans", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE.in_chans", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vit.ViT_MAE.depth", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE.depth", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.model.vit.ViT_MAE.convert_to_expected_dim", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE.convert_to_expected_dim", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs_</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vit.ViT_MAE.forward_features", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE.forward_features", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vit.ViT_MAE.forward", "modulename": "torch_em.model.vit", "qualname": "ViT_MAE.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.model.vit.get_vision_transformer", "modulename": "torch_em.model.vit", "qualname": "get_vision_transformer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">img_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.segmentation", "modulename": "torch_em.segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.segmentation.DEFAULT_SCHEDULER_KWARGS", "modulename": "torch_em.segmentation", "qualname": "DEFAULT_SCHEDULER_KWARGS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;mode&#x27;: &#x27;min&#x27;, &#x27;factor&#x27;: 0.5, &#x27;patience&#x27;: 5}"}, {"fullname": "torch_em.segmentation.samples_to_datasets", "modulename": "torch_em.segmentation", "qualname": "samples_to_datasets", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">n_samples</span>, </span><span class=\"param\"><span class=\"n\">raw_paths</span>, </span><span class=\"param\"><span class=\"n\">raw_key</span>, </span><span class=\"param\"><span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s1\">&#39;uniform&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.segmentation.check_paths", "modulename": "torch_em.segmentation", "qualname": "check_paths", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw_paths</span>, </span><span class=\"param\"><span class=\"n\">label_paths</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.segmentation.is_segmentation_dataset", "modulename": "torch_em.segmentation", "qualname": "is_segmentation_dataset", "kind": "function", "doc": "<p>Check if we can load the data as SegmentationDataset</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw_paths</span>, </span><span class=\"param\"><span class=\"n\">raw_key</span>, </span><span class=\"param\"><span class=\"n\">label_paths</span>, </span><span class=\"param\"><span class=\"n\">label_key</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.segmentation.default_segmentation_loader", "modulename": "torch_em.segmentation", "qualname": "default_segmentation_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform2</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_seg_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">with_label_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">loader_kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.segmentation.default_segmentation_dataset", "modulename": "torch_em.segmentation", "qualname": "default_segmentation_dataset", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform2</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">label_dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_seg_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">with_label_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.segmentation.get_data_loader", "modulename": "torch_em.segmentation", "qualname": "get_data_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">loader_kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.segmentation.default_segmentation_trainer", "modulename": "torch_em.segmentation", "qualname": "default_segmentation_trainer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span>,</span><span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_image_interval</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">mixed_precision</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\tlogger=&lt;class &#x27;torch_em.trainer.tensorboard_logger.TensorboardLogger&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">logger_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scheduler_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;mode&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;min&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;factor&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"s1\">&#39;patience&#39;</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">}</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer_kwargs</span><span class=\"o\">=</span><span class=\"p\">{}</span>,</span><span class=\"param\">\ttrainer_class=&lt;class &#x27;torch_em.trainer.default_trainer.DefaultTrainer&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">id_</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_root</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">compile_model</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training", "modulename": "torch_em.self_training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.augmentations", "modulename": "torch_em.self_training.augmentations", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match", "modulename": "torch_em.self_training.fix_match", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer", "kind": "class", "doc": "<p>This trainer implements self-traning for semi-supervised learning and domain following the 'FixMatch' approach\nof Sohn et al. (https://arxiv.org/abs/2001.07685). This approach uses a (teacher) model derived from the\nstudent model via sharing the weights to predict pseudo-labels on unlabeled data.\nWe support two training strategies: joint training on labeled and unlabeled data\n(with a supervised and unsupervised loss function). And training only on the unsupervised data.</p>\n\n<p>This class expects the following data loaders:</p>\n\n<ul>\n<li>unsupervised_train_loader: Returns two augmentations (weak and strong) of the same input.</li>\n<li>supervised_train_loader (optional): Returns input and labels.</li>\n<li>unsupervised_val_loader (optional): Same as unsupervised_train_loader</li>\n<li>supervised_val_loader (optional): Same as supervised_train_loader\nAt least one of unsupervised_val_loader and supervised_val_loader must be given.</li>\n</ul>\n\n<p>And the following elements to customize the pseudo labeling:</p>\n\n<ul>\n<li>pseudo_labeler: to compute the psuedo-labels\n<ul>\n<li>Parameters: model, teacher_input</li>\n<li>Returns: pseudo_labels, label_filter (&lt;- label filter can for example be mask, weight or None)</li>\n</ul></li>\n<li>unsupervised_loss: the loss between model predictions and pseudo labels\n<ul>\n<li>Parameters: model, model_input, pseudo_labels, label_filter</li>\n<li>Returns: loss</li>\n</ul></li>\n<li>supervised_loss (optional): the supervised loss function\n<ul>\n<li>Parameters: model, input, labels</li>\n<li>Returns: loss</li>\n</ul></li>\n<li>unsupervised_loss_and_metric (optional): the unsupervised loss function and metric\n<ul>\n<li>Parameters: model, model_input, pseudo_labels, label_filter</li>\n<li>Returns: loss, metric</li>\n</ul></li>\n<li>supervised_loss_and_metric (optional): the supervised loss function and metric\n<ul>\n<li>Parameters: model, input, labels</li>\n<li>Returns: loss, metric\nAt least one of unsupervised_loss_and_metric and supervised_loss_and_metric must be given.</li>\n</ul></li>\n</ul>\n\n<p>Note: adjust the batch size ratio between the 'unsupervised_train_loader' and 'supervised_train_loader'\nfor setting the ratio between supervised and unsupervised training samples</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>model [nn.Module] -</li>\n<li>unsupervised_train_loader [torch.DataLoader] -</li>\n<li>unsupervised_loss [callable] -</li>\n<li>pseudo_labeler [callable] -</li>\n<li><strong>supervised_train_loader [torch.DataLoader] - (default:</strong>  None)</li>\n<li><strong>supervised_loss [callable] - (default:</strong>  None)</li>\n<li><strong>unsupervised_loss_and_metric [callable] - (default:</strong>  None)</li>\n<li><strong>supervised_loss_and_metric [callable] - (default:</strong>  None)</li>\n<li><strong>logger [TorchEmLogger] - (default:</strong>  SelfTrainingTensorboardLogger)</li>\n<li><strong>momentum [float] - (default:</strong>  0.999)</li>\n<li><strong>source_distribution [list] - (default:</strong>  None)</li>\n<li>**kwargs - keyword arguments for torch_em.DataLoader</li>\n</ul>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.__init__", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_train_loader</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_loss</span>,</span><span class=\"param\">\t<span class=\"n\">pseudo_labeler</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_train_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_val_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_val_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_loss</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_loss_and_metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_loss_and_metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\tlogger=&lt;class &#x27;torch_em.self_training.logger.SelfTrainingTensorboardLogger&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">source_distribution</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.unsupervised_train_loader", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.unsupervised_train_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.supervised_train_loader", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.supervised_train_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.supervised_val_loader", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.supervised_val_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.unsupervised_val_loader", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.unsupervised_val_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.supervised_loss_and_metric", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.supervised_loss_and_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.unsupervised_loss_and_metric", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.unsupervised_loss_and_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.unsupervised_loss", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.unsupervised_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.supervised_loss", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.supervised_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.pseudo_labeler", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.pseudo_labeler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.save_checkpoint", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.save_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">current_metric</span>, </span><span class=\"param\"><span class=\"n\">best_metric</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">extra_save_dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.fix_match.FixMatchTrainer.get_distribution_alignment", "modulename": "torch_em.self_training.fix_match", "qualname": "FixMatchTrainer.get_distribution_alignment", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pseudo_labels</span>, </span><span class=\"param\"><span class=\"n\">label_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.logger", "modulename": "torch_em.self_training.logger", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger", "kind": "class", "doc": "<p></p>\n", "bases": "torch_em.trainer.logger_base.TorchEmLogger"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.__init__", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">save_root</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">unused_kwargs</span></span>)</span>"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.my_root", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.my_root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_dir", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.tb", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.tb", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_image_interval", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_image_interval", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_combined_loss", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_combined_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">loss</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_lr", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_lr", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">lr</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_train_supervised", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_train_supervised", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">pred</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_validation_supervised", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_validation_supervised", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">metric</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">pred</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_train_unsupervised", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_train_unsupervised", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">x1</span>, </span><span class=\"param\"><span class=\"n\">x2</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">pseudo_labels</span>, </span><span class=\"param\"><span class=\"n\">label_filter</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_validation_unsupervised", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_validation_unsupervised", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">step</span>,</span><span class=\"param\">\t<span class=\"n\">metric</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span>,</span><span class=\"param\">\t<span class=\"n\">x1</span>,</span><span class=\"param\">\t<span class=\"n\">x2</span>,</span><span class=\"param\">\t<span class=\"n\">pred</span>,</span><span class=\"param\">\t<span class=\"n\">pseudo_labels</span>,</span><span class=\"param\">\t<span class=\"n\">label_filter</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.logger.SelfTrainingTensorboardLogger.log_validation", "modulename": "torch_em.self_training.logger", "qualname": "SelfTrainingTensorboardLogger.log_validation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">step</span>,</span><span class=\"param\">\t<span class=\"n\">metric</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span>,</span><span class=\"param\">\t<span class=\"n\">xt</span>,</span><span class=\"param\">\t<span class=\"n\">xt1</span>,</span><span class=\"param\">\t<span class=\"n\">xt2</span>,</span><span class=\"param\">\t<span class=\"n\">y</span>,</span><span class=\"param\">\t<span class=\"n\">z</span>,</span><span class=\"param\">\t<span class=\"n\">gt</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span>,</span><span class=\"param\">\t<span class=\"n\">gt_metric</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.loss", "modulename": "torch_em.self_training.loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLoss", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLoss", "kind": "class", "doc": "<p>Loss function for self training.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss [nn.Module] - the loss function to be used. (default:</strong>  torch_em.loss.DiceLoss)</li>\n<li>activation [nn.Module, callable] - the activation function to be applied to the prediction\nbefore passing it to the loss. (default: None)</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLoss.__init__", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">DiceLoss</span><span class=\"p\">()</span>, </span><span class=\"param\"><span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLoss.activation", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLoss.activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLoss.loss", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLoss.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLoss.init_kwargs", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLoss.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLossAndMetric", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLossAndMetric", "kind": "class", "doc": "<p>Loss and metric function for self training.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss [nn.Module] - the loss function to be used. (default:</strong>  torch_em.loss.DiceLoss)</li>\n<li><strong>metric [nn.Module] - the metric function to be used. (default:</strong>  torch_em.loss.DiceLoss)</li>\n<li>activation [nn.Module, callable] - the activation function to be applied to the prediction\nbefore passing it to the loss. (default: None)</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLossAndMetric.__init__", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLossAndMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">DiceLoss</span><span class=\"p\">()</span>, </span><span class=\"param\"><span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"n\">DiceLoss</span><span class=\"p\">()</span>, </span><span class=\"param\"><span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLossAndMetric.activation", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLossAndMetric.activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLossAndMetric.loss", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLossAndMetric.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLossAndMetric.metric", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLossAndMetric.metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.DefaultSelfTrainingLossAndMetric.init_kwargs", "modulename": "torch_em.self_training.loss", "qualname": "DefaultSelfTrainingLossAndMetric.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.l2_regularisation", "modulename": "torch_em.self_training.loss", "qualname": "l2_regularisation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">m</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLoss", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLoss", "kind": "class", "doc": "<p>Loss function for Probabilistic UNet</p>\n\n<h6 id=\"parameters\">Parameters :</h6>\n\n<blockquote>\n  <h1 id=\"todo-implement-a-generic-utility-function-for-all-probabilistic-unet-schemes-elbo-geco-etc\">TODO : Implement a generic utility function for all Probabilistic UNet schemes (ELBO, GECO, etc.)</h1>\n  \n  <p>loss [nn.Module] - the loss function to be used. (default: None)</p>\n</blockquote>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLoss.__init__", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLoss.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLoss.loss", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLoss.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLossAndMetric", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLossAndMetric", "kind": "class", "doc": "<p>Loss and metric function for Probabilistic UNet.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong># TODO :</strong>  Implement a generic utility function for all Probabilistic UNet schemes (ELBO, GECO, etc.)</li>\n<li><strong>loss [nn.Module] - the loss function to be used. (default:</strong>  None)</li>\n<li><strong>metric [nn.Module] - the metric function to be used. (default:</strong>  torch_em.loss.DiceLoss)</li>\n<li>activation [nn.Module, callable] - the activation function to be applied to the prediction\nbefore evaluating the average predictions. (default: None)</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLossAndMetric.__init__", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLossAndMetric.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"n\">DiceLoss</span><span class=\"p\">()</span>, </span><span class=\"param\"><span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"n\">Sigmoid</span><span class=\"p\">()</span>, </span><span class=\"param\"><span class=\"n\">prior_samples</span><span class=\"o\">=</span><span class=\"mi\">16</span></span>)</span>"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLossAndMetric.activation", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLossAndMetric.activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLossAndMetric.metric", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLossAndMetric.metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLossAndMetric.loss", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLossAndMetric.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.loss.ProbabilisticUNetLossAndMetric.prior_samples", "modulename": "torch_em.self_training.loss", "qualname": "ProbabilisticUNetLossAndMetric.prior_samples", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher", "modulename": "torch_em.self_training.mean_teacher", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.Dummy", "modulename": "torch_em.self_training.mean_teacher", "qualname": "Dummy", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.self_training.mean_teacher.Dummy.init_kwargs", "modulename": "torch_em.self_training.mean_teacher", "qualname": "Dummy.init_kwargs", "kind": "variable", "doc": "<p></p>\n", "default_value": "{}"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer", "kind": "class", "doc": "<p>This trainer implements self-training for semi-supervised learning and domain following the 'MeanTeacher'\napproach of Tarvainen &amp; Vapola (https://arxiv.org/abs/1703.01780). This approach uses a teacher model derived from\nthe student model via EMA of weights to predict pseudo-labels on unlabeled data.\nWe support two training strategies: joint training on labeled and unlabeled data\n(with a supervised and unsupervised loss function). And training only on the unsupervised data.</p>\n\n<p>This class expects the following data loaders:</p>\n\n<ul>\n<li>unsupervised_train_loader: Returns two augmentations of the same input.</li>\n<li>supervised_train_loader (optional): Returns input and labels.</li>\n<li>unsupervised_val_loader (optional): Same as unsupervised_train_loader</li>\n<li>supervised_val_loader (optional): Same as supervised_train_loader\nAt least one of unsupervised_val_loader and supervised_val_loader must be given.</li>\n</ul>\n\n<p>And the following elements to customize the pseudo labeling:</p>\n\n<ul>\n<li>pseudo_labeler: to compute the psuedo-labels\n<ul>\n<li>Parameters: teacher, teacher_input</li>\n<li>Returns: pseudo_labels, label_filter (&lt;- label filter can for example be mask, weight or None)</li>\n</ul></li>\n<li>unsupervised_loss: the loss between model predictions and pseudo labels\n<ul>\n<li>Parameters: model, model_input, pseudo_labels, label_filter</li>\n<li>Returns: loss</li>\n</ul></li>\n<li>supervised_loss (optional): the supervised loss function\n<ul>\n<li>Parameters: model, input, labels</li>\n<li>Returns: loss</li>\n</ul></li>\n<li>unsupervised_loss_and_metric (optional): the unsupervised loss function and metric\n<ul>\n<li>Parameters: model, model_input, pseudo_labels, label_filter</li>\n<li>Returns: loss, metric</li>\n</ul></li>\n<li>supervised_loss_and_metric (optional): the supervised loss function and metric\n<ul>\n<li>Parameters: model, input, labels</li>\n<li>Returns: loss, metric\nAt least one of unsupervised_loss_and_metric and supervised_loss_and_metric must be given.</li>\n</ul></li>\n</ul>\n\n<p>If the parameter reinit_teacher is set to true, the teacher weights are re-initialized.\nIf it is None, the most appropriate initialization scheme for the training approach is chosen:</p>\n\n<ul>\n<li>semi-supervised training -> reinit, because we usually train a model from scratch</li>\n<li>unsupervised training -> do not reinit, because we usually fine-tune a model</li>\n</ul>\n\n<p>Note: adjust the batch size ratio between the 'unsupervised_train_loader' and 'supervised_train_loader'\nfor setting the ratio between supervised and unsupervised training samples</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>model [nn.Module] -</li>\n<li>unsupervised_train_loader [torch.DataLoader] -</li>\n<li>unsupervised_loss [callable] -</li>\n<li>pseudo_labeler [callable] -</li>\n<li><strong>supervised_train_loader [torch.DataLoader] - (default:</strong>  None)</li>\n<li><strong>supervised_loss [callable] - (default:</strong>  None)</li>\n<li><strong>unsupervised_loss_and_metric [callable] - (default:</strong>  None)</li>\n<li><strong>supervised_loss_and_metric [callable] - (default:</strong>  None)</li>\n<li><strong>logger [TorchEmLogger] - (default:</strong>  SelfTrainingTensorboardLogger)</li>\n<li><strong>momentum [float] - (default:</strong>  0.999)</li>\n<li><strong>reinit_teacher [bool] - (default:</strong>  None)</li>\n<li>**kwargs - keyword arguments for torch_em.DataLoader</li>\n</ul>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.__init__", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_train_loader</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_loss</span>,</span><span class=\"param\">\t<span class=\"n\">pseudo_labeler</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_train_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_val_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_val_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_loss</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unsupervised_loss_and_metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">supervised_loss_and_metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\tlogger=&lt;class &#x27;torch_em.self_training.logger.SelfTrainingTensorboardLogger&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.999</span>,</span><span class=\"param\">\t<span class=\"n\">reinit_teacher</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.unsupervised_train_loader", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.unsupervised_train_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.supervised_train_loader", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.supervised_train_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.supervised_val_loader", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.supervised_val_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.unsupervised_val_loader", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.unsupervised_val_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.supervised_loss_and_metric", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.supervised_loss_and_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.unsupervised_loss_and_metric", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.unsupervised_loss_and_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.unsupervised_loss", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.unsupervised_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.supervised_loss", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.supervised_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.pseudo_labeler", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.pseudo_labeler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.momentum", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.momentum", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.save_checkpoint", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.save_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">current_metric</span>, </span><span class=\"param\"><span class=\"n\">best_metric</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">extra_save_dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.mean_teacher.MeanTeacherTrainer.load_checkpoint", "modulename": "torch_em.self_training.mean_teacher", "qualname": "MeanTeacherTrainer.load_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer.DummyLoss", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "qualname": "DummyLoss", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer.ProbabilisticUNetTrainer", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "qualname": "ProbabilisticUNetTrainer", "kind": "class", "doc": "<p>This trainer implements training for the 'Probabilistic UNet' of Kohl et al.: (https://arxiv.org/abs/1806.05034).\nThis approach combines the learnings from UNet and VAEs (Prior and Posterior networks) to obtain generative\nsegmentations. The heuristic trains by taking into account the feature maps from UNet and the samples from\nthe posterior distribution, estimating the loss and further sampling from the prior for validation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>clipping_value [float] - (default:</strong>  None)</li>\n<li><strong>prior_samples [int] - (default:</strong>  16)</li>\n<li><strong>loss [callable] - (default:</strong>  None)</li>\n<li><strong>loss_and_metric [callable] - (default:</strong>  None)</li>\n</ul>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer.ProbabilisticUNetTrainer.__init__", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "qualname": "ProbabilisticUNetTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">clipping_value</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prior_samples</span><span class=\"o\">=</span><span class=\"mi\">16</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">loss_and_metric</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer.ProbabilisticUNetTrainer.loss_and_metric", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "qualname": "ProbabilisticUNetTrainer.loss_and_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer.ProbabilisticUNetTrainer.clipping_value", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "qualname": "ProbabilisticUNetTrainer.clipping_value", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer.ProbabilisticUNetTrainer.prior_samples", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "qualname": "ProbabilisticUNetTrainer.prior_samples", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.probabilistic_unet_trainer.ProbabilisticUNetTrainer.sigmoid", "modulename": "torch_em.self_training.probabilistic_unet_trainer", "qualname": "ProbabilisticUNetTrainer.sigmoid", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling", "modulename": "torch_em.self_training.pseudo_labeling", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.DefaultPseudoLabeler", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "DefaultPseudoLabeler", "kind": "class", "doc": "<p>Compute pseudo labels.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>activation [nn.Module, callable] - activation function applied to the teacher prediction.</li>\n<li>confidence_threshold [float] - threshold for computing a mask for filterign the pseudo labels.\nIf none is given no mask will be computed (default: None)</li>\n<li>threshold_from_both_sides [bool] - whether to include both values bigger than the threshold\nand smaller than 1 - it, or only values bigger than it in the mask.\nThe former should be used for binary labels, the latter for for multiclass labels (default: False)</li>\n</ul>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.DefaultPseudoLabeler.__init__", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "DefaultPseudoLabeler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">confidence_threshold</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">threshold_from_both_sides</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.self_training.pseudo_labeling.DefaultPseudoLabeler.activation", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "DefaultPseudoLabeler.activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.DefaultPseudoLabeler.confidence_threshold", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "DefaultPseudoLabeler.confidence_threshold", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.DefaultPseudoLabeler.threshold_from_both_sides", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "DefaultPseudoLabeler.threshold_from_both_sides", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.DefaultPseudoLabeler.init_kwargs", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "DefaultPseudoLabeler.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler", "kind": "class", "doc": "<p>Compute pseudo labels from the Probabilistic UNet.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>activation [nn.Module, callable] - activation function applied to the teacher prediction.</li>\n<li>confidence_threshold [float] - threshold for computing a mask for filterign the pseudo labels.\nIf none is given no mask will be computed (default: None)</li>\n<li>threshold_from_both_sides [bool] - whether to include both values bigger than the threshold\nand smaller than 1 - it, or only values bigger than it in the mask.\nThe former should be used for binary labels, the latter for for multiclass labels (default: False)</li>\n<li>prior_samples [int] - the number of times we want to sample from the\nprior distribution per inputs (default: 16)</li>\n<li><strong>consensus_masking [bool] - whether to activate consensus masking in the label filter (default:</strong>  False)\nIf false, the weighted consensus response (weighted per-pixel response) is returned\nIf true, the masked consensus response (complete aggrement of pixels) is returned</li>\n</ul>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler.__init__", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">confidence_threshold</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">threshold_from_both_sides</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">prior_samples</span><span class=\"o\">=</span><span class=\"mi\">16</span>,</span><span class=\"param\">\t<span class=\"n\">consensus_masking</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler.activation", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler.activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler.confidence_threshold", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler.confidence_threshold", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler.threshold_from_both_sides", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler.threshold_from_both_sides", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler.prior_samples", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler.prior_samples", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler.consensus_masking", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler.consensus_masking", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.self_training.pseudo_labeling.ProbabilisticPseudoLabeler.init_kwargs", "modulename": "torch_em.self_training.pseudo_labeling", "qualname": "ProbabilisticPseudoLabeler.init_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep", "modulename": "torch_em.shallow2deep", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.RFSegmentationDataset", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "RFSegmentationDataset", "kind": "class", "doc": "<p></p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.RFSegmentationDataset.patch_shape_min", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "RFSegmentationDataset.patch_shape_min", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.RFSegmentationDataset.patch_shape_max", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "RFSegmentationDataset.patch_shape_max", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.RFImageCollectionDataset", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "RFImageCollectionDataset", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><code>~torch.utils.data.DataLoader</code> by default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.RFImageCollectionDataset.patch_shape_min", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "RFImageCollectionDataset.patch_shape_min", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.RFImageCollectionDataset.patch_shape_max", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "RFImageCollectionDataset.patch_shape_max", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.prepare_shallow2deep", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "prepare_shallow2deep", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape_min</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape_max</span>,</span><span class=\"param\">\t<span class=\"n\">n_forests</span>,</span><span class=\"param\">\t<span class=\"n\">n_threads</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_seg_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">balance_labels</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">filter_config</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">rf_kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.worst_points", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "worst_points", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">rf_id</span>,</span><span class=\"param\">\t<span class=\"n\">forests</span>,</span><span class=\"param\">\t<span class=\"n\">forests_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sample_fraction_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_samples</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.uncertain_points", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "uncertain_points", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">rf_id</span>,</span><span class=\"param\">\t<span class=\"n\">forests</span>,</span><span class=\"param\">\t<span class=\"n\">forests_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sample_fraction_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_samples</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.uncertain_worst_points", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "uncertain_worst_points", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">rf_id</span>,</span><span class=\"param\">\t<span class=\"n\">forests</span>,</span><span class=\"param\">\t<span class=\"n\">forests_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sample_fraction_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_samples</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.random_points", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "random_points", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">rf_id</span>,</span><span class=\"param\">\t<span class=\"n\">forests</span>,</span><span class=\"param\">\t<span class=\"n\">forests_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sample_fraction_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_samples</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.worst_tiles", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "worst_tiles", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">rf_id</span>,</span><span class=\"param\">\t<span class=\"n\">forests</span>,</span><span class=\"param\">\t<span class=\"n\">forests_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sample_fraction_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">img_shape</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">,</span> <span class=\"mi\">25</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">smoothing_sigma</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_samples</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.balanced_dense_accumulate", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "balanced_dense_accumulate", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">rf_id</span>,</span><span class=\"param\">\t<span class=\"n\">forests</span>,</span><span class=\"param\">\t<span class=\"n\">forests_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sample_fraction_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">accumulate_samples</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.SAMPLING_STRATEGIES", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "SAMPLING_STRATEGIES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;random_points&#x27;: &lt;function random_points&gt;, &#x27;uncertain_points&#x27;: &lt;function uncertain_points&gt;, &#x27;uncertain_worst_points&#x27;: &lt;function uncertain_worst_points&gt;, &#x27;worst_points&#x27;: &lt;function worst_points&gt;, &#x27;worst_tiles&#x27;: &lt;function worst_tiles&gt;, &#x27;balanced_dense_accumulate&#x27;: &lt;function balanced_dense_accumulate&gt;}"}, {"fullname": "torch_em.shallow2deep.prepare_shallow2deep.prepare_shallow2deep_advanced", "modulename": "torch_em.shallow2deep.prepare_shallow2deep", "qualname": "prepare_shallow2deep_advanced", "kind": "function", "doc": "<p>Advanced training of random forests for shallow2deep enhancer training.</p>\n\n<p>This function accepts the 'sampling_strategy' parameter, which allows to implement custom\nsampling strategies for the samples used for training the random forests.\nTraining operates in stages, the parameter 'forests_per_stage' determines how many forests\nare trained in each stage, and 'sample_fraction_per_stage' which fraction of the samples is\ntaken per stage. The random forests in stage 0 are trained from random balanced labels.\nFor the other stages 'sampling_strategy' enables specifying the strategy; it has to be a function\nwith signature '(features, labels, forests, rf_id, forests_per_stage, sample_fraction_per_stage)',\nand return the sampled features and labels. See for the 'worst_points' function\nin this file for an example implementation.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape_min</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape_max</span>,</span><span class=\"param\">\t<span class=\"n\">n_forests</span>,</span><span class=\"param\">\t<span class=\"n\">n_threads</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span>,</span><span class=\"param\">\t<span class=\"n\">forests_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sample_fraction_per_stage</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"o\">=</span><span class=\"s1\">&#39;worst_points&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_kwargs</span><span class=\"o\">=</span><span class=\"p\">{}</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_seg_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">filter_config</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">rf_kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.pseudolabel_training", "modulename": "torch_em.shallow2deep.pseudolabel_training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.pseudolabel_training.check_paths", "modulename": "torch_em.shallow2deep.pseudolabel_training", "qualname": "check_paths", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw_paths</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.pseudolabel_training.get_pseudolabel_dataset", "modulename": "torch_em.shallow2deep.pseudolabel_training", "qualname": "get_pseudolabel_dataset", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint</span>,</span><span class=\"param\">\t<span class=\"n\">rf_config</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_raw_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pseudo_labeler_device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cpu&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.pseudolabel_training.get_pseudolabel_loader", "modulename": "torch_em.shallow2deep.pseudolabel_training", "qualname": "get_pseudolabel_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint</span>,</span><span class=\"param\">\t<span class=\"n\">rf_config</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_raw_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pseudo_labeler_device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cpu&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">loader_kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_dataset", "modulename": "torch_em.shallow2deep.shallow2deep_dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_dataset.Shallow2DeepDataset", "modulename": "torch_em.shallow2deep.shallow2deep_dataset", "qualname": "Shallow2DeepDataset", "kind": "class", "doc": "<p></p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.shallow2deep.shallow2deep_dataset.Shallow2DeepImageCollectionDataset", "modulename": "torch_em.shallow2deep.shallow2deep_dataset", "qualname": "Shallow2DeepImageCollectionDataset", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><code>~torch.utils.data.DataLoader</code> by default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "torch_em.shallow2deep.shallow2deep_dataset.get_shallow2deep_dataset", "modulename": "torch_em.shallow2deep.shallow2deep_dataset", "qualname": "get_shallow2deep_dataset", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span>,</span><span class=\"param\">\t<span class=\"n\">rf_paths</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_seg_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">filter_config</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rf_channels</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,)</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_dataset.get_shallow2deep_loader", "modulename": "torch_em.shallow2deep.shallow2deep_dataset", "qualname": "get_shallow2deep_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span>,</span><span class=\"param\">\t<span class=\"n\">rf_paths</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">filter_config</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_seg_dataset</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rf_channels</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,)</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">loader_kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_eval", "modulename": "torch_em.shallow2deep.shallow2deep_eval", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_eval.visualize_pretrained_rfs", "modulename": "torch_em.shallow2deep.shallow2deep_eval", "qualname": "visualize_pretrained_rfs", "kind": "function", "doc": "<p>Visualize pretrained random forests from a shallow2depp checkpoint.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>checkpoint [str] - path to the checkpoint folder</li>\n<li>raw [np.ndarray] - the raw data for prediction</li>\n<li>n_forests [int] - the number of forests to use</li>\n<li><strong>sample_random [bool] - whether to subsample forests randomly or regularly (default:</strong>  False)</li>\n<li><strong>filter_config [list] - the filter configuration (default:</strong>  None)</li>\n<li><strong>n_threads [int] - number of threads for parallel prediction of forests (default:</strong>  None)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span>,</span><span class=\"param\">\t<span class=\"n\">raw</span>,</span><span class=\"param\">\t<span class=\"n\">n_forests</span>,</span><span class=\"param\">\t<span class=\"n\">sample_random</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">filter_config</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_threads</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_eval.evaluate_enhancers", "modulename": "torch_em.shallow2deep.shallow2deep_eval", "qualname": "evaluate_enhancers", "kind": "function", "doc": "<p>Evaluate enhancers on ilastik random forests from multiple projects.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>data [np.ndarray] - the data for evaluation</li>\n<li>labels [np.ndarray] - the labels for evaluation</li>\n<li>enhancers [dict[str, str] - map of enhancer names to filepath with enhancer\nmodels saved in the biomage.io model format</li>\n<li>ilastik_projects [dict[str, str]] - map of names to ilastik project paths</li>\n<li>metric [callable] - the metric used for evaluation</li>\n<li>prediction_function [callable] - function to run prediction with the enhancer.\nBy default the bioimageio.prediction pipeline is called directly.\nIf given, needs to take the prediction pipeline and data (as xarray)\nas input (default: None)</li>\n<li>rf_channel [int, list[int]] - the channel(s) of the random forest to be passed\nas input to the enhancer (default: 1)</li>\n<li>is2d [bool] - whether to process 3d data as individual slices and average the scores.\nIs ignored if the data is 2d (default: False)</li>\n<li>save_path [str] -</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>[pd.DataFrame] - a table with the scores of the enhancers for the different forests\n      and scores of the raw forest predictions</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">enhancers</span>,</span><span class=\"param\">\t<span class=\"n\">ilastik_projects</span>,</span><span class=\"param\">\t<span class=\"n\">metric</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_function</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rf_channel</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">is2d</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_eval.load_predictions", "modulename": "torch_em.shallow2deep.shallow2deep_eval", "qualname": "load_predictions", "kind": "function", "doc": "<p>Helper functions to load predictions from a save_path created by evaluate_enhancers</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_path</span>, </span><span class=\"param\"><span class=\"n\">n_threads</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model", "modulename": "torch_em.shallow2deep.shallow2deep_model", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.RFWithFilters", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "RFWithFilters", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.RFWithFilters.__init__", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "RFWithFilters.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">rf_path</span>, </span><span class=\"param\"><span class=\"n\">ndim</span>, </span><span class=\"param\"><span class=\"n\">filter_config</span>, </span><span class=\"param\"><span class=\"n\">output_channel</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.RFWithFilters.filters_and_sigmas", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "RFWithFilters.filters_and_sigmas", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.RFWithFilters.output_channel", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "RFWithFilters.output_channel", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.IlastikPredicter", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "IlastikPredicter", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.IlastikPredicter.__init__", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "IlastikPredicter.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ilp_path</span>, </span><span class=\"param\"><span class=\"n\">ndim</span>, </span><span class=\"param\"><span class=\"n\">ilastik_multi_thread</span>, </span><span class=\"param\"><span class=\"n\">output_channel</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.IlastikPredicter.ilp", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "IlastikPredicter.ilp", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.IlastikPredicter.dims", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "IlastikPredicter.dims", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.IlastikPredicter.output_channel", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "IlastikPredicter.output_channel", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.__init__", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span>,</span><span class=\"param\">\t<span class=\"n\">rf_config</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">rf_channel</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">ilastik_multi_thread</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.load_model", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.load_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">checkpoint</span>, </span><span class=\"param\"><span class=\"n\">device</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.load_rf", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.load_rf", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">rf_config</span>, </span><span class=\"param\"><span class=\"n\">rf_channel</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">ilastik_multi_thread</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.model", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.rf_predicter", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.rf_predicter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.device", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.checkpoint", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.checkpoint", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.shallow2deep_model.Shallow2DeepModel.rf_config", "modulename": "torch_em.shallow2deep.shallow2deep_model", "qualname": "Shallow2DeepModel.rf_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform", "modulename": "torch_em.shallow2deep.transform", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.ForegroundTransform", "modulename": "torch_em.shallow2deep.transform", "qualname": "ForegroundTransform", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.ForegroundTransform.__init__", "modulename": "torch_em.shallow2deep.transform", "qualname": "ForegroundTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">label_id</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">ignore_radius</span><span class=\"o\">=</span><span class=\"mi\">1</span></span>)</span>"}, {"fullname": "torch_em.shallow2deep.transform.ForegroundTransform.label_id", "modulename": "torch_em.shallow2deep.transform", "qualname": "ForegroundTransform.label_id", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.ForegroundTransform.ndim", "modulename": "torch_em.shallow2deep.transform", "qualname": "ForegroundTransform.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.ForegroundTransform.ignore_radius", "modulename": "torch_em.shallow2deep.transform", "qualname": "ForegroundTransform.ignore_radius", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.BoundaryTransform", "modulename": "torch_em.shallow2deep.transform", "qualname": "BoundaryTransform", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.BoundaryTransform.__init__", "modulename": "torch_em.shallow2deep.transform", "qualname": "BoundaryTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;thick&#39;</span>, </span><span class=\"param\"><span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">ignore_radius</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">add_binary_target</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.shallow2deep.transform.BoundaryTransform.mode", "modulename": "torch_em.shallow2deep.transform", "qualname": "BoundaryTransform.mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.BoundaryTransform.ndim", "modulename": "torch_em.shallow2deep.transform", "qualname": "BoundaryTransform.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.BoundaryTransform.ignore_radius", "modulename": "torch_em.shallow2deep.transform", "qualname": "BoundaryTransform.ignore_radius", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.shallow2deep.transform.BoundaryTransform.foreground_trafo", "modulename": "torch_em.shallow2deep.transform", "qualname": "BoundaryTransform.foreground_trafo", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer", "modulename": "torch_em.trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer", "modulename": "torch_em.trainer.default_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer", "kind": "class", "doc": "<p>Trainer class for 2d/3d training on a single GPU.</p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.__init__", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">metric</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_image_interval</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">mixed_precision</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\tlogger=&lt;class &#x27;torch_em.trainer.tensorboard_logger.TensorboardLogger&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">logger_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">id_</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_root</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">compile_model</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.name", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.id_", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.id_", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.train_loader", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.train_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.val_loader", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.val_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.model", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.loss", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.optimizer", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.optimizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.metric", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.device", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.lr_scheduler", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.lr_scheduler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.log_image_interval", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.log_image_interval", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.save_root", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.save_root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.compile_model", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.compile_model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.mixed_precision", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.mixed_precision", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.early_stopping", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.early_stopping", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.train_time", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.train_time", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.scaler", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.scaler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.logger_class", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.logger_class", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.logger_kwargs", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.logger_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.checkpoint_folder", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.checkpoint_folder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.iteration", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.iteration", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.epoch", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.epoch", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer", "kind": "class", "doc": "<p>Determines how to deserialize the trainer kwargs from serialized 'init_data'</p>\n\n<h6 id=\"examples\">Examples:</h6>\n\n<blockquote>\n  <p>To extend the initialization process you can inherite from this Deserializer in an inherited Trainer class.\n  Note that <code>DefaultTrainer.Deserializer.load_generic()</code> covers most cases already.</p>\n  \n  <p>This example adds <code>the_answer</code> kwarg, which requires 'calculations' upon initialization:</p>\n  \n  <div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">class</span> <span class=\"nc\">MyTrainer</span><span class=\"p\">(</span><span class=\"n\">DefaultTrainer</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">the_answer</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">the_answer</span> <span class=\"o\">=</span> <span class=\"n\">the_answer</span>  <span class=\"c1\"># this allows the default Serializer to save the new kwarg,</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>                                      <span class=\"c1\"># see DefaultTrainer.Serializer</span>\n<span class=\"gp\">&gt;&gt;&gt;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">class</span> <span class=\"nc\">Deserializer</span><span class=\"p\">(</span><span class=\"n\">DefaultTrainer</span><span class=\"o\">.</span><span class=\"n\">Deserializer</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"k\">def</span> <span class=\"nf\">load_the_answer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"n\">generic_answer</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">init_data</span><span class=\"p\">[</span><span class=\"s2\">&quot;the_answer&quot;</span><span class=\"p\">]</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"c1\"># (device dependent) special deserialization</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer_kwargs</span><span class=\"p\">[</span><span class=\"s2\">&quot;device&quot;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">type</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;cpu&quot;</span><span class=\"p\">:</span>  <span class=\"c1\"># accessing previously deserialized kwarg</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer_kwargs</span><span class=\"p\">[</span><span class=\"s2\">&quot;the_answer&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">generic_answer</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"k\">else</span><span class=\"p\">:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer_kwargs</span><span class=\"p\">[</span><span class=\"s2\">&quot;the_answer&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">generic_answer</span> <span class=\"o\">*</span> <span class=\"mi\">2</span>\n</code></pre>\n  </div>\n</blockquote>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.__init__", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">init_data</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.init_data", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.init_data", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.save_path", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.save_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.trainer_kwargs", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.trainer_kwargs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Any]"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.load", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.load", "kind": "function", "doc": "<p><code>optional</code> is True if self.trainer.__class__.__init__ specifies a default value for 'kwarg_name'</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">optional</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.load_data_loader", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.load_data_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">loader_name</span>, </span><span class=\"param\"><span class=\"n\">optional</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.load_generic", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.load_generic", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"o\">*</span><span class=\"n\">dynamic_args</span>,</span><span class=\"param\">\t<span class=\"n\">optional</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">only_class</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">dynamic_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.load_name", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.load_name", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">optional</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.load_optimizer", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.load_optimizer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">optional</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.load_lr_scheduler", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.load_lr_scheduler", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">optional</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Deserializer.load_logger", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Deserializer.load_logger", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">optional</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.from_checkpoint", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.from_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">checkpoint_folder</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span>, </span><span class=\"param\"><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer", "kind": "class", "doc": "<p>Implements how to serialize trainer kwargs from a trainer instance</p>\n\n<h6 id=\"examples\">Examples:</h6>\n\n<blockquote>\n  <p>To extend the serialization process you can inherite from this Serializer in a derived Trainer class.\n  Note that the methods <code>dump_generic_builtin()</code>, <code>dump_generic_class()</code> and <code>dump_generic_instance()</code>\n  called by the <code>dump()</code> method when appropriate cover most cases already.</p>\n  \n  <p>This example adds <code>the_answer</code> kwarg, which requires extra steps on dumping only because we don't keep a\n  'the_answer' attribute:</p>\n  \n  <div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">class</span> <span class=\"nc\">MyTrainer</span><span class=\"p\">(</span><span class=\"n\">DefaultTrainer</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">the_answer</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"c1\"># self.the_answer = the_answer  # this would allow the default Serializer to save the new kwarg,</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"c1\"># but let&#39;s make things more interesting...</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">the</span> <span class=\"o\">=</span> <span class=\"n\">the_answer</span> <span class=\"o\">//</span> <span class=\"mi\">10</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">answer</span> <span class=\"o\">=</span> <span class=\"n\">the_answer</span> <span class=\"o\">%</span> <span class=\"mi\">10</span>\n<span class=\"gp\">&gt;&gt;&gt;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">class</span> <span class=\"nc\">Serializer</span><span class=\"p\">(</span><span class=\"n\">DefaultTrainer</span><span class=\"o\">.</span><span class=\"n\">Serializer</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">MyTrainer</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"k\">def</span> <span class=\"nf\">dump_the_answer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>  <span class=\"c1\"># custom dump method for &#39;the_answer&#39; kwarg</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"k\">assert</span> <span class=\"n\">kwarg_name</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;the_answer&quot;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"c1\"># populate self.init_data with the serialized data required by Deserializer</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"c1\"># to restore the trainer kwargs</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">init_data</span><span class=\"p\">[</span><span class=\"s2\">&quot;the_answer&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">the</span> <span class=\"o\">*</span> <span class=\"mi\">10</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">answer</span>\n</code></pre>\n  </div>\n  \n  <p>This example with both Serializer and Deserializer adds <code>the_answer</code> kwarg,\n  while saving it in two separate entries 'the' and 'answer'</p>\n  \n  <div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">class</span> <span class=\"nc\">MyTrainer</span><span class=\"p\">(</span><span class=\"n\">DefaultTrainer</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">the_answer</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">the_answer</span> <span class=\"o\">=</span> <span class=\"n\">the_answer</span>\n<span class=\"gp\">&gt;&gt;&gt;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">class</span> <span class=\"nc\">Serializer</span><span class=\"p\">(</span><span class=\"n\">DefaultTrainer</span><span class=\"o\">.</span><span class=\"n\">Serializer</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">MyTrainer</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"k\">def</span> <span class=\"nf\">dump_the_answer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"k\">assert</span> <span class=\"n\">kwarg_name</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;the_answer&quot;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">init_data</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">({</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>                <span class=\"s2\">&quot;the&quot;</span><span class=\"p\">:</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">the_answer</span> <span class=\"o\">//</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>                <span class=\"s2\">&quot;answer&quot;</span><span class=\"p\">:</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">the_answer</span> <span class=\"o\">%</span> <span class=\"mi\">10</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">class</span> <span class=\"nc\">Deserializer</span><span class=\"p\">(</span><span class=\"n\">DefaultTrainer</span><span class=\"o\">.</span><span class=\"n\">Deserializer</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"k\">def</span> <span class=\"nf\">load_the_answer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">optional</span><span class=\"p\">:</span> <span class=\"nb\">bool</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"k\">assert</span> <span class=\"n\">kwarg_name</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;the_answer&quot;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"c1\"># &#39;optional&#39; is True if MyTrainer.__init__ specifies a default value for &#39;kwarg_name&#39;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">trainer_kwargs</span><span class=\"p\">[</span><span class=\"n\">kwarg_name</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">init_data</span><span class=\"p\">[</span><span class=\"s2\">&quot;the&quot;</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mi\">10</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">init_data</span><span class=\"p\">[</span><span class=\"s2\">&quot;answer&quot;</span><span class=\"p\">]</span>\n</code></pre>\n  </div>\n</blockquote>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.__init__", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">torch_em</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">default_trainer</span><span class=\"o\">.</span><span class=\"n\">DefaultTrainer</span></span>)</span>"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.trainer", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.trainer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.init_data", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.init_data", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump_generic_builtin", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump_generic_builtin", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump_generic_class", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump_generic_class", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump_generic_instance", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump_generic_instance", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump_device", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump_device", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump_data_loader", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump_data_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump_logger", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump_logger", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.Serializer.dump_model", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.Serializer.dump_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">kwarg_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.save_checkpoint", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.save_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">name</span>,</span><span class=\"param\">\t<span class=\"n\">current_metric</span>,</span><span class=\"param\">\t<span class=\"n\">best_metric</span>,</span><span class=\"param\">\t<span class=\"n\">train_time</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">extra_save_dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.load_checkpoint", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.load_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.default_trainer.DefaultTrainer.fit", "modulename": "torch_em.trainer.default_trainer", "qualname": "DefaultTrainer.fit", "kind": "function", "doc": "<p>Run neural network training.</p>\n\n<p>Exactly one of 'iterations' or 'epochs' has to be passed.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>iterations [int] - how long to train, specified in iterations (default:</strong>  None)</li>\n<li><strong>load_from_checkpoint [str] - path to a checkpoint from where training should be continued (default:</strong>  None)</li>\n<li><strong>epochs [int] - how long to train, specified in epochs (default:</strong>  None)</li>\n<li>save_every_kth_epoch [int] - save checkpoints after every kth epoch separately.\nThe corresponding checkpoints will be saved with the naming scheme 'epoch-{epoch}.pt'. (default: None)</li>\n<li>progress [progress_bar] - optional progress bar for integration with external tools.\nExpected to follow the tqdm interface.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">load_from_checkpoint</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_every_kth_epoch</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">progress</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.logger_base", "modulename": "torch_em.trainer.logger_base", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.logger_base.TorchEmLogger", "modulename": "torch_em.trainer.logger_base", "qualname": "TorchEmLogger", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.logger_base.TorchEmLogger.__init__", "modulename": "torch_em.trainer.logger_base", "qualname": "TorchEmLogger.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">save_root</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.trainer.logger_base.TorchEmLogger.trainer", "modulename": "torch_em.trainer.logger_base", "qualname": "TorchEmLogger.trainer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.logger_base.TorchEmLogger.save_root", "modulename": "torch_em.trainer.logger_base", "qualname": "TorchEmLogger.save_root", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.logger_base.TorchEmLogger.log_train", "modulename": "torch_em.trainer.logger_base", "qualname": "TorchEmLogger.log_train", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">lr</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">log_gradients</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.logger_base.TorchEmLogger.log_validation", "modulename": "torch_em.trainer.logger_base", "qualname": "TorchEmLogger.log_validation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">metric</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.spoco_trainer", "modulename": "torch_em.trainer.spoco_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer", "kind": "class", "doc": "<p>Trainer class for 2d/3d training on a single GPU.</p>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer.__init__", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.999</span>,</span><span class=\"param\">\t<span class=\"n\">semisupervised_loss</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">semisupervised_loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\tlogger=&lt;class &#x27;torch_em.trainer.tensorboard_logger.TensorboardLogger&#x27;&gt;,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer.momentum", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer.momentum", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer.model2", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer.model2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer.semisupervised_loader", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer.semisupervised_loader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer.semisupervised_loss", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer.semisupervised_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer.save_checkpoint", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer.save_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">current_metric</span>, </span><span class=\"param\"><span class=\"n\">best_metric</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">extra_save_dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.spoco_trainer.SPOCOTrainer.load_checkpoint", "modulename": "torch_em.trainer.spoco_trainer", "qualname": "SPOCOTrainer.load_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.tensorboard_logger", "modulename": "torch_em.trainer.tensorboard_logger", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.tensorboard_logger.normalize_im", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "normalize_im", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">im</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.tensorboard_logger.make_grid_image", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "make_grid_image", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">selection</span>, </span><span class=\"param\"><span class=\"n\">gradients</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.tensorboard_logger.make_embedding_image", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "make_embedding_image", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">selection</span>, </span><span class=\"param\"><span class=\"n\">gradients</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger", "kind": "class", "doc": "<p></p>\n", "bases": "torch_em.trainer.logger_base.TorchEmLogger"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger.__init__", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">save_root</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">unused_kwargs</span></span>)</span>"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger.log_dir", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger.log_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger.tb", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger.tb", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger.log_image_interval", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger.log_image_interval", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger.log_images", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger.log_images", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">gradients</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger.log_train", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger.log_train", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">lr</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">log_gradients</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.tensorboard_logger.TensorboardLogger.log_validation", "modulename": "torch_em.trainer.tensorboard_logger", "qualname": "TensorboardLogger.log_validation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">metric</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.wandb_logger", "modulename": "torch_em.trainer.wandb_logger", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger", "kind": "class", "doc": "<p></p>\n", "bases": "torch_em.trainer.logger_base.TorchEmLogger"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger.__init__", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">trainer</span>,</span><span class=\"param\">\t<span class=\"n\">save_root</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">project_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;gradients&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;parameters&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;all&#39;</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;all&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">log_model_freq</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">log_model_graph</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;online&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;offline&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;disabled&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;online&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">resume</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">unused_kwargs</span></span>)</span>"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger.log_dir", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger.log_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger.wand_run", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger.wand_run", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger.log_image_interval", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger.log_image_interval", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger.log_train", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger.log_train", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">lr</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span>, </span><span class=\"param\"><span class=\"n\">log_gradients</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger.log_validation", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger.log_validation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">step</span>, </span><span class=\"param\"><span class=\"n\">metric</span>, </span><span class=\"param\"><span class=\"n\">loss</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">prediction</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.trainer.wandb_logger.WandbLogger.get_wandb", "modulename": "torch_em.trainer.wandb_logger", "qualname": "WandbLogger.get_wandb", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform", "modulename": "torch_em.transform", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation", "modulename": "torch_em.transform.augmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformationStacked", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformationStacked", "kind": "class", "doc": "<p>AugmentationBase3D base class for customized augmentation implementations.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>p:</strong>  probability for applying an augmentation. This param controls the augmentation probabilities\nelement-wise for a batch.</li>\n<li><strong>p_batch:</strong>  probability for applying an augmentation to a batch. This param controls the augmentation\nprobabilities batch-wise.</li>\n<li><strong>same_on_batch:</strong>  apply the same transformation across the batch.</li>\n</ul>\n", "bases": "kornia.augmentation._3d.base.AugmentationBase3D"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformationStacked.__init__", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformationStacked.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">control_point_spacing</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">sigma</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">32.0</span><span class=\"p\">,</span> <span class=\"mf\">32.0</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">4.0</span><span class=\"p\">,</span> <span class=\"mf\">4.0</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">interpolation</span><span class=\"o\">=&lt;</span><span class=\"n\">Resample</span><span class=\"o\">.</span><span class=\"n\">BILINEAR</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">keepdim</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">same_on_batch</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformationStacked.interpolation", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformationStacked.interpolation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformationStacked.flags", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformationStacked.flags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformationStacked.generate_parameters", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformationStacked.generate_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformation", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformation", "kind": "class", "doc": "<p>AugmentationBase2D base class for customized augmentation implementations.</p>\n\n<p>AugmentationBase2D aims at offering a generic base class for a greater level of customization.\nIf the subclass contains routined matrix-based transformations, <code>RigidAffineAugmentationBase2D</code>\nmight be a better fit.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>p:</strong>  probability for applying an augmentation. This param controls the augmentation probabilities\nelement-wise for a batch.</li>\n<li><strong>p_batch:</strong>  probability for applying an augmentation to a batch. This param controls the augmentation\nprobabilities batch-wise.</li>\n<li><strong>same_on_batch:</strong>  apply the same transformation across the batch.</li>\n<li><strong>keepdim:</strong>  whether to keep the output shape the same as input <code>True</code> or broadcast it to the batch\nform <code>False</code>.</li>\n</ul>\n", "bases": "kornia.augmentation._2d.base.AugmentationBase2D"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformation.__init__", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformation.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">control_point_spacing</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">sigma</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">4.0</span><span class=\"p\">,</span> <span class=\"mf\">4.0</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">32.0</span><span class=\"p\">,</span> <span class=\"mf\">32.0</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">resample</span><span class=\"o\">=&lt;</span><span class=\"n\">Resample</span><span class=\"o\">.</span><span class=\"n\">BILINEAR</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">keepdim</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">same_on_batch</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformation.resample", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformation.resample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformation.flags", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformation.flags", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation.RandomElasticDeformation.generate_parameters", "modulename": "torch_em.transform.augmentation", "qualname": "RandomElasticDeformation.generate_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.__init__", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">kornia_augmentations</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span></span>)</span>"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.interpolatable_torch_types", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.interpolatable_torch_types", "kind": "variable", "doc": "<p></p>\n", "default_value": "[torch.float16, torch.float32, torch.float64]"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.interpolatable_numpy_types", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.interpolatable_numpy_types", "kind": "variable", "doc": "<p></p>\n", "default_value": "[dtype(&#x27;float32&#x27;), dtype(&#x27;float64&#x27;)]"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.augmentations", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.augmentations", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.dtype", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.halo", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.halo", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.compute_halo", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.compute_halo", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.is_interpolatable", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.is_interpolatable", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.transform_tensor", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.transform_tensor", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">augmentation</span>, </span><span class=\"param\"><span class=\"n\">tensor</span>, </span><span class=\"param\"><span class=\"n\">interpolatable</span>, </span><span class=\"param\"><span class=\"n\">params</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.KorniaAugmentationPipeline.forward", "modulename": "torch_em.transform.augmentation", "qualname": "KorniaAugmentationPipeline.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">tensors</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.AUGMENTATIONS", "modulename": "torch_em.transform.augmentation", "qualname": "AUGMENTATIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;RandomAffine&#x27;: {&#x27;degrees&#x27;: 90, &#x27;scale&#x27;: (0.9, 1.1)}, &#x27;RandomAffine3D&#x27;: {&#x27;degrees&#x27;: (90, 90, 90), &#x27;scale&#x27;: (0.0, 1.1)}, &#x27;RandomDepthicalFlip3D&#x27;: {}, &#x27;RandomHorizontalFlip&#x27;: {}, &#x27;RandomHorizontalFlip3D&#x27;: {}, &#x27;RandomRotation&#x27;: {&#x27;degrees&#x27;: 90}, &#x27;RandomRotation3D&#x27;: {&#x27;degrees&#x27;: (90, 90, 90)}, &#x27;RandomVerticalFlip&#x27;: {}, &#x27;RandomVerticalFlip3D&#x27;: {}, &#x27;RandomElasticDeformation3D&#x27;: {&#x27;alpha&#x27;: [5, 5], &#x27;sigma&#x27;: [30, 30]}}"}, {"fullname": "torch_em.transform.augmentation.DEFAULT_2D_AUGMENTATIONS", "modulename": "torch_em.transform.augmentation", "qualname": "DEFAULT_2D_AUGMENTATIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;RandomHorizontalFlip&#x27;, &#x27;RandomVerticalFlip&#x27;]"}, {"fullname": "torch_em.transform.augmentation.DEFAULT_3D_AUGMENTATIONS", "modulename": "torch_em.transform.augmentation", "qualname": "DEFAULT_3D_AUGMENTATIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;RandomHorizontalFlip3D&#x27;, &#x27;RandomVerticalFlip3D&#x27;, &#x27;RandomDepthicalFlip3D&#x27;]"}, {"fullname": "torch_em.transform.augmentation.DEFAULT_ANISOTROPIC_AUGMENTATIONS", "modulename": "torch_em.transform.augmentation", "qualname": "DEFAULT_ANISOTROPIC_AUGMENTATIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;RandomHorizontalFlip3D&#x27;, &#x27;RandomVerticalFlip3D&#x27;, &#x27;RandomDepthicalFlip3D&#x27;]"}, {"fullname": "torch_em.transform.augmentation.create_augmentation", "modulename": "torch_em.transform.augmentation", "qualname": "create_augmentation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trafo</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.augmentation.get_augmentations", "modulename": "torch_em.transform.augmentation", "qualname": "get_augmentations", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">transforms</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.defect", "modulename": "torch_em.transform.defect", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.get_artifact_source", "modulename": "torch_em.transform.defect", "qualname": "get_artifact_source", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">artifact_path</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span>,</span><span class=\"param\">\t<span class=\"n\">min_mask_fraction</span>,</span><span class=\"param\">\t<span class=\"n\">normalizer</span><span class=\"o\">=&lt;</span><span class=\"n\">function</span> <span class=\"n\">standardize</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span><span class=\"o\">=</span><span class=\"s1\">&#39;artifacts&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">mask_key</span><span class=\"o\">=</span><span class=\"s1\">&#39;alpha_mask&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation", "kind": "class", "doc": "<p>Augment raw data with transformations similar to defects common in EM data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>p_drop_slice:</strong>  probability for a missing slice</li>\n<li><strong>p_low_contrast:</strong>  probability for a low contrast slice</li>\n<li><strong>p_deform_slice:</strong>  probaboloty for a deformed slice</li>\n<li><strong>p_paste_artifact:</strong>  probability for inserting an artifact from data source</li>\n<li><strong>contrast_scale:</strong>  scale of low contrast transformation</li>\n<li><strong>deformation_mode:</strong>  deformation mode that should be used</li>\n<li><strong>deformation_strength:</strong>  deformation strength in pixel</li>\n<li><strong>artifact_source:</strong>  data source for additional artifacts</li>\n<li><strong>mean_val:</strong>  mean value for artifact normalization</li>\n<li><strong>std_val:</strong>  std value for artifact normalization</li>\n</ul>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.__init__", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">p_drop_slice</span>,</span><span class=\"param\">\t<span class=\"n\">p_low_contrast</span>,</span><span class=\"param\">\t<span class=\"n\">p_deform_slice</span>,</span><span class=\"param\">\t<span class=\"n\">p_paste_artifact</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">contrast_scale</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">deformation_mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;undirected&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">deformation_strength</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">artifact_source</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mean_val</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">std_val</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.artifact_source", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.artifact_source", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.p_drop_slice", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.p_drop_slice", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.p_low_contrast", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.p_low_contrast", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.p_deform_slice", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.p_deform_slice", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.p_paste_artifact", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.p_paste_artifact", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.contrast_scale", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.contrast_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.mean_val", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.mean_val", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.std_val", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.std_val", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.deformation_strength", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.deformation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.drop_slice", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.drop_slice", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">raw</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.low_contrast", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.low_contrast", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">raw</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.compress_slice", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.compress_slice", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">raw</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.undirected_deformation", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.undirected_deformation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">raw</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.deform_slice", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.deform_slice", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">raw</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.defect.EMDefectAugmentation.paste_artifact", "modulename": "torch_em.transform.defect", "qualname": "EMDefectAugmentation.paste_artifact", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">raw</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.generic", "modulename": "torch_em.transform.generic", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.Tile", "modulename": "torch_em.transform.generic", "qualname": "Tile", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "torch_em.transform.generic.Tile.__init__", "modulename": "torch_em.transform.generic", "qualname": "Tile.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">reps</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,)</span>, </span><span class=\"param\"><span class=\"n\">match_shape_exactly</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "torch_em.transform.generic.Tile.reps", "modulename": "torch_em.transform.generic", "qualname": "Tile.reps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.Tile.match_shape_exactly", "modulename": "torch_em.transform.generic", "qualname": "Tile.match_shape_exactly", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.Tile.forward", "modulename": "torch_em.transform.generic", "qualname": "Tile.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.generic.Compose", "modulename": "torch_em.transform.generic", "qualname": "Compose", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.Compose.__init__", "modulename": "torch_em.transform.generic", "qualname": "Compose.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">transforms</span></span>)</span>"}, {"fullname": "torch_em.transform.generic.Compose.transforms", "modulename": "torch_em.transform.generic", "qualname": "Compose.transforms", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.Rescale", "modulename": "torch_em.transform.generic", "qualname": "Rescale", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.Rescale.__init__", "modulename": "torch_em.transform.generic", "qualname": "Rescale.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scale</span>, </span><span class=\"param\"><span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.generic.Rescale.scale", "modulename": "torch_em.transform.generic", "qualname": "Rescale.scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.Rescale.with_channels", "modulename": "torch_em.transform.generic", "qualname": "Rescale.with_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.ResizeInputs", "modulename": "torch_em.transform.generic", "qualname": "ResizeInputs", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.ResizeInputs.__init__", "modulename": "torch_em.transform.generic", "qualname": "ResizeInputs.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">target_shape</span>, </span><span class=\"param\"><span class=\"n\">is_label</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">is_rgb</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.transform.generic.ResizeInputs.target_shape", "modulename": "torch_em.transform.generic", "qualname": "ResizeInputs.target_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.ResizeInputs.is_label", "modulename": "torch_em.transform.generic", "qualname": "ResizeInputs.is_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.ResizeInputs.is_rgb", "modulename": "torch_em.transform.generic", "qualname": "ResizeInputs.is_rgb", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.PadIfNecessary", "modulename": "torch_em.transform.generic", "qualname": "PadIfNecessary", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.generic.PadIfNecessary.__init__", "modulename": "torch_em.transform.generic", "qualname": "PadIfNecessary.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span></span>)</span>"}, {"fullname": "torch_em.transform.generic.PadIfNecessary.shape", "modulename": "torch_em.transform.generic", "qualname": "PadIfNecessary.shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label", "modulename": "torch_em.transform.label", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.connected_components", "modulename": "torch_em.transform.label", "qualname": "connected_components", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">labels</span>, </span><span class=\"param\"><span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">ensure_zero</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.label.labels_to_binary", "modulename": "torch_em.transform.label", "qualname": "labels_to_binary", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">labels</span>, </span><span class=\"param\"><span class=\"n\">background_label</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.label.label_consecutive", "modulename": "torch_em.transform.label", "qualname": "label_consecutive", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">labels</span>, </span><span class=\"param\"><span class=\"n\">with_background</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.label.BoundaryTransform", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransform", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransform.__init__", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;thick&#39;</span>, </span><span class=\"param\"><span class=\"n\">add_binary_target</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.label.BoundaryTransform.mode", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransform.mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransform.add_binary_target", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransform.add_binary_target", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransform.ndim", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransform.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.NoToBackgroundBoundaryTransform", "modulename": "torch_em.transform.label", "qualname": "NoToBackgroundBoundaryTransform", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.NoToBackgroundBoundaryTransform.__init__", "modulename": "torch_em.transform.label", "qualname": "NoToBackgroundBoundaryTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">bg_label</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">mask_label</span><span class=\"o\">=-</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;thick&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">add_binary_target</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.label.NoToBackgroundBoundaryTransform.bg_label", "modulename": "torch_em.transform.label", "qualname": "NoToBackgroundBoundaryTransform.bg_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.NoToBackgroundBoundaryTransform.mask_label", "modulename": "torch_em.transform.label", "qualname": "NoToBackgroundBoundaryTransform.mask_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.NoToBackgroundBoundaryTransform.mode", "modulename": "torch_em.transform.label", "qualname": "NoToBackgroundBoundaryTransform.mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.NoToBackgroundBoundaryTransform.ndim", "modulename": "torch_em.transform.label", "qualname": "NoToBackgroundBoundaryTransform.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.NoToBackgroundBoundaryTransform.add_binary_target", "modulename": "torch_em.transform.label", "qualname": "NoToBackgroundBoundaryTransform.add_binary_target", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransformWithIgnoreLabel", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransformWithIgnoreLabel", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransformWithIgnoreLabel.__init__", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransformWithIgnoreLabel.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ignore_label</span><span class=\"o\">=-</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;thick&#39;</span>, </span><span class=\"param\"><span class=\"n\">add_binary_target</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">ndim</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.label.BoundaryTransformWithIgnoreLabel.ignore_label", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransformWithIgnoreLabel.ignore_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransformWithIgnoreLabel.mode", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransformWithIgnoreLabel.mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransformWithIgnoreLabel.ndim", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransformWithIgnoreLabel.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.BoundaryTransformWithIgnoreLabel.add_binary_target", "modulename": "torch_em.transform.label", "qualname": "BoundaryTransformWithIgnoreLabel.add_binary_target", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform.__init__", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">offsets</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_label</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">add_binary_target</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">add_mask</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">include_ignore_transitions</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "torch_em.transform.label.AffinityTransform.offsets", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.offsets", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform.ndim", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform.ignore_label", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.ignore_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform.add_binary_target", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.add_binary_target", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform.add_mask", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.add_mask", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform.include_ignore_transitions", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.include_ignore_transitions", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.AffinityTransform.add_ignore_transitions", "modulename": "torch_em.transform.label", "qualname": "AffinityTransform.add_ignore_transitions", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">affs</span>, </span><span class=\"param\"><span class=\"n\">mask</span>, </span><span class=\"param\"><span class=\"n\">labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.label.OneHotTransform", "modulename": "torch_em.transform.label", "qualname": "OneHotTransform", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.OneHotTransform.__init__", "modulename": "torch_em.transform.label", "qualname": "OneHotTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_ids</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.label.OneHotTransform.class_ids", "modulename": "torch_em.transform.label", "qualname": "OneHotTransform.class_ids", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform", "kind": "class", "doc": "<p>Compute distances to foreground in the labels.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>distances:</strong>  Whether to compute the absolute distances.</li>\n<li><strong>directed_distances:</strong>  Whether to compute the directed distances (vector distances).</li>\n<li><strong>normalize:</strong>  Whether to normalize the computed distances.</li>\n<li><strong>max_distance:</strong>  Maximal distance at which to threshold the distances.</li>\n<li><strong>foreground_id:</strong>  Label id to which the distance is compute.</li>\n<li><strong>invert Whether to invert the distances:</strong> </li>\n<li><strong>func:</strong>  Normalization function for the distances.</li>\n</ul>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform.__init__", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">distances</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">directed_distances</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">normalize</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">max_distance</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_id</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">invert</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">func</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.label.DistanceTransform.eps", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.eps", "kind": "variable", "doc": "<p></p>\n", "default_value": "1e-07"}, {"fullname": "torch_em.transform.label.DistanceTransform.directed_distances", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.directed_distances", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform.distances", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.distances", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform.normalize", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.normalize", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform.max_distance", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.max_distance", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform.foreground_id", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.foreground_id", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform.invert", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.invert", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.DistanceTransform.func", "modulename": "torch_em.transform.label", "qualname": "DistanceTransform.func", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform", "kind": "class", "doc": "<p>Compute normalized distances per object in a segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>distances:</strong>  Whether to compute the undirected distances.</li>\n<li><strong>boundary_distances:</strong>  Whether to compute the distances to the object boundaries.</li>\n<li><strong>directed_distances:</strong>  Whether to compute the directed distances (vector distances).</li>\n<li><strong>foreground:</strong>  Whether to return a foreground channel.</li>\n<li><strong>apply_label:</strong>  Whether to apply connected components to the labels before computing distances.</li>\n<li><strong>correct_centers:</strong>  Whether to correct centers that are not in the objects.</li>\n<li><strong>min_size:</strong>  Minimal size of objects for distance calculdation.</li>\n<li><strong>distance_fill_value:</strong>  Fill value for the distances outside of objects.</li>\n</ul>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.__init__", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">distances</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distances</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">directed_distances</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">foreground</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">instances</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">apply_label</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">correct_centers</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">distance_fill_value</span><span class=\"o\">=</span><span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.eps", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.eps", "kind": "variable", "doc": "<p></p>\n", "default_value": "1e-07"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.distances", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.distances", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.boundary_distances", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.boundary_distances", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.directed_distances", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.directed_distances", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.foreground", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.foreground", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.instances", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.instances", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.apply_label", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.apply_label", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.correct_centers", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.correct_centers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.min_size", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.min_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.distance_fill_value", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.distance_fill_value", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.label.PerObjectDistanceTransform.compute_normalized_object_distances", "modulename": "torch_em.transform.label", "qualname": "PerObjectDistanceTransform.compute_normalized_object_distances", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mask</span>, </span><span class=\"param\"><span class=\"n\">boundaries</span>, </span><span class=\"param\"><span class=\"n\">bb</span>, </span><span class=\"param\"><span class=\"n\">center</span>, </span><span class=\"param\"><span class=\"n\">distances</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.nnunet_raw", "modulename": "torch_em.transform.nnunet_raw", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform", "kind": "class", "doc": "<p>Apply transformation on the raw inputs.\nAdapted from nnUNetv2's <code>ImageNormalization</code>:\n    - <a href=\"https://github.com/MIC-DKFZ/nnUNet/tree/master/nnunetv2/preprocessing/normalization\">https://github.com/MIC-DKFZ/nnUNet/tree/master/nnunetv2/preprocessing/normalization</a></p>\n\n<p>You can use this class to apply the necessary raw transformations on input modalities.</p>\n\n<p>(Current Support - CT and PET): The inputs should be of dimension 2 * (H * W * D).\n    - The first channel should be CT volume\n    - The second channel should be PET volume</p>\n\n<p>Here's an example for how to use this class:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># Initialize the raw transform.</span>\n<span class=\"n\">raw_transform</span> <span class=\"o\">=</span> <span class=\"n\">nnUNetRawTransform</span><span class=\"p\">(</span><span class=\"n\">plans_file</span><span class=\"o\">=</span><span class=\"s2\">&quot;.../nnUNetPlans.json&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Apply transformation on the inputs.</span>\n<span class=\"n\">patient_vol</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">concatenate</span><span class=\"p\">(</span><span class=\"n\">ct_vol</span><span class=\"p\">,</span> <span class=\"n\">pet_vol</span><span class=\"p\">)</span>\n<span class=\"n\">patient_transformed</span> <span class=\"o\">=</span> <span class=\"n\">raw_transform</span><span class=\"p\">(</span><span class=\"n\">patient_vol</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform.__init__", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">plans_file</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\texpected_dtype: type = &lt;class &#x27;numpy.float32&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">tolerance</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;3d_fullres&#39;</span></span>)</span>"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform.expected_dtype", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform.expected_dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform.tolerance", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform.tolerance", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform.intensity_properties", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform.intensity_properties", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform.per_channel_scheme", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform.per_channel_scheme", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform.load_json", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform.load_json", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">_file</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.nnunet_raw.nnUNetRawTransform.ct_transform", "modulename": "torch_em.transform.nnunet_raw", "qualname": "nnUNetRawTransform.ct_transform", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">channel</span>, </span><span class=\"param\"><span class=\"n\">properties</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.raw", "modulename": "torch_em.transform.raw", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.TORCH_DTYPES", "modulename": "torch_em.transform.raw", "qualname": "TORCH_DTYPES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;float16&#x27;: torch.float16, &#x27;float32&#x27;: torch.float32, &#x27;float64&#x27;: torch.float64, &#x27;complex64&#x27;: torch.complex64, &#x27;complex128&#x27;: torch.complex128, &#x27;uint8&#x27;: torch.uint8, &#x27;int8&#x27;: torch.int8, &#x27;int16&#x27;: torch.int16, &#x27;int32&#x27;: torch.int32, &#x27;int64&#x27;: torch.int64, &#x27;bool&#x27;: torch.bool}"}, {"fullname": "torch_em.transform.raw.cast", "modulename": "torch_em.transform.raw", "qualname": "cast", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">inpt</span>, </span><span class=\"param\"><span class=\"n\">typestring</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.raw.standardize", "modulename": "torch_em.transform.raw", "qualname": "standardize", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw</span>, </span><span class=\"param\"><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">std</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.raw.normalize", "modulename": "torch_em.transform.raw", "qualname": "normalize", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw</span>, </span><span class=\"param\"><span class=\"n\">minval</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">maxval</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.raw.normalize_percentile", "modulename": "torch_em.transform.raw", "qualname": "normalize_percentile", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw</span>, </span><span class=\"param\"><span class=\"n\">lower</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">upper</span><span class=\"o\">=</span><span class=\"mf\">99.0</span>, </span><span class=\"param\"><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-07</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.raw.RandomContrast", "modulename": "torch_em.transform.raw", "qualname": "RandomContrast", "kind": "class", "doc": "<p>Adjust contrast by scaling image to <code>mean + alpha * (image - mean)</code>.</p>\n"}, {"fullname": "torch_em.transform.raw.RandomContrast.__init__", "modulename": "torch_em.transform.raw", "qualname": "RandomContrast.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>, </span><span class=\"param\"><span class=\"n\">clip_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;a_min&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">&#39;a_max&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">}</span></span>)</span>"}, {"fullname": "torch_em.transform.raw.RandomContrast.alpha", "modulename": "torch_em.transform.raw", "qualname": "RandomContrast.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.RandomContrast.mean", "modulename": "torch_em.transform.raw", "qualname": "RandomContrast.mean", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.RandomContrast.clip_kwargs", "modulename": "torch_em.transform.raw", "qualname": "RandomContrast.clip_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.AdditiveGaussianNoise", "modulename": "torch_em.transform.raw", "qualname": "AdditiveGaussianNoise", "kind": "class", "doc": "<p>Add random Gaussian noise to image.</p>\n"}, {"fullname": "torch_em.transform.raw.AdditiveGaussianNoise.__init__", "modulename": "torch_em.transform.raw", "qualname": "AdditiveGaussianNoise.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">clip_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;a_min&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">&#39;a_max&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">}</span></span>)</span>"}, {"fullname": "torch_em.transform.raw.AdditiveGaussianNoise.scale", "modulename": "torch_em.transform.raw", "qualname": "AdditiveGaussianNoise.scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.AdditiveGaussianNoise.clip_kwargs", "modulename": "torch_em.transform.raw", "qualname": "AdditiveGaussianNoise.clip_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.AdditivePoissonNoise", "modulename": "torch_em.transform.raw", "qualname": "AdditivePoissonNoise", "kind": "class", "doc": "<p>Add random Poisson noise to image.</p>\n"}, {"fullname": "torch_em.transform.raw.AdditivePoissonNoise.__init__", "modulename": "torch_em.transform.raw", "qualname": "AdditivePoissonNoise.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">lam</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">clip_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;a_min&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">&#39;a_max&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">}</span></span>)</span>"}, {"fullname": "torch_em.transform.raw.AdditivePoissonNoise.lam", "modulename": "torch_em.transform.raw", "qualname": "AdditivePoissonNoise.lam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.AdditivePoissonNoise.clip_kwargs", "modulename": "torch_em.transform.raw", "qualname": "AdditivePoissonNoise.clip_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.PoissonNoise", "modulename": "torch_em.transform.raw", "qualname": "PoissonNoise", "kind": "class", "doc": "<p>Add random data-dependent Poisson noise to image.</p>\n"}, {"fullname": "torch_em.transform.raw.PoissonNoise.__init__", "modulename": "torch_em.transform.raw", "qualname": "PoissonNoise.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">multiplier</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">5.0</span><span class=\"p\">,</span> <span class=\"mf\">10.0</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">clip_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;a_min&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">&#39;a_max&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">}</span></span>)</span>"}, {"fullname": "torch_em.transform.raw.PoissonNoise.multiplier", "modulename": "torch_em.transform.raw", "qualname": "PoissonNoise.multiplier", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.PoissonNoise.clip_kwargs", "modulename": "torch_em.transform.raw", "qualname": "PoissonNoise.clip_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.GaussianBlur", "modulename": "torch_em.transform.raw", "qualname": "GaussianBlur", "kind": "class", "doc": "<p>Blur the image.</p>\n"}, {"fullname": "torch_em.transform.raw.GaussianBlur.__init__", "modulename": "torch_em.transform.raw", "qualname": "GaussianBlur.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">sigma</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mf\">2.5</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "torch_em.transform.raw.GaussianBlur.kernel_size", "modulename": "torch_em.transform.raw", "qualname": "GaussianBlur.kernel_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.GaussianBlur.sigma", "modulename": "torch_em.transform.raw", "qualname": "GaussianBlur.sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.RawTransform", "modulename": "torch_em.transform.raw", "qualname": "RawTransform", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.RawTransform.__init__", "modulename": "torch_em.transform.raw", "qualname": "RawTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">normalizer</span>, </span><span class=\"param\"><span class=\"n\">augmentation1</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">augmentation2</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.transform.raw.RawTransform.normalizer", "modulename": "torch_em.transform.raw", "qualname": "RawTransform.normalizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.RawTransform.augmentation1", "modulename": "torch_em.transform.raw", "qualname": "RawTransform.augmentation1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.RawTransform.augmentation2", "modulename": "torch_em.transform.raw", "qualname": "RawTransform.augmentation2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.transform.raw.get_raw_transform", "modulename": "torch_em.transform.raw", "qualname": "get_raw_transform", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">normalizer</span><span class=\"o\">=&lt;</span><span class=\"n\">function</span> <span class=\"n\">standardize</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">augmentation1</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">augmentation2</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.transform.raw.get_default_mean_teacher_augmentations", "modulename": "torch_em.transform.raw", "qualname": "get_default_mean_teacher_augmentations", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.3</span>,</span><span class=\"param\">\t<span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">blur_kwargs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">poisson_kwargs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gaussian_kwargs</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util", "modulename": "torch_em.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.debug", "modulename": "torch_em.util.debug", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.debug.check_trainer", "modulename": "torch_em.util.debug", "qualname": "check_trainer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">trainer</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span>,</span><span class=\"param\">\t<span class=\"n\">instance_labels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"o\">=</span><span class=\"s1\">&#39;val&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">loader</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">plt</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.debug.check_loader", "modulename": "torch_em.util.debug", "qualname": "check_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">loader</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span>,</span><span class=\"param\">\t<span class=\"n\">instance_labels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">plt</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rgb</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.grid_search", "modulename": "torch_em.util.grid_search", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.grid_search.default_grid_search_values_boundary_based_instance_segmentation", "modulename": "torch_em.util.grid_search", "qualname": "default_grid_search_values_boundary_based_instance_segmentation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">threshold1_values</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">threshold2_values</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">min_size_values</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.grid_search.BoundaryBasedInstanceSegmentation", "modulename": "torch_em.util.grid_search", "qualname": "BoundaryBasedInstanceSegmentation", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a decoder.</p>\n\n<p>Implements the same interface as <code>AutomaticMaskGenerator</code>.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">segmenter</span> <span class=\"o\">=</span> <span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">,</span> <span class=\"n\">decoder</span><span class=\"p\">)</span>\n<span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>   <span class=\"c1\"># Predict the image embeddings and decoder outputs.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">center_distance_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the instance segmentation.</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>decoder:</strong>  The decoder to predict intermediate representations\nfor instance segmentation.</li>\n</ul>\n", "bases": "_InstanceSegmentationBase"}, {"fullname": "torch_em.util.grid_search.BoundaryBasedInstanceSegmentation.__init__", "modulename": "torch_em.util.grid_search", "qualname": "BoundaryBasedInstanceSegmentation.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">preprocess</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">halo</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.util.grid_search.BoundaryBasedInstanceSegmentation.initialize", "modulename": "torch_em.util.grid_search", "qualname": "BoundaryBasedInstanceSegmentation.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to be verbose.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.grid_search.BoundaryBasedInstanceSegmentation.generate", "modulename": "torch_em.util.grid_search", "qualname": "BoundaryBasedInstanceSegmentation.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>center_distance_threshold:</strong>  Center distance predictions below this value will be\nused to find seeds (intersected with thresholded boundary distance predictions).</li>\n<li><strong>boundary_distance_threshold:</strong>  Boundary distance predictions below this value will be\nused to find seeds (intersected with thresholded center distance predictions).</li>\n<li><strong>foreground_smoothing:</strong>  Sigma value for smoothing the foreground predictions, to avoid\ncheckerboard artifacts in the prediction.</li>\n<li><strong>foreground_threshold:</strong>  Foreground predictions above this value will be used as foreground mask.</li>\n<li><strong>distance_smoothing:</strong>  Sigma value for smoothing the distance predictions.</li>\n<li><strong>min_size:</strong>  Minimal object size in the segmentation result.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in. Pass None to directly return the instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">50</span>,</span><span class=\"param\">\t<span class=\"n\">threshold1</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">threshold2</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.grid_search.DistanceBasedInstanceSegmentation", "modulename": "torch_em.util.grid_search", "qualname": "DistanceBasedInstanceSegmentation", "kind": "class", "doc": "<p>Over-write micro_sam functionality so that it works for distance based\nsegmentation with a U-net.</p>\n", "bases": "_InstanceSegmentationBase"}, {"fullname": "torch_em.util.grid_search.DistanceBasedInstanceSegmentation.__init__", "modulename": "torch_em.util.grid_search", "qualname": "DistanceBasedInstanceSegmentation.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">preprocess</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">halo</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "torch_em.util.grid_search.DistanceBasedInstanceSegmentation.initialize", "modulename": "torch_em.util.grid_search", "qualname": "DistanceBasedInstanceSegmentation.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to be verbose.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.grid_search.instance_segmentation_grid_search", "modulename": "torch_em.util.grid_search", "qualname": "instance_segmentation_grid_search", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">image_key</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gt_key</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.image", "modulename": "torch_em.util.image", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.image.TIF_EXTS", "modulename": "torch_em.util.image", "qualname": "TIF_EXTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "(&#x27;.tif&#x27;, &#x27;.tiff&#x27;)"}, {"fullname": "torch_em.util.image.supports_memmap", "modulename": "torch_em.util.image", "qualname": "supports_memmap", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.image.load_image", "modulename": "torch_em.util.image", "qualname": "load_image", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image_path</span>, </span><span class=\"param\"><span class=\"n\">memmap</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.image.MultiDatasetWrapper", "modulename": "torch_em.util.image", "qualname": "MultiDatasetWrapper", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.image.MultiDatasetWrapper.__init__", "modulename": "torch_em.util.image", "qualname": "MultiDatasetWrapper.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">file_datasets</span></span>)</span>"}, {"fullname": "torch_em.util.image.MultiDatasetWrapper.file_datasets", "modulename": "torch_em.util.image", "qualname": "MultiDatasetWrapper.file_datasets", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.image.MultiDatasetWrapper.shape", "modulename": "torch_em.util.image", "qualname": "MultiDatasetWrapper.shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.image.load_data", "modulename": "torch_em.util.image", "qualname": "load_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">key</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;r&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo", "modulename": "torch_em.util.modelzoo", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.modelzoo.normalize_with_batch", "modulename": "torch_em.util.modelzoo", "qualname": "normalize_with_batch", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data</span>, </span><span class=\"param\"><span class=\"n\">normalizer</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.get_default_citations", "modulename": "torch_em.util.modelzoo", "qualname": "get_default_citations", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">model_output</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.export_bioimageio_model", "modulename": "torch_em.util.modelzoo", "qualname": "export_bioimageio_model", "kind": "function", "doc": "<p>Export model to bioimage.io model format.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span>,</span><span class=\"param\">\t<span class=\"n\">input_data</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">description</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">authors</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tags</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">license</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">documentation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">covers</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">git_repo</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cite</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_optional_parameters</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">model_postprocessing</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">for_deepimagej</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">links</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">maintainers</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_shape</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">training_data</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"o\">=</span><span class=\"p\">{}</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.main", "modulename": "torch_em.util.modelzoo", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.import_bioimageio_model", "modulename": "torch_em.util.modelzoo", "qualname": "import_bioimageio_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spec_path</span>, </span><span class=\"param\"><span class=\"n\">return_spec</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cpu&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.import_trainer_from_bioimageio_model", "modulename": "torch_em.util.modelzoo", "qualname": "import_trainer_from_bioimageio_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spec_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.convert_to_onnx", "modulename": "torch_em.util.modelzoo", "qualname": "convert_to_onnx", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spec_path</span>, </span><span class=\"param\"><span class=\"n\">opset_version</span><span class=\"o\">=</span><span class=\"mi\">12</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.convert_to_torchscript", "modulename": "torch_em.util.modelzoo", "qualname": "convert_to_torchscript", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.add_weight_formats", "modulename": "torch_em.util.modelzoo", "qualname": "add_weight_formats", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_path</span>, </span><span class=\"param\"><span class=\"n\">additional_formats</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.convert_main", "modulename": "torch_em.util.modelzoo", "qualname": "convert_main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.export_parser_helper", "modulename": "torch_em.util.modelzoo", "qualname": "export_parser_helper", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.get_mws_config", "modulename": "torch_em.util.modelzoo", "qualname": "get_mws_config", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">offsets</span>, </span><span class=\"param\"><span class=\"n\">config</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.modelzoo.get_shallow2deep_config", "modulename": "torch_em.util.modelzoo", "qualname": "get_shallow2deep_config", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">rf_path</span>, </span><span class=\"param\"><span class=\"n\">config</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.prediction", "modulename": "torch_em.util.prediction", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.prediction.predict_with_padding", "modulename": "torch_em.util.prediction", "qualname": "predict_with_padding", "kind": "function", "doc": "<p>Run prediction with padding for a model that can only deal with\ninputs divisible by specific factors.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model [torch.nn.Module]:</strong>  the model</li>\n<li><strong>input_ [np.ndarray]:</strong>  the input ()</li>\n<li><strong>min_divisible [tuple]:</strong>  the divisibe shape factors\n(e.g. (16, 16) for a model that needs inputs divisible by at least 16 pixels)</li>\n<li><strong>device [str, torch.device]:</strong>  the device of the model</li>\n<li><strong>with_channels [bool]:</strong>  Whether the input data contains channels (default: False)</li>\n<li>prediction_function [callable] - A wrapper function for prediction to enable custom prediction procedures\n(default: None)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>np.ndarray: the ouptut of the model</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">input_</span>,</span><span class=\"param\">\t<span class=\"n\">min_divisible</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_function</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.prediction.predict_with_halo", "modulename": "torch_em.util.prediction", "qualname": "predict_with_halo", "kind": "function", "doc": "<p>Run block-wise network prediction with halo.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>input_ [arraylike] - the input data, can be a numpy array, a hdf5/zarr/z5py dataset or similar</li>\n<li>model [nn.Module] - the network</li>\n<li>gpu_ids [list[int or string]] - list of gpus id used for prediction</li>\n<li>block_shape [tuple] - shape of inner block used for prediction</li>\n<li>halo [tuple] - shape of halo used for prediction</li>\n<li>output [arraylike or list[tuple[arraylike, slice]]] - output data, will be allocated if None is passed.\nInstead of a single output, this can also be a list of outputs and the corresponding channels.\n(default: None)</li>\n<li>preprocess [callable] - function to preprocess input data before passing it to the network.\n(default: standardize)</li>\n<li><strong>postprocess [callable] - function to postprocess the network predictions (default:</strong>  None)</li>\n<li><strong>with_channels [bool] - whether the input has a channel axis (default:</strong>  False)</li>\n<li><strong>skip_block [callable] - function to evaluate wheter a given input block should be skipped (default:</strong>  None)</li>\n<li><strong>mask [arraylike] - elements outside the mask will be ignored in the prediction (default:</strong>  None)</li>\n<li>disable_tqdm [bool] - flag that allows to disable tqdm output (e.g. if function is called multiple times)</li>\n<li>tqdm_desc [str] - description shown by the tqdm output</li>\n<li>prediction_function [callable] - a wrapper function for prediction to enable custom prediction procedures.\n(default: None)</li>\n<li><strong>roi [tuple[slice]] - a region of interest for which to run prediction. (default:</strong>  None)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_</span>,</span><span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">gpu_ids</span>,</span><span class=\"param\">\t<span class=\"n\">block_shape</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span>,</span><span class=\"param\">\t<span class=\"n\">output</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">preprocess</span><span class=\"o\">=&lt;</span><span class=\"n\">function</span> <span class=\"n\">standardize</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">postprocess</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">skip_block</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">disable_tqdm</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">tqdm_desc</span><span class=\"o\">=</span><span class=\"s1\">&#39;predict with halo&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_function</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">roi</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.reporting", "modulename": "torch_em.util.reporting", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.reporting.get_training_summary", "modulename": "torch_em.util.reporting", "qualname": "get_training_summary", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ckpt</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span>, </span><span class=\"param\"><span class=\"n\">to_md</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.segmentation", "modulename": "torch_em.util.segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.segmentation.size_filter", "modulename": "torch_em.util.segmentation", "qualname": "size_filter", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">seg</span>, </span><span class=\"param\"><span class=\"n\">min_size</span>, </span><span class=\"param\"><span class=\"n\">hmap</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">with_background</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.segmentation.mutex_watershed_segmentation", "modulename": "torch_em.util.segmentation", "qualname": "mutex_watershed_segmentation", "kind": "function", "doc": "<p>Computes the mutex watershed segmentation using the affinity maps for respective pixel offsets</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>- foreground:</strong>  [np.ndarray] - The foreground background channel for the objects</li>\n<li><ul>\n<li>affinities [np.ndarray] - The input affinity maps</li>\n</ul></li>\n\n<p><li><strong>- offsets:</strong>  [list[list[int]]] - The pixel offsets corresponding to the affinity channels</li>\n<li><strong>- min_size:</strong>  [int] - The minimum pixels (below which) to filter objects</li>\n<li><strong>- threshold:</strong>  [float] - To threshold foreground predictions</li>\n</ul></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">foreground</span>,</span><span class=\"param\">\t<span class=\"n\">affinities</span>,</span><span class=\"param\">\t<span class=\"n\">offsets</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">50</span>,</span><span class=\"param\">\t<span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.segmentation.connected_components_with_boundaries", "modulename": "torch_em.util.segmentation", "qualname": "connected_components_with_boundaries", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">foreground</span>, </span><span class=\"param\"><span class=\"n\">boundaries</span>, </span><span class=\"param\"><span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.segmentation.watershed_from_components", "modulename": "torch_em.util.segmentation", "qualname": "watershed_from_components", "kind": "function", "doc": "<p>The default approach:</p>\n\n<ul>\n<li>Subtract the boundaries from the foreground to separate touching objects.</li>\n<li>Use the connected components of this as seeds.</li>\n<li>Use the thresholded foreground predictions as mask to grow back the pieces\nlost by subtracting the boundary prediction.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>- boundaries:</strong>  [np.ndarray] - The boundaries for objects</li>\n<li><strong>- foreground:</strong>  [np.ndarray] - The foregrounds for objects</li>\n<li><strong>- min_size:</strong>  [int] - The minimum pixels (below which) to filter objects</li>\n<li><strong>- threshold1:</strong>  [float] - To separate touching objects (by subtracting bd and fg) above threshold</li>\n<li><strong>- threshold2:</strong>  [float] - To threshold foreground predictions</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>seg: [np.ndarray] - instance segmentation</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">boundaries</span>, </span><span class=\"param\"><span class=\"n\">foreground</span>, </span><span class=\"param\"><span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">50</span>, </span><span class=\"param\"><span class=\"n\">threshold1</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>, </span><span class=\"param\"><span class=\"n\">threshold2</span><span class=\"o\">=</span><span class=\"mf\">0.5</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.segmentation.watershed_from_maxima", "modulename": "torch_em.util.segmentation", "qualname": "watershed_from_maxima", "kind": "function", "doc": "<p>Find objects via seeded watershed starting from the maxima of the distance transform instead.\nThis has the advantage that objects can be better separated, but it may over-segment\nif the objects have complex shapes.</p>\n\n<p>The min_distance parameter controls the minimal distance between seeds, which\ncorresponds to the minimal distance between object centers.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>- boundaries:</strong>  [np.ndarray] - The boundaries for objects</li>\n<li><strong>- foreground:</strong>  [np.ndarray] - The foreground for objects</li>\n<li><strong>- min_size:</strong>  [int] - min. pixels (below which) to filter objects</li>\n<li><strong>- min_distance:</strong>  [int] - min. distance of peaks (see <code>from skimage.feature import peak_local_max</code>)</li>\n<li><strong>- sigma:</strong>  [float] - standard deviation for gaussian kernel. (see <code>from skimage.filters import gaussian</code>)</li>\n<li><strong>- threshold1:</strong>  [float] - To threshold foreground predictions</li>\n</ul>\n\n<p>Returns\n    seg: [np.ndarray] - instance segmentation</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">boundaries</span>,</span><span class=\"param\">\t<span class=\"n\">foreground</span>,</span><span class=\"param\">\t<span class=\"n\">min_distance</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">50</span>,</span><span class=\"param\">\t<span class=\"n\">sigma</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">threshold1</span><span class=\"o\">=</span><span class=\"mf\">0.5</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.segmentation.watershed_from_center_and_boundary_distances", "modulename": "torch_em.util.segmentation", "qualname": "watershed_from_center_and_boundary_distances", "kind": "function", "doc": "<p>Seeded watershed based on distance predictions to object center and boundaries.</p>\n\n<p>The seeds are computed by finding connected components where both distance predictions\nare smaller than the respective thresholds. Using both distances here should prevent merging\nnarrow adjacent objects (if only using the center distance) or finding multiple seeds for non-convex\ncells (if only using the boundary distances).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>center_distances [np.ndarray] - Distance prediction to the objcet center.</li>\n<li>boundary_distances [np.ndarray] - Inverted distance prediction to object boundaries.</li>\n<li>foreground_map [np.ndarray] - Predictio for foreground probabilities.</li>\n<li>center_distance_threshold [float] - Center distance predictions below this value will be\nused to find seeds (intersected with thresholded boundary distance predictions).</li>\n<li>boundary_distance_threshold [float] - Boundary distance predictions below this value will be\nused to find seeds (intersected with thresholded center distance predictions).</li>\n<li>foreground_threshold [float] - Foreground predictions above this value will be used as foreground mask.</li>\n<li>distance_smoothing [float] - Sigma value for smoothing the distance predictions.</li>\n<li>min_size [int] - Minimal object size in the segmentation result.</li>\n<li>debug [bool] - Return all intermediate results for debugging.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>np.ndarray - The instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">center_distances</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distances</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_map</span>,</span><span class=\"param\">\t<span class=\"n\">center_distance_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing</span><span class=\"o\">=</span><span class=\"mf\">1.6</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">debug</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.submit_slurm", "modulename": "torch_em.util.submit_slurm", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.submit_slurm.TWO_DAYS", "modulename": "torch_em.util.submit_slurm", "qualname": "TWO_DAYS", "kind": "variable", "doc": "<p></p>\n", "default_value": "2880"}, {"fullname": "torch_em.util.submit_slurm.write_slurm_template", "modulename": "torch_em.util.submit_slurm", "qualname": "write_slurm_template", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">script</span>,</span><span class=\"param\">\t<span class=\"n\">out_path</span>,</span><span class=\"param\">\t<span class=\"n\">env_name</span>,</span><span class=\"param\">\t<span class=\"n\">n_threads</span>,</span><span class=\"param\">\t<span class=\"n\">gpu_type</span>,</span><span class=\"param\">\t<span class=\"n\">n_gpus</span>,</span><span class=\"param\">\t<span class=\"n\">mem_limit</span>,</span><span class=\"param\">\t<span class=\"n\">time_limit</span>,</span><span class=\"param\">\t<span class=\"n\">qos</span>,</span><span class=\"param\">\t<span class=\"n\">mail_address</span>,</span><span class=\"param\">\t<span class=\"n\">exclude_nodes</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.submit_slurm.submit_slurm", "modulename": "torch_em.util.submit_slurm", "qualname": "submit_slurm", "kind": "function", "doc": "<p>Submit python script that needs gpus with given inputs on a slurm node.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">script</span>,</span><span class=\"param\">\t<span class=\"n\">input_</span>,</span><span class=\"param\">\t<span class=\"n\">n_threads</span><span class=\"o\">=</span><span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">n_gpus</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">gpu_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;2080Ti&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">mem_limit</span><span class=\"o\">=</span><span class=\"s1\">&#39;64G&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">time_limit</span><span class=\"o\">=</span><span class=\"mi\">2880</span>,</span><span class=\"param\">\t<span class=\"n\">qos</span><span class=\"o\">=</span><span class=\"s1\">&#39;normal&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">env_name</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mail_address</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">exclude_nodes</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.submit_slurm.scrape_kwargs", "modulename": "torch_em.util.submit_slurm", "qualname": "scrape_kwargs", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.submit_slurm.main", "modulename": "torch_em.util.submit_slurm", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.test", "modulename": "torch_em.util.test", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.test.make_gt", "modulename": "torch_em.util.test", "qualname": "make_gt", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spatial_shape</span>,</span><span class=\"param\">\t<span class=\"n\">n_batches</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.test.create_segmentation_test_data", "modulename": "torch_em.util.test", "qualname": "create_segmentation_test_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data_path</span>, </span><span class=\"param\"><span class=\"n\">raw_key</span>, </span><span class=\"param\"><span class=\"n\">label_key</span>, </span><span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">chunks</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.test.create_image_collection_test_data", "modulename": "torch_em.util.test", "qualname": "create_image_collection_test_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">folder</span>, </span><span class=\"param\"><span class=\"n\">n_images</span>, </span><span class=\"param\"><span class=\"n\">min_shape</span>, </span><span class=\"param\"><span class=\"n\">max_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.training", "modulename": "torch_em.util.training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.training.parser_helper", "modulename": "torch_em.util.training", "qualname": "parser_helper", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">description</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_iterations</span><span class=\"o\">=</span><span class=\"mi\">100000</span>,</span><span class=\"param\">\t<span class=\"n\">default_batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">require_input</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util", "modulename": "torch_em.util.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.util.DTYPE_MAP", "modulename": "torch_em.util.util", "qualname": "DTYPE_MAP", "kind": "variable", "doc": "<p></p>\n", "default_value": "{dtype(&#x27;uint16&#x27;): &lt;class &#x27;numpy.int16&#x27;&gt;, dtype(&#x27;uint32&#x27;): &lt;class &#x27;numpy.int32&#x27;&gt;, dtype(&#x27;uint64&#x27;): &lt;class &#x27;numpy.int64&#x27;&gt;}"}, {"fullname": "torch_em.util.util.is_compiled", "modulename": "torch_em.util.util", "qualname": "is_compiled", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.auto_compile", "modulename": "torch_em.util.util", "qualname": "auto_compile", "kind": "function", "doc": "<p>Model compilation for pytorch &gt;= 2</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>model [torch.nn.Module] - the model</li>\n<li>compile_model [None, bool, str] - whether to comile the model.\nIf None, it will not be compiled for torch &lt; 2, and for torch &gt; 2 the behavior\nspecificed by 'default_compile' will be used. If a string is given it will be\nintepreted as the compile mode (torch.compile(model, mode=compile_model)) (default: None)</li>\n<li>default_compile [bool] - the default compilation behavior for torch 2</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">compile_model</span>, </span><span class=\"param\"><span class=\"n\">default_compile</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.ensure_tensor", "modulename": "torch_em.util.util", "qualname": "ensure_tensor", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tensor</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.ensure_tensor_with_channels", "modulename": "torch_em.util.util", "qualname": "ensure_tensor_with_channels", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tensor</span>, </span><span class=\"param\"><span class=\"n\">ndim</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.ensure_array", "modulename": "torch_em.util.util", "qualname": "ensure_array", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">array</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.ensure_spatial_array", "modulename": "torch_em.util.util", "qualname": "ensure_spatial_array", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">array</span>, </span><span class=\"param\"><span class=\"n\">ndim</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.get_constructor_arguments", "modulename": "torch_em.util.util", "qualname": "get_constructor_arguments", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">obj</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.get_trainer", "modulename": "torch_em.util.util", "qualname": "get_trainer", "kind": "function", "doc": "<p>Load trainer from a checkpoint.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">checkpoint</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span>, </span><span class=\"param\"><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.get_normalizer", "modulename": "torch_em.util.util", "qualname": "get_normalizer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trainer</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.load_model", "modulename": "torch_em.util.util", "qualname": "load_model", "kind": "function", "doc": "<p>Convenience function to load a model from a trainer checkpoint.</p>\n\n<p>This function can either load the model directly from the trainer (model is not passed),\nor deserialize the model state from the trainer and load the model state (model is passed).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li>checkpoint [str] - path to the checkpoint folder.</li>\n<li>model [torch.nn.Module] - the model for which the state should be loaded.\nIf it is not passed the model class and parameters will also be loaded from the trainer. (default: None)</li>\n<li><strong>name [str] - the name of the checkpoint. (default:</strong>  \"best\")</li>\n<li><strong>state_key [str] - the name of the model state to load. (default:</strong>  \"model_state\")</li>\n<li><strong>device [torch.device] - the device on which to load the model. (default:</strong>  None)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">state_key</span><span class=\"o\">=</span><span class=\"s1\">&#39;model_state&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.model_is_equal", "modulename": "torch_em.util.util", "qualname": "model_is_equal", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model1</span>, </span><span class=\"param\"><span class=\"n\">model2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.util.get_random_colors", "modulename": "torch_em.util.util", "qualname": "get_random_colors", "kind": "function", "doc": "<p>Function to generate a random color map for a label image</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.validation", "modulename": "torch_em.util.validation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.validation.SampleGenerator", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.validation.SampleGenerator.__init__", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">max_samples</span>, </span><span class=\"param\"><span class=\"n\">need_gt</span>, </span><span class=\"param\"><span class=\"n\">n_threads</span></span>)</span>"}, {"fullname": "torch_em.util.validation.SampleGenerator.need_gt", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.need_gt", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.validation.SampleGenerator.n_threads", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.n_threads", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.validation.SampleGenerator.ndim", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.ndim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.validation.SampleGenerator.load_2d_from_3d", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.load_2d_from_3d", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.validation.SampleGenerator.rois", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.rois", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "torch_em.util.validation.SampleGenerator.paths_from_ds", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.paths_from_ds", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataset</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.validation.SampleGenerator.load_data", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.load_data", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">key</span>, </span><span class=\"param\"><span class=\"n\">roi</span>, </span><span class=\"param\"><span class=\"n\">z</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.validation.SampleGenerator.load_sample", "modulename": "torch_em.util.validation", "qualname": "SampleGenerator.load_sample", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">sample_id</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.validation.validate_checkpoint", "modulename": "torch_em.util.validation", "qualname": "validate_checkpoint", "kind": "function", "doc": "<p>Validate model for the given checkpoint visually and/or via metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span>,</span><span class=\"param\">\t<span class=\"n\">gpu_ids</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">visualize</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_threads</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "torch_em.util.validation.main", "modulename": "torch_em.util.validation", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();